

============================== 2022-11-27 11:36:53.440223 | 6ff85ab8-fabf-46b0-a0ba-b180c313a00e ==============================
[0m11:36:53.440223 [info ] [MainThread]: Running with dbt=1.3.1
[0m11:36:53.441211 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m11:36:53.441211 [debug] [MainThread]: Tracking: tracking
[0m11:36:53.458971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298B436E830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298B436F250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298B436E8C0>]}
[0m11:36:53.934616 [debug] [MainThread]: Executing "git --help"
[0m11:36:53.956870 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:36:53.956870 [debug] [MainThread]: STDERR: "b''"
[0m11:36:53.969004 [debug] [MainThread]: Acquiring new databricks connection "debug"
[0m11:36:53.969004 [debug] [MainThread]: Using databricks connection "debug"
[0m11:36:53.969004 [debug] [MainThread]: On debug: select 1 as id
[0m11:36:53.969004 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:40:14.052566 [debug] [MainThread]: SQL status: OK in 200.08 seconds
[0m11:40:14.053641 [debug] [MainThread]: On debug: Close
[0m11:40:15.470903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298C39BF100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298C39C95A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298C39C9390>]}
[0m11:40:15.470903 [debug] [MainThread]: Flushing usage events
[0m11:40:17.409650 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2022-11-27 12:08:20.431627 | 251170ec-a0a1-4cef-8090-8c7203cb4273 ==============================
[0m12:08:20.431627 [info ] [MainThread]: Running with dbt=1.3.1
[0m12:08:20.433614 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/finance_bronze_to_silver.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m12:08:20.434112 [debug] [MainThread]: Tracking: tracking
[0m12:08:20.442943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DF8249C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DF8249870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DF8249AB0>]}
[0m12:08:20.456609 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m12:08:20.461159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '251170ec-a0a1-4cef-8090-8c7203cb4273', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DF825B790>]}
[0m12:08:20.898744 [debug] [MainThread]: Parsing macros\adapters.sql
[0m12:08:20.921500 [debug] [MainThread]: Parsing macros\catalog.sql
[0m12:08:20.921500 [debug] [MainThread]: Parsing macros\statement.sql
[0m12:08:20.921500 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m12:08:20.931199 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m12:08:20.941658 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m12:08:20.941658 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m12:08:20.941658 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m12:08:20.951342 [debug] [MainThread]: Parsing macros\adapters.sql
[0m12:08:20.982728 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m12:08:20.982728 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m12:08:21.006357 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m12:08:21.006357 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m12:08:21.006357 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m12:08:21.011457 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m12:08:21.014463 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
[0m12:08:21.021056 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m12:08:21.031174 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m12:08:21.036179 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m12:08:21.041275 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m12:08:21.041275 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m12:08:21.047833 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m12:08:21.051612 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m12:08:21.062633 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m12:08:21.062633 [debug] [MainThread]: Parsing macros\adapters\timestamps.sql
[0m12:08:21.062633 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m12:08:21.071144 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m12:08:21.081043 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m12:08:21.083133 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m12:08:21.083133 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m12:08:21.083133 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m12:08:21.083133 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m12:08:21.086069 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m12:08:21.086069 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m12:08:21.086069 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m12:08:21.091080 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m12:08:21.091080 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m12:08:21.104297 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m12:08:21.114823 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m12:08:21.115638 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m12:08:21.122968 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m12:08:21.130992 [debug] [MainThread]: Parsing macros\materializations\models\incremental\strategies.sql
[0m12:08:21.141658 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m12:08:21.141658 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m12:08:21.151022 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m12:08:21.157106 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m12:08:21.157106 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m12:08:21.161493 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m12:08:21.162709 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m12:08:21.171701 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m12:08:21.187209 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m12:08:21.191212 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m12:08:21.203654 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m12:08:21.203654 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m12:08:21.218737 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m12:08:21.218737 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m12:08:21.221258 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m12:08:21.221258 [debug] [MainThread]: Parsing macros\python_model\python.sql
[0m12:08:21.221258 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m12:08:21.221258 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m12:08:21.221258 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m12:08:21.231014 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m12:08:21.231014 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m12:08:21.231014 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m12:08:21.231014 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m12:08:21.234745 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m12:08:21.234745 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m12:08:21.234745 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m12:08:21.241259 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m12:08:21.241259 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m12:08:21.241259 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m12:08:21.241259 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m12:08:21.241259 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m12:08:21.241259 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m12:08:21.241259 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m12:08:21.241259 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m12:08:21.241259 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m12:08:21.241259 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m12:08:21.241259 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m12:08:21.250980 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m12:08:21.250980 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m12:08:21.250980 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m12:08:21.250980 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m12:08:21.521182 [debug] [MainThread]: 1699: static parser successfully parsed finance_bronze_to_silver.sql
[0m12:08:21.531256 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m12:08:21.531256 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m12:08:21.584110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '251170ec-a0a1-4cef-8090-8c7203cb4273', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DF8426290>]}
[0m12:08:21.594271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '251170ec-a0a1-4cef-8090-8c7203cb4273', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DF8249AB0>]}
[0m12:08:21.594271 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m12:08:21.594271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '251170ec-a0a1-4cef-8090-8c7203cb4273', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DF834A320>]}
[0m12:08:21.594271 [info ] [MainThread]: 
[0m12:08:21.594271 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m12:08:21.598877 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m12:08:21.606221 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m12:08:21.606221 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:08:21.606221 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:08:24.431638 [debug] [ThreadPool]: SQL status: OK in 2.83 seconds
[0m12:08:24.461138 [debug] [ThreadPool]: On list_schemas: Close
[0m12:08:26.501470 [debug] [ThreadPool]: Acquiring new databricks connection "create__finance"
[0m12:08:26.501470 [debug] [ThreadPool]: Acquiring new databricks connection "create__finance"
[0m12:08:26.501470 [debug] [ThreadPool]: Creating schema "_ReferenceKey(database=None, schema='finance', identifier=None)"
[0m12:08:26.508362 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:08:26.508362 [debug] [ThreadPool]: Using databricks connection "create__finance"
[0m12:08:26.508362 [debug] [ThreadPool]: On create__finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "create__finance"} */
create schema if not exists finance
  
[0m12:08:26.508362 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:08:31.341466 [debug] [ThreadPool]: SQL status: OK in 4.83 seconds
[0m12:08:31.341466 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m12:08:31.351131 [debug] [ThreadPool]: On create__finance: ROLLBACK
[0m12:08:31.351131 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:08:31.351131 [debug] [ThreadPool]: On create__finance: Close
[0m12:08:34.251828 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m12:08:34.257481 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:08:34.257481 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m12:08:34.257481 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m12:08:34.257481 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:08:38.649397 [debug] [ThreadPool]: SQL status: OK in 4.39 seconds
[0m12:08:38.657392 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m12:08:38.658391 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:08:38.658391 [debug] [ThreadPool]: On list_None_finance: Close
[0m12:08:39.786263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '251170ec-a0a1-4cef-8090-8c7203cb4273', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DF94279A0>]}
[0m12:08:39.787200 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:08:39.788225 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:08:39.789149 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:08:39.790213 [info ] [MainThread]: 
[0m12:08:39.829450 [debug] [Thread-1 (]: Began running node model.finance.finance_bronze_to_silver
[0m12:08:39.829450 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.finance_bronze_to_silver .................. [RUN]
[0m12:08:39.830457 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.finance_bronze_to_silver"
[0m12:08:39.831596 [debug] [Thread-1 (]: Began compiling node model.finance.finance_bronze_to_silver
[0m12:08:39.832049 [debug] [Thread-1 (]: Compiling model.finance.finance_bronze_to_silver
[0m12:08:39.835555 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.finance_bronze_to_silver"
[0m12:08:39.837121 [debug] [Thread-1 (]: finished collecting timing info
[0m12:08:39.837367 [debug] [Thread-1 (]: Began executing node model.finance.finance_bronze_to_silver
[0m12:08:39.877131 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.finance_bronze_to_silver"
[0m12:08:39.878177 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:08:39.878525 [debug] [Thread-1 (]: Using databricks connection "model.finance.finance_bronze_to_silver"
[0m12:08:39.878811 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

USE finance;
DROP TABLE IF EXISTS finance_employees;
CREATE TABLE finance_employees 
    USING csv 
    OPTIONS (
        path='wasb://courseware@dbacademy.blob.core.windows.net/data-analysis-with-databricks/v01/web_events/web-events.csv',
        header="true",
        inferSchema="true"
    );
DROP TABLE IF EXISTS finance_emp;
CREATE OR REPLACE TABLE finance_emp AS
    SELECT * FROM finance_employees;
  
[0m12:08:39.878811 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:08:45.227955 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

USE finance;
DROP TABLE IF EXISTS finance_employees;
CREATE TABLE finance_employees 
    USING csv 
    OPTIONS (
        path='wasb://courseware@dbacademy.blob.core.windows.net/data-analysis-with-databricks/v01/web_events/web-events.csv',
        header="true",
        inferSchema="true"
    );
DROP TABLE IF EXISTS finance_emp;
CREATE OR REPLACE TABLE finance_emp AS
    SELECT * FROM finance_employees;
  
[0m12:08:45.229046 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'USE'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

USE finance;
^^^
DROP TABLE IF EXISTS finance_employees;
CREATE TABLE finance_employees 
    USING csv 
    OPTIONS (
        path='wasb:REDACTED_LOCAL_PART@dbacademy.blob.core.windows.net/data-analysis-with-databricks/v01/web_events/web-events.csv',
        header="true",
        inferSchema="true"
    );
DROP TABLE IF EXISTS finance_emp;
CREATE OR REPLACE TABLE finance_emp AS
    SELECT * FROM finance_employees

[0m12:08:45.229046 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'USE'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

USE finance;
^^^
DROP TABLE IF EXISTS finance_employees;
CREATE TABLE finance_employees 
    USING csv 
    OPTIONS (
        path='wasb:REDACTED_LOCAL_PART@dbacademy.blob.core.windows.net/data-analysis-with-databricks/v01/web_events/web-events.csv',
        header="true",
        inferSchema="true"
    );
DROP TABLE IF EXISTS finance_emp;
CREATE OR REPLACE TABLE finance_emp AS
    SELECT * FROM finance_employees

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'USE'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

USE finance;
^^^
DROP TABLE IF EXISTS finance_employees;
CREATE TABLE finance_employees 
    USING csv 
    OPTIONS (
        path='wasb:REDACTED_LOCAL_PART@dbacademy.blob.core.windows.net/data-analysis-with-databricks/v01/web_events/web-events.csv',
        header="true",
        inferSchema="true"
    );
DROP TABLE IF EXISTS finance_emp;
CREATE OR REPLACE TABLE finance_emp AS
    SELECT * FROM finance_employees

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:356)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:166)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:90)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:112)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:346)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:346)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:333)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:392)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:691)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:392)
	... 19 more

[0m12:08:45.230005 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'(\x17\tQZ\x1eKT\xb8\xc7\x87&\xad\x13\xd52'
[0m12:08:45.230555 [debug] [Thread-1 (]: finished collecting timing info
[0m12:08:45.231061 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: ROLLBACK
[0m12:08:45.231061 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:08:45.231061 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: Close
[0m12:08:46.264816 [debug] [Thread-1 (]: Runtime Error in model finance_bronze_to_silver (models\finance_bronze_to_silver.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'USE'(line 18, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */
  
    
      
        create or replace table finance.finance_bronze_to_silver
      
      
      using delta
      
      
      
      
      
      
      as
        
  
  USE finance;
  ^^^
  DROP TABLE IF EXISTS finance_employees;
  CREATE TABLE finance_employees 
      USING csv 
      OPTIONS (
          path='wasb:REDACTED_LOCAL_PART@dbacademy.blob.core.windows.net/data-analysis-with-databricks/v01/web_events/web-events.csv',
          header="true",
          inferSchema="true"
      );
  DROP TABLE IF EXISTS finance_emp;
  CREATE OR REPLACE TABLE finance_emp AS
      SELECT * FROM finance_employees
  
[0m12:08:46.265819 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '251170ec-a0a1-4cef-8090-8c7203cb4273', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DF8397940>]}
[0m12:08:46.266659 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model finance.finance_bronze_to_silver ......... [[31mERROR[0m in 6.43s]
[0m12:08:46.269402 [debug] [Thread-1 (]: Finished running node model.finance.finance_bronze_to_silver
[0m12:08:46.271324 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m12:08:46.271324 [debug] [MainThread]: On master: ROLLBACK
[0m12:08:46.272433 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:08:47.309369 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:08:47.310360 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:08:47.310360 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:08:47.311281 [debug] [MainThread]: On master: ROLLBACK
[0m12:08:47.312093 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:08:47.312093 [debug] [MainThread]: On master: Close
[0m12:08:48.293936 [info ] [MainThread]: 
[0m12:08:48.296039 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 26.70 seconds (26.70s).
[0m12:08:48.297044 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:08:48.298110 [debug] [MainThread]: Connection 'create__finance' was properly closed.
[0m12:08:48.298110 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m12:08:48.298110 [debug] [MainThread]: Connection 'model.finance.finance_bronze_to_silver' was properly closed.
[0m12:08:48.309220 [info ] [MainThread]: 
[0m12:08:48.309595 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m12:08:48.310095 [info ] [MainThread]: 
[0m12:08:48.311124 [error] [MainThread]: [33mRuntime Error in model finance_bronze_to_silver (models\finance_bronze_to_silver.sql)[0m
[0m12:08:48.312188 [error] [MainThread]:   
[0m12:08:48.313570 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near 'USE'(line 18, pos 0)
[0m12:08:48.314703 [error] [MainThread]:   
[0m12:08:48.314703 [error] [MainThread]:   == SQL ==
[0m12:08:48.316479 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */
[0m12:08:48.316922 [error] [MainThread]:   
[0m12:08:48.317915 [error] [MainThread]:     
[0m12:08:48.317915 [error] [MainThread]:       
[0m12:08:48.318922 [error] [MainThread]:         create or replace table finance.finance_bronze_to_silver
[0m12:08:48.318922 [error] [MainThread]:       
[0m12:08:48.319918 [error] [MainThread]:       
[0m12:08:48.321031 [error] [MainThread]:       using delta
[0m12:08:48.321835 [error] [MainThread]:       
[0m12:08:48.322835 [error] [MainThread]:       
[0m12:08:48.323580 [error] [MainThread]:       
[0m12:08:48.323960 [error] [MainThread]:       
[0m12:08:48.323960 [error] [MainThread]:       
[0m12:08:48.325133 [error] [MainThread]:       
[0m12:08:48.326129 [error] [MainThread]:       as
[0m12:08:48.326129 [error] [MainThread]:         
[0m12:08:48.330288 [error] [MainThread]:   
[0m12:08:48.334602 [error] [MainThread]:   USE finance;
[0m12:08:48.335385 [error] [MainThread]:   ^^^
[0m12:08:48.336119 [error] [MainThread]:   DROP TABLE IF EXISTS finance_employees;
[0m12:08:48.336119 [error] [MainThread]:   CREATE TABLE finance_employees 
[0m12:08:48.337324 [error] [MainThread]:       USING csv 
[0m12:08:48.337324 [error] [MainThread]:       OPTIONS (
[0m12:08:48.338739 [error] [MainThread]:           path='wasb:REDACTED_LOCAL_PART@dbacademy.blob.core.windows.net/data-analysis-with-databricks/v01/web_events/web-events.csv',
[0m12:08:48.338739 [error] [MainThread]:           header="true",
[0m12:08:48.340072 [error] [MainThread]:           inferSchema="true"
[0m12:08:48.340072 [error] [MainThread]:       );
[0m12:08:48.341176 [error] [MainThread]:   DROP TABLE IF EXISTS finance_emp;
[0m12:08:48.341176 [error] [MainThread]:   CREATE OR REPLACE TABLE finance_emp AS
[0m12:08:48.341797 [error] [MainThread]:       SELECT * FROM finance_employees
[0m12:08:48.343007 [error] [MainThread]:   
[0m12:08:48.346478 [info ] [MainThread]: 
[0m12:08:48.362723 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m12:08:48.364715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DF825B100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DF9520610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DF94DBAF0>]}
[0m12:08:48.364715 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 12:12:06.892876 | 44f650d5-2b3c-4ca7-9b2a-f8f02d8f3e63 ==============================
[0m12:12:06.892876 [info ] [MainThread]: Running with dbt=1.3.1
[0m12:12:06.894342 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/finance_bronze_to_silver.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m12:12:06.894342 [debug] [MainThread]: Tracking: tracking
[0m12:12:06.904552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B5D799C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B5D799870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B5D799AB0>]}
[0m12:12:06.971670 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:12:06.972658 [debug] [MainThread]: Partial parsing: updated file: finance://models\finance_bronze_to_silver.sql
[0m12:12:06.981706 [debug] [MainThread]: 1699: static parser successfully parsed finance_bronze_to_silver.sql
[0m12:12:07.008296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '44f650d5-2b3c-4ca7-9b2a-f8f02d8f3e63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B5E9736A0>]}
[0m12:12:07.015386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '44f650d5-2b3c-4ca7-9b2a-f8f02d8f3e63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B5D90F820>]}
[0m12:12:07.015386 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m12:12:07.016387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '44f650d5-2b3c-4ca7-9b2a-f8f02d8f3e63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B5D92DDB0>]}
[0m12:12:07.018380 [info ] [MainThread]: 
[0m12:12:07.019377 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m12:12:07.020372 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m12:12:07.028534 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m12:12:07.029532 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:12:07.029532 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:12:09.656392 [debug] [ThreadPool]: SQL status: OK in 2.63 seconds
[0m12:12:09.663371 [debug] [ThreadPool]: On list_schemas: Close
[0m12:12:10.657586 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m12:12:10.666838 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:12:10.666838 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m12:12:10.667831 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m12:12:10.667831 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:12:12.754647 [debug] [ThreadPool]: SQL status: OK in 2.09 seconds
[0m12:12:12.760399 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m12:12:12.760399 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:12:12.760399 [debug] [ThreadPool]: On list_None_finance: Close
[0m12:12:14.740279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '44f650d5-2b3c-4ca7-9b2a-f8f02d8f3e63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B5D786710>]}
[0m12:12:14.741272 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:12:14.741272 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:12:14.743269 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:12:14.744262 [info ] [MainThread]: 
[0m12:12:14.761307 [debug] [Thread-1 (]: Began running node model.finance.finance_bronze_to_silver
[0m12:12:14.762640 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.finance_bronze_to_silver .................. [RUN]
[0m12:12:14.763722 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.finance_bronze_to_silver"
[0m12:12:14.763722 [debug] [Thread-1 (]: Began compiling node model.finance.finance_bronze_to_silver
[0m12:12:14.764724 [debug] [Thread-1 (]: Compiling model.finance.finance_bronze_to_silver
[0m12:12:14.824933 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.finance_bronze_to_silver"
[0m12:12:14.826601 [debug] [Thread-1 (]: finished collecting timing info
[0m12:12:14.827097 [debug] [Thread-1 (]: Began executing node model.finance.finance_bronze_to_silver
[0m12:12:14.874030 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.finance_bronze_to_silver"
[0m12:12:14.875027 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:12:14.875027 [debug] [Thread-1 (]: Using databricks connection "model.finance.finance_bronze_to_silver"
[0m12:12:14.876019 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

USE finance;
DROP TABLE IF EXISTS finance_employees;
CREATE TABLE finance_employees 
    USING parquet 
    OPTIONS (
        path='/mnt/cntdlt/source/finance_tmp/*.parquet',
        header="true",
        inferSchema="true"
    );
DROP TABLE IF EXISTS finance_emp;
CREATE OR REPLACE TABLE finance_emp AS
    SELECT * FROM finance_employees;
  
[0m12:12:14.876019 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:12:17.332914 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

USE finance;
DROP TABLE IF EXISTS finance_employees;
CREATE TABLE finance_employees 
    USING parquet 
    OPTIONS (
        path='/mnt/cntdlt/source/finance_tmp/*.parquet',
        header="true",
        inferSchema="true"
    );
DROP TABLE IF EXISTS finance_emp;
CREATE OR REPLACE TABLE finance_emp AS
    SELECT * FROM finance_employees;
  
[0m12:12:17.332914 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'USE'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

USE finance;
^^^
DROP TABLE IF EXISTS finance_employees;
CREATE TABLE finance_employees 
    USING parquet 
    OPTIONS (
        path='/mnt/cntdlt/source/finance_tmp/*.parquet',
        header="true",
        inferSchema="true"
    );
DROP TABLE IF EXISTS finance_emp;
CREATE OR REPLACE TABLE finance_emp AS
    SELECT * FROM finance_employees

[0m12:12:17.333914 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'USE'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

USE finance;
^^^
DROP TABLE IF EXISTS finance_employees;
CREATE TABLE finance_employees 
    USING parquet 
    OPTIONS (
        path='/mnt/cntdlt/source/finance_tmp/*.parquet',
        header="true",
        inferSchema="true"
    );
DROP TABLE IF EXISTS finance_emp;
CREATE OR REPLACE TABLE finance_emp AS
    SELECT * FROM finance_employees

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'USE'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

USE finance;
^^^
DROP TABLE IF EXISTS finance_employees;
CREATE TABLE finance_employees 
    USING parquet 
    OPTIONS (
        path='/mnt/cntdlt/source/finance_tmp/*.parquet',
        header="true",
        inferSchema="true"
    );
DROP TABLE IF EXISTS finance_emp;
CREATE OR REPLACE TABLE finance_emp AS
    SELECT * FROM finance_employees

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:356)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:166)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:90)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:112)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:346)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:346)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:333)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:392)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:691)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:392)
	... 19 more

[0m12:12:17.333914 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'R.0}2\x99N\xaa\xaa\xe3es\xe9\x88:)'
[0m12:12:17.334907 [debug] [Thread-1 (]: finished collecting timing info
[0m12:12:17.335899 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: ROLLBACK
[0m12:12:17.335899 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:12:17.336895 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: Close
[0m12:12:18.277376 [debug] [Thread-1 (]: Runtime Error in model finance_bronze_to_silver (models\finance_bronze_to_silver.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'USE'(line 18, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */
  
    
      
        create or replace table finance.finance_bronze_to_silver
      
      
      using delta
      
      
      
      
      
      
      as
        
  
  USE finance;
  ^^^
  DROP TABLE IF EXISTS finance_employees;
  CREATE TABLE finance_employees 
      USING parquet 
      OPTIONS (
          path='/mnt/cntdlt/source/finance_tmp/*.parquet',
          header="true",
          inferSchema="true"
      );
  DROP TABLE IF EXISTS finance_emp;
  CREATE OR REPLACE TABLE finance_emp AS
      SELECT * FROM finance_employees
  
[0m12:12:18.278373 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '44f650d5-2b3c-4ca7-9b2a-f8f02d8f3e63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B5D93F6D0>]}
[0m12:12:18.279370 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model finance.finance_bronze_to_silver ......... [[31mERROR[0m in 3.52s]
[0m12:12:18.282290 [debug] [Thread-1 (]: Finished running node model.finance.finance_bronze_to_silver
[0m12:12:18.283305 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m12:12:18.283305 [debug] [MainThread]: On master: ROLLBACK
[0m12:12:18.283305 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:12:19.276790 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:12:19.277776 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:12:19.277776 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:12:19.278710 [debug] [MainThread]: On master: ROLLBACK
[0m12:12:19.278710 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:12:19.279706 [debug] [MainThread]: On master: Close
[0m12:12:20.463348 [info ] [MainThread]: 
[0m12:12:20.464960 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 13.44 seconds (13.44s).
[0m12:12:20.467202 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:12:20.467202 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m12:12:20.467202 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m12:12:20.467202 [debug] [MainThread]: Connection 'model.finance.finance_bronze_to_silver' was properly closed.
[0m12:12:20.475492 [info ] [MainThread]: 
[0m12:12:20.479473 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m12:12:20.480666 [info ] [MainThread]: 
[0m12:12:20.481350 [error] [MainThread]: [33mRuntime Error in model finance_bronze_to_silver (models\finance_bronze_to_silver.sql)[0m
[0m12:12:20.481350 [error] [MainThread]:   
[0m12:12:20.483136 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near 'USE'(line 18, pos 0)
[0m12:12:20.484142 [error] [MainThread]:   
[0m12:12:20.486147 [error] [MainThread]:   == SQL ==
[0m12:12:20.504637 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */
[0m12:12:20.505682 [error] [MainThread]:   
[0m12:12:20.523240 [error] [MainThread]:     
[0m12:12:20.524312 [error] [MainThread]:       
[0m12:12:20.538233 [error] [MainThread]:         create or replace table finance.finance_bronze_to_silver
[0m12:12:20.539579 [error] [MainThread]:       
[0m12:12:20.541504 [error] [MainThread]:       
[0m12:12:20.541504 [error] [MainThread]:       using delta
[0m12:12:20.542507 [error] [MainThread]:       
[0m12:12:20.543500 [error] [MainThread]:       
[0m12:12:20.544718 [error] [MainThread]:       
[0m12:12:20.545714 [error] [MainThread]:       
[0m12:12:20.549046 [error] [MainThread]:       
[0m12:12:20.565223 [error] [MainThread]:       
[0m12:12:20.566142 [error] [MainThread]:       as
[0m12:12:20.580850 [error] [MainThread]:         
[0m12:12:20.597450 [error] [MainThread]:   
[0m12:12:20.599444 [error] [MainThread]:   USE finance;
[0m12:12:20.618377 [error] [MainThread]:   ^^^
[0m12:12:20.619436 [error] [MainThread]:   DROP TABLE IF EXISTS finance_employees;
[0m12:12:20.635873 [error] [MainThread]:   CREATE TABLE finance_employees 
[0m12:12:20.636999 [error] [MainThread]:       USING parquet 
[0m12:12:20.636999 [error] [MainThread]:       OPTIONS (
[0m12:12:20.638004 [error] [MainThread]:           path='/mnt/cntdlt/source/finance_tmp/*.parquet',
[0m12:12:20.638996 [error] [MainThread]:           header="true",
[0m12:12:20.645956 [error] [MainThread]:           inferSchema="true"
[0m12:12:20.660452 [error] [MainThread]:       );
[0m12:12:20.661451 [error] [MainThread]:   DROP TABLE IF EXISTS finance_emp;
[0m12:12:20.678519 [error] [MainThread]:   CREATE OR REPLACE TABLE finance_emp AS
[0m12:12:20.679557 [error] [MainThread]:       SELECT * FROM finance_employees
[0m12:12:20.693612 [error] [MainThread]:   
[0m12:12:20.694684 [info ] [MainThread]: 
[0m12:12:20.712057 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m12:12:20.713949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B5D93F820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B5D7AB670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B5EA19E10>]}
[0m12:12:20.713949 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 12:13:27.666891 | 6f8f1af6-45bf-49f5-b025-66e0bb717438 ==============================
[0m12:13:27.666891 [info ] [MainThread]: Running with dbt=1.3.1
[0m12:13:27.667880 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/finance_bronze_to_silver.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m12:13:27.668803 [debug] [MainThread]: Tracking: tracking
[0m12:13:27.677817 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000112E01A9C60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000112E01A98A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000112E01A9FC0>]}
[0m12:13:27.743339 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:13:27.743339 [debug] [MainThread]: Partial parsing: updated file: finance://models\finance_bronze_to_silver.sql
[0m12:13:27.753773 [debug] [MainThread]: 1699: static parser successfully parsed finance_bronze_to_silver.sql
[0m12:13:27.777509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6f8f1af6-45bf-49f5-b025-66e0bb717438', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000112E137F670>]}
[0m12:13:27.783510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6f8f1af6-45bf-49f5-b025-66e0bb717438', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000112E031E4D0>]}
[0m12:13:27.783510 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m12:13:27.784426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6f8f1af6-45bf-49f5-b025-66e0bb717438', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000112E033DDE0>]}
[0m12:13:27.785839 [info ] [MainThread]: 
[0m12:13:27.786836 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m12:13:27.787812 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m12:13:27.797513 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m12:13:27.798598 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:13:27.799176 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:13:29.846498 [debug] [ThreadPool]: SQL status: OK in 2.05 seconds
[0m12:13:29.856473 [debug] [ThreadPool]: On list_schemas: Close
[0m12:13:31.228613 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m12:13:31.237007 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:13:31.237007 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m12:13:31.237929 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m12:13:31.237929 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:13:34.401486 [debug] [ThreadPool]: SQL status: OK in 3.16 seconds
[0m12:13:34.408513 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m12:13:34.409743 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:13:34.410506 [debug] [ThreadPool]: On list_None_finance: Close
[0m12:13:35.338835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6f8f1af6-45bf-49f5-b025-66e0bb717438', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000112E034FA00>]}
[0m12:13:35.339794 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:13:35.340862 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:13:35.341788 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:13:35.343825 [info ] [MainThread]: 
[0m12:13:35.358032 [debug] [Thread-1 (]: Began running node model.finance.finance_bronze_to_silver
[0m12:13:35.358032 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.finance_bronze_to_silver .................. [RUN]
[0m12:13:35.358875 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.finance_bronze_to_silver"
[0m12:13:35.358875 [debug] [Thread-1 (]: Began compiling node model.finance.finance_bronze_to_silver
[0m12:13:35.358875 [debug] [Thread-1 (]: Compiling model.finance.finance_bronze_to_silver
[0m12:13:35.395129 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.finance_bronze_to_silver"
[0m12:13:35.396132 [debug] [Thread-1 (]: finished collecting timing info
[0m12:13:35.396132 [debug] [Thread-1 (]: Began executing node model.finance.finance_bronze_to_silver
[0m12:13:35.435605 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.finance_bronze_to_silver"
[0m12:13:35.435605 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:13:35.436601 [debug] [Thread-1 (]: Using databricks connection "model.finance.finance_bronze_to_silver"
[0m12:13:35.436601 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS finance.finance_employees;
CREATE TABLE finance.finance_employees 
    USING parquet 
    OPTIONS (
        path='/mnt/cntdlt/source/finance_tmp/*.parquet',
        header="true",
        inferSchema="true"
    );
DROP TABLE IF EXISTS finance.finance_emp;
CREATE OR REPLACE TABLE finance.finance_emp AS
    SELECT * FROM finance.finance_employees;
  
[0m12:13:35.436601 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:13:37.639024 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS finance.finance_employees;
CREATE TABLE finance.finance_employees 
    USING parquet 
    OPTIONS (
        path='/mnt/cntdlt/source/finance_tmp/*.parquet',
        header="true",
        inferSchema="true"
    );
DROP TABLE IF EXISTS finance.finance_emp;
CREATE OR REPLACE TABLE finance.finance_emp AS
    SELECT * FROM finance.finance_employees;
  
[0m12:13:37.640088 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS finance.finance_employees;
^^^
CREATE TABLE finance.finance_employees 
    USING parquet 
    OPTIONS (
        path='/mnt/cntdlt/source/finance_tmp/*.parquet',
        header="true",
        inferSchema="true"
    );
DROP TABLE IF EXISTS finance.finance_emp;
CREATE OR REPLACE TABLE finance.finance_emp AS
    SELECT * FROM finance.finance_employees

[0m12:13:37.641099 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS finance.finance_employees;
^^^
CREATE TABLE finance.finance_employees 
    USING parquet 
    OPTIONS (
        path='/mnt/cntdlt/source/finance_tmp/*.parquet',
        header="true",
        inferSchema="true"
    );
DROP TABLE IF EXISTS finance.finance_emp;
CREATE OR REPLACE TABLE finance.finance_emp AS
    SELECT * FROM finance.finance_employees

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS finance.finance_employees;
^^^
CREATE TABLE finance.finance_employees 
    USING parquet 
    OPTIONS (
        path='/mnt/cntdlt/source/finance_tmp/*.parquet',
        header="true",
        inferSchema="true"
    );
DROP TABLE IF EXISTS finance.finance_emp;
CREATE OR REPLACE TABLE finance.finance_emp AS
    SELECT * FROM finance.finance_employees

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:356)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:166)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:90)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:112)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:346)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:346)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:333)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:392)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:691)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:392)
	... 19 more

[0m12:13:37.641397 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\xff\xb2my\x1e_D\x8d\x86P\xd9}\xc8+\x1bO'
[0m12:13:37.642400 [debug] [Thread-1 (]: finished collecting timing info
[0m12:13:37.642582 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: ROLLBACK
[0m12:13:37.642582 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:13:37.643588 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: Close
[0m12:13:38.603889 [debug] [Thread-1 (]: Runtime Error in model finance_bronze_to_silver (models\finance_bronze_to_silver.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */
  
    
      
        create or replace table finance.finance_bronze_to_silver
      
      
      using delta
      
      
      
      
      
      
      as
        
  
  DROP TABLE IF EXISTS finance.finance_employees;
  ^^^
  CREATE TABLE finance.finance_employees 
      USING parquet 
      OPTIONS (
          path='/mnt/cntdlt/source/finance_tmp/*.parquet',
          header="true",
          inferSchema="true"
      );
  DROP TABLE IF EXISTS finance.finance_emp;
  CREATE OR REPLACE TABLE finance.finance_emp AS
      SELECT * FROM finance.finance_employees
  
[0m12:13:38.604886 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f8f1af6-45bf-49f5-b025-66e0bb717438', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000112E0360820>]}
[0m12:13:38.604886 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model finance.finance_bronze_to_silver ......... [[31mERROR[0m in 3.25s]
[0m12:13:38.607501 [debug] [Thread-1 (]: Finished running node model.finance.finance_bronze_to_silver
[0m12:13:38.608504 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m12:13:38.609741 [debug] [MainThread]: On master: ROLLBACK
[0m12:13:38.609880 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:13:39.564179 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:13:39.564179 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:13:39.565183 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:13:39.566099 [debug] [MainThread]: On master: ROLLBACK
[0m12:13:39.566099 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:13:39.566099 [debug] [MainThread]: On master: Close
[0m12:13:40.525716 [info ] [MainThread]: 
[0m12:13:40.527103 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 12.74 seconds (12.74s).
[0m12:13:40.528107 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:13:40.529102 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m12:13:40.529439 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m12:13:40.530455 [debug] [MainThread]: Connection 'model.finance.finance_bronze_to_silver' was properly closed.
[0m12:13:40.538435 [info ] [MainThread]: 
[0m12:13:40.539432 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m12:13:40.541043 [info ] [MainThread]: 
[0m12:13:40.541043 [error] [MainThread]: [33mRuntime Error in model finance_bronze_to_silver (models\finance_bronze_to_silver.sql)[0m
[0m12:13:40.542230 [error] [MainThread]:   
[0m12:13:40.542784 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)
[0m12:13:40.542784 [error] [MainThread]:   
[0m12:13:40.543783 [error] [MainThread]:   == SQL ==
[0m12:13:40.543783 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */
[0m12:13:40.545667 [error] [MainThread]:   
[0m12:13:40.546666 [error] [MainThread]:     
[0m12:13:40.547198 [error] [MainThread]:       
[0m12:13:40.548239 [error] [MainThread]:         create or replace table finance.finance_bronze_to_silver
[0m12:13:40.548744 [error] [MainThread]:       
[0m12:13:40.549455 [error] [MainThread]:       
[0m12:13:40.549455 [error] [MainThread]:       using delta
[0m12:13:40.550979 [error] [MainThread]:       
[0m12:13:40.550979 [error] [MainThread]:       
[0m12:13:40.551984 [error] [MainThread]:       
[0m12:13:40.551984 [error] [MainThread]:       
[0m12:13:40.553045 [error] [MainThread]:       
[0m12:13:40.553045 [error] [MainThread]:       
[0m12:13:40.558351 [error] [MainThread]:       as
[0m12:13:40.568887 [error] [MainThread]:         
[0m12:13:40.569894 [error] [MainThread]:   
[0m12:13:40.572341 [error] [MainThread]:   DROP TABLE IF EXISTS finance.finance_employees;
[0m12:13:40.587442 [error] [MainThread]:   ^^^
[0m12:13:40.588991 [error] [MainThread]:   CREATE TABLE finance.finance_employees 
[0m12:13:40.605448 [error] [MainThread]:       USING parquet 
[0m12:13:40.606446 [error] [MainThread]:       OPTIONS (
[0m12:13:40.619700 [error] [MainThread]:           path='/mnt/cntdlt/source/finance_tmp/*.parquet',
[0m12:13:40.620618 [error] [MainThread]:           header="true",
[0m12:13:40.639555 [error] [MainThread]:           inferSchema="true"
[0m12:13:40.640481 [error] [MainThread]:       );
[0m12:13:40.653659 [error] [MainThread]:   DROP TABLE IF EXISTS finance.finance_emp;
[0m12:13:40.654662 [error] [MainThread]:   CREATE OR REPLACE TABLE finance.finance_emp AS
[0m12:13:40.657375 [error] [MainThread]:       SELECT * FROM finance.finance_employees
[0m12:13:40.657926 [error] [MainThread]:   
[0m12:13:40.658959 [info ] [MainThread]: 
[0m12:13:40.659923 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m12:13:40.660925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000112E0362800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000112E1447B20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000112E142E260>]}
[0m12:13:40.661917 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 12:14:41.713702 | 21a1b342-5fd6-47e3-b853-477260fdce8b ==============================
[0m12:14:41.713702 [info ] [MainThread]: Running with dbt=1.3.1
[0m12:14:41.713702 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/finance_bronze_to_silver.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m12:14:41.714628 [debug] [MainThread]: Tracking: tracking
[0m12:14:41.725912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801D259C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801D259870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801D259AB0>]}
[0m12:14:41.792342 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:14:41.793377 [debug] [MainThread]: Partial parsing: updated file: finance://models\finance_bronze_to_silver.sql
[0m12:14:41.802833 [debug] [MainThread]: 1699: static parser successfully parsed finance_bronze_to_silver.sql
[0m12:14:41.827406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '21a1b342-5fd6-47e3-b853-477260fdce8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801D45F6D0>]}
[0m12:14:41.832490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '21a1b342-5fd6-47e3-b853-477260fdce8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801D3CE4D0>]}
[0m12:14:41.833519 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m12:14:41.834503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '21a1b342-5fd6-47e3-b853-477260fdce8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801D3EDD80>]}
[0m12:14:41.835499 [info ] [MainThread]: 
[0m12:14:41.836496 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m12:14:41.848759 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m12:14:41.858729 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m12:14:41.858729 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:14:41.858729 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:14:45.488706 [debug] [ThreadPool]: SQL status: OK in 3.63 seconds
[0m12:14:45.498113 [debug] [ThreadPool]: On list_schemas: Close
[0m12:14:46.630845 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m12:14:46.640316 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:14:46.641316 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m12:14:46.641316 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m12:14:46.641316 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:14:48.878393 [debug] [ThreadPool]: SQL status: OK in 2.24 seconds
[0m12:14:48.884953 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m12:14:48.884953 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:14:48.885876 [debug] [ThreadPool]: On list_None_finance: Close
[0m12:14:49.856076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '21a1b342-5fd6-47e3-b853-477260fdce8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801D2453F0>]}
[0m12:14:49.857080 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:14:49.858073 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:14:49.859069 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:14:49.860066 [info ] [MainThread]: 
[0m12:14:49.876164 [debug] [Thread-1 (]: Began running node model.finance.finance_bronze_to_silver
[0m12:14:49.876164 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.finance_bronze_to_silver .................. [RUN]
[0m12:14:49.877163 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.finance_bronze_to_silver"
[0m12:14:49.878165 [debug] [Thread-1 (]: Began compiling node model.finance.finance_bronze_to_silver
[0m12:14:49.878165 [debug] [Thread-1 (]: Compiling model.finance.finance_bronze_to_silver
[0m12:14:49.916286 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.finance_bronze_to_silver"
[0m12:14:49.917213 [debug] [Thread-1 (]: finished collecting timing info
[0m12:14:49.917213 [debug] [Thread-1 (]: Began executing node model.finance.finance_bronze_to_silver
[0m12:14:49.956189 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.finance_bronze_to_silver"
[0m12:14:49.957176 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:14:49.957176 [debug] [Thread-1 (]: Using databricks connection "model.finance.finance_bronze_to_silver"
[0m12:14:49.958101 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

-- DROP TABLE IF EXISTS finance.finance_employees;
CREATE TABLE finance.finance_employees 
    USING parquet 
    OPTIONS (
        path='/mnt/cntdlt/source/finance_tmp/*.parquet',
        header="true",
        inferSchema="true"
    );
-- DROP TABLE IF EXISTS finance.finance_emp;
-- CREATE OR REPLACE TABLE finance.finance_emp AS
--     SELECT * FROM finance.finance_employees;
  
[0m12:14:49.958101 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:14:52.941713 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

-- DROP TABLE IF EXISTS finance.finance_employees;
CREATE TABLE finance.finance_employees 
    USING parquet 
    OPTIONS (
        path='/mnt/cntdlt/source/finance_tmp/*.parquet',
        header="true",
        inferSchema="true"
    );
-- DROP TABLE IF EXISTS finance.finance_emp;
-- CREATE OR REPLACE TABLE finance.finance_emp AS
--     SELECT * FROM finance.finance_employees;
  
[0m12:14:52.942711 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE': extra input 'CREATE'(line 19, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

-- DROP TABLE IF EXISTS finance.finance_employees;
CREATE TABLE finance.finance_employees 
^^^
    USING parquet 
    OPTIONS (
        path='/mnt/cntdlt/source/finance_tmp/*.parquet',
        header="true",
        inferSchema="true"
    );
-- DROP TABLE IF EXISTS finance.finance_emp;
-- CREATE OR REPLACE TABLE finance.finance_emp AS
--     SELECT * FROM finance.finance_employees

[0m12:14:52.942711 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE': extra input 'CREATE'(line 19, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

-- DROP TABLE IF EXISTS finance.finance_employees;
CREATE TABLE finance.finance_employees 
^^^
    USING parquet 
    OPTIONS (
        path='/mnt/cntdlt/source/finance_tmp/*.parquet',
        header="true",
        inferSchema="true"
    );
-- DROP TABLE IF EXISTS finance.finance_emp;
-- CREATE OR REPLACE TABLE finance.finance_emp AS
--     SELECT * FROM finance.finance_employees

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE': extra input 'CREATE'(line 19, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

-- DROP TABLE IF EXISTS finance.finance_employees;
CREATE TABLE finance.finance_employees 
^^^
    USING parquet 
    OPTIONS (
        path='/mnt/cntdlt/source/finance_tmp/*.parquet',
        header="true",
        inferSchema="true"
    );
-- DROP TABLE IF EXISTS finance.finance_emp;
-- CREATE OR REPLACE TABLE finance.finance_emp AS
--     SELECT * FROM finance.finance_employees

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:356)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:166)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:90)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:112)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:346)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:346)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:333)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:392)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:691)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:392)
	... 19 more

[0m12:14:52.943708 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'K\xae\x1a\x9b\xd1\xd2Ds\x8ek\x9b\xc4\xf5/n\xac'
[0m12:14:52.944703 [debug] [Thread-1 (]: finished collecting timing info
[0m12:14:52.944988 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: ROLLBACK
[0m12:14:52.944988 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:14:52.944988 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: Close
[0m12:14:54.207022 [debug] [Thread-1 (]: Runtime Error in model finance_bronze_to_silver (models\finance_bronze_to_silver.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE': extra input 'CREATE'(line 19, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */
  
    
      
        create or replace table finance.finance_bronze_to_silver
      
      
      using delta
      
      
      
      
      
      
      as
        
  
  -- DROP TABLE IF EXISTS finance.finance_employees;
  CREATE TABLE finance.finance_employees 
  ^^^
      USING parquet 
      OPTIONS (
          path='/mnt/cntdlt/source/finance_tmp/*.parquet',
          header="true",
          inferSchema="true"
      );
  -- DROP TABLE IF EXISTS finance.finance_emp;
  -- CREATE OR REPLACE TABLE finance.finance_emp AS
  --     SELECT * FROM finance.finance_employees
  
[0m12:14:54.207951 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '21a1b342-5fd6-47e3-b853-477260fdce8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801D244C70>]}
[0m12:14:54.208937 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model finance.finance_bronze_to_silver ......... [[31mERROR[0m in 4.33s]
[0m12:14:54.210653 [debug] [Thread-1 (]: Finished running node model.finance.finance_bronze_to_silver
[0m12:14:54.211690 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m12:14:54.211690 [debug] [MainThread]: On master: ROLLBACK
[0m12:14:54.212650 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:14:55.245120 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:14:55.246114 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:14:55.247038 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:14:55.247038 [debug] [MainThread]: On master: ROLLBACK
[0m12:14:55.247038 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:14:55.248039 [debug] [MainThread]: On master: Close
[0m12:14:56.423914 [info ] [MainThread]: 
[0m12:14:56.424521 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 14.59 seconds (14.59s).
[0m12:14:56.425449 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:14:56.425449 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m12:14:56.426537 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m12:14:56.426537 [debug] [MainThread]: Connection 'model.finance.finance_bronze_to_silver' was properly closed.
[0m12:14:56.433902 [info ] [MainThread]: 
[0m12:14:56.434352 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m12:14:56.439747 [info ] [MainThread]: 
[0m12:14:56.440455 [error] [MainThread]: [33mRuntime Error in model finance_bronze_to_silver (models\finance_bronze_to_silver.sql)[0m
[0m12:14:56.452484 [error] [MainThread]:   
[0m12:14:56.454521 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE': extra input 'CREATE'(line 19, pos 0)
[0m12:14:56.470742 [error] [MainThread]:   
[0m12:14:56.471541 [error] [MainThread]:   == SQL ==
[0m12:14:56.484271 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */
[0m12:14:56.486534 [error] [MainThread]:   
[0m12:14:56.487247 [error] [MainThread]:     
[0m12:14:56.502965 [error] [MainThread]:       
[0m12:14:56.502965 [error] [MainThread]:         create or replace table finance.finance_bronze_to_silver
[0m12:14:56.520236 [error] [MainThread]:       
[0m12:14:56.520362 [error] [MainThread]:       
[0m12:14:56.537250 [error] [MainThread]:       using delta
[0m12:14:56.538263 [error] [MainThread]:       
[0m12:14:56.554231 [error] [MainThread]:       
[0m12:14:56.555234 [error] [MainThread]:       
[0m12:14:56.568827 [error] [MainThread]:       
[0m12:14:56.570823 [error] [MainThread]:       
[0m12:14:56.587955 [error] [MainThread]:       
[0m12:14:56.589090 [error] [MainThread]:       as
[0m12:14:56.600226 [error] [MainThread]:         
[0m12:14:56.602521 [error] [MainThread]:   
[0m12:14:56.603742 [error] [MainThread]:   -- DROP TABLE IF EXISTS finance.finance_employees;
[0m12:14:56.604770 [error] [MainThread]:   CREATE TABLE finance.finance_employees 
[0m12:14:56.610515 [error] [MainThread]:   ^^^
[0m12:14:56.626061 [error] [MainThread]:       USING parquet 
[0m12:14:56.627070 [error] [MainThread]:       OPTIONS (
[0m12:14:56.644764 [error] [MainThread]:           path='/mnt/cntdlt/source/finance_tmp/*.parquet',
[0m12:14:56.645893 [error] [MainThread]:           header="true",
[0m12:14:56.663585 [error] [MainThread]:           inferSchema="true"
[0m12:14:56.664613 [error] [MainThread]:       );
[0m12:14:56.679742 [error] [MainThread]:   -- DROP TABLE IF EXISTS finance.finance_emp;
[0m12:14:56.680739 [error] [MainThread]:   -- CREATE OR REPLACE TABLE finance.finance_emp AS
[0m12:14:56.682031 [error] [MainThread]:   --     SELECT * FROM finance.finance_employees
[0m12:14:56.682661 [error] [MainThread]:   
[0m12:14:56.683391 [info ] [MainThread]: 
[0m12:14:56.684112 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m12:14:56.684112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801D246200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801D26A2C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801D508970>]}
[0m12:14:56.685119 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:01:44.966652 | eda5eebe-a3a3-461c-b271-f1bfd39e220b ==============================
[0m14:01:44.966652 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:01:44.966652 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/finance_bronze_to_silver.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:01:44.966652 [debug] [MainThread]: Tracking: tracking
[0m14:01:44.984748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024123609C60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241236098A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024123609FC0>]}
[0m14:01:45.669416 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:01:45.669416 [debug] [MainThread]: Partial parsing: updated file: finance://models\finance_bronze_to_silver.sql
[0m14:01:45.676309 [debug] [MainThread]: 1699: static parser successfully parsed finance_bronze_to_silver.sql
[0m14:01:45.714716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'eda5eebe-a3a3-461c-b271-f1bfd39e220b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241247E3670>]}
[0m14:01:45.729533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'eda5eebe-a3a3-461c-b271-f1bfd39e220b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002412377E4D0>]}
[0m14:01:45.729784 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:01:45.729784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eda5eebe-a3a3-461c-b271-f1bfd39e220b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002412379DDE0>]}
[0m14:01:45.729784 [info ] [MainThread]: 
[0m14:01:45.729784 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:01:45.734514 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m14:01:45.745449 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:01:45.746313 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:01:45.746313 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:01:47.626788 [debug] [ThreadPool]: SQL status: OK in 1.88 seconds
[0m14:01:47.630178 [debug] [ThreadPool]: On list_schemas: Close
[0m14:01:48.624858 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:01:48.624858 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:01:48.624858 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:01:48.624858 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:01:48.634694 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:01:50.259443 [debug] [ThreadPool]: SQL status: OK in 1.62 seconds
[0m14:01:50.267384 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:01:50.267384 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:01:50.269074 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:01:51.084866 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eda5eebe-a3a3-461c-b271-f1bfd39e220b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241237AFA00>]}
[0m14:01:51.086473 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:01:51.086473 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:01:51.086473 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:01:51.090446 [info ] [MainThread]: 
[0m14:01:51.134499 [debug] [Thread-1 (]: Began running node model.finance.finance_bronze_to_silver
[0m14:01:51.134499 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.finance_bronze_to_silver .................. [RUN]
[0m14:01:51.134499 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.finance_bronze_to_silver"
[0m14:01:51.134499 [debug] [Thread-1 (]: Began compiling node model.finance.finance_bronze_to_silver
[0m14:01:51.134499 [debug] [Thread-1 (]: Compiling model.finance.finance_bronze_to_silver
[0m14:01:51.185039 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.finance_bronze_to_silver"
[0m14:01:51.185039 [debug] [Thread-1 (]: finished collecting timing info
[0m14:01:51.185039 [debug] [Thread-1 (]: Began executing node model.finance.finance_bronze_to_silver
[0m14:01:51.234758 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.finance_bronze_to_silver"
[0m14:01:51.239232 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:01:51.239232 [debug] [Thread-1 (]: Using databricks connection "model.finance.finance_bronze_to_silver"
[0m14:01:51.239232 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as{
    select * from data.table_finance;
}
  
[0m14:01:51.240625 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:01:52.514549 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as{
    select * from data.table_finance;
}
  
[0m14:01:52.514549 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 16)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as{
----------------^^^
    select * from data.table_finance;
}
  

[0m14:01:52.514549 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 16)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as{
----------------^^^
    select * from data.table_finance;
}
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 16)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as{
----------------^^^
    select * from data.table_finance;
}
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:356)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:166)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:90)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:112)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:346)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:346)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:333)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:392)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:691)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:392)
	... 19 more

[0m14:01:52.514549 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'r^t\t\xbbHEz\x93\xbd\xa4~MaU\x83'
[0m14:01:52.514549 [debug] [Thread-1 (]: finished collecting timing info
[0m14:01:52.514549 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: ROLLBACK
[0m14:01:52.514549 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:01:52.514549 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: Close
[0m14:01:53.134606 [debug] [Thread-1 (]: Runtime Error in model finance_bronze_to_silver (models\finance_bronze_to_silver.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 16)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */
  
    
      
        create or replace table finance.finance_bronze_to_silver
      
      
      using delta
      
      
      
      
      
      
      as
        
  
  with customer as{
  ----------------^^^
      select * from data.table_finance;
  }
    
  
[0m14:01:53.134606 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eda5eebe-a3a3-461c-b271-f1bfd39e220b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241237C4820>]}
[0m14:01:53.134606 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model finance.finance_bronze_to_silver ......... [[31mERROR[0m in 2.00s]
[0m14:01:53.145023 [debug] [Thread-1 (]: Finished running node model.finance.finance_bronze_to_silver
[0m14:01:53.149280 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:01:53.149280 [debug] [MainThread]: On master: ROLLBACK
[0m14:01:53.149280 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:01:53.744615 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:01:53.744615 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:01:53.744615 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:01:53.744615 [debug] [MainThread]: On master: ROLLBACK
[0m14:01:53.744615 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:01:53.744615 [debug] [MainThread]: On master: Close
[0m14:01:54.415202 [info ] [MainThread]: 
[0m14:01:54.415202 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 8.69 seconds (8.69s).
[0m14:01:54.415202 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:01:54.424698 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:01:54.425149 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:01:54.425149 [debug] [MainThread]: Connection 'model.finance.finance_bronze_to_silver' was properly closed.
[0m14:01:54.430882 [info ] [MainThread]: 
[0m14:01:54.436719 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:01:54.438440 [info ] [MainThread]: 
[0m14:01:54.438440 [error] [MainThread]: [33mRuntime Error in model finance_bronze_to_silver (models\finance_bronze_to_silver.sql)[0m
[0m14:01:54.444736 [error] [MainThread]:   
[0m14:01:54.444736 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 16)
[0m14:01:54.444736 [error] [MainThread]:   
[0m14:01:54.444736 [error] [MainThread]:   == SQL ==
[0m14:01:54.454722 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */
[0m14:01:54.471236 [error] [MainThread]:   
[0m14:01:54.471745 [error] [MainThread]:     
[0m14:01:54.484777 [error] [MainThread]:       
[0m14:01:54.484777 [error] [MainThread]:         create or replace table finance.finance_bronze_to_silver
[0m14:01:54.504736 [error] [MainThread]:       
[0m14:01:54.504736 [error] [MainThread]:       
[0m14:01:54.524974 [error] [MainThread]:       using delta
[0m14:01:54.524974 [error] [MainThread]:       
[0m14:01:54.544842 [error] [MainThread]:       
[0m14:01:54.550874 [error] [MainThread]:       
[0m14:01:54.564825 [error] [MainThread]:       
[0m14:01:54.564825 [error] [MainThread]:       
[0m14:01:54.584871 [error] [MainThread]:       
[0m14:01:54.591379 [error] [MainThread]:       as
[0m14:01:54.606243 [error] [MainThread]:         
[0m14:01:54.607307 [error] [MainThread]:   
[0m14:01:54.621686 [error] [MainThread]:   with customer as{
[0m14:01:54.622694 [error] [MainThread]:   ----------------^^^
[0m14:01:54.634649 [error] [MainThread]:       select * from data.table_finance;
[0m14:01:54.634649 [error] [MainThread]:   }
[0m14:01:54.655145 [error] [MainThread]:     
[0m14:01:54.655145 [error] [MainThread]:   
[0m14:01:54.669879 [info ] [MainThread]: 
[0m14:01:54.675153 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m14:01:54.685217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241237C60B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241248AFB20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241248854E0>]}
[0m14:01:54.685217 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:03:24.964292 | 347e9af1-0543-4ff1-8ca3-88dd77c3f9ac ==============================
[0m14:03:24.964292 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:03:24.964292 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/finance_bronze_to_silver.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:03:24.964292 [debug] [MainThread]: Tracking: tracking
[0m14:03:24.984515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F60EB49C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F60EB49870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F60EB49AB0>]}
[0m14:03:25.079673 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:03:25.079673 [debug] [MainThread]: Partial parsing: updated file: finance://models\finance_bronze_to_silver.sql
[0m14:03:25.084529 [debug] [MainThread]: 1603: static parser failed on finance_bronze_to_silver.sql
[0m14:03:25.104657 [debug] [MainThread]: 1602: parser fallback to jinja rendering on finance_bronze_to_silver.sql
[0m14:03:25.127189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '347e9af1-0543-4ff1-8ca3-88dd77c3f9ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F60FD0ACB0>]}
[0m14:03:25.134428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '347e9af1-0543-4ff1-8ca3-88dd77c3f9ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F60ECBE530>]}
[0m14:03:25.134428 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:03:25.134428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '347e9af1-0543-4ff1-8ca3-88dd77c3f9ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F60ECDDDB0>]}
[0m14:03:25.134428 [info ] [MainThread]: 
[0m14:03:25.134428 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:03:25.134428 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m14:03:25.154611 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:03:25.154611 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:03:25.159907 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:03:28.155494 [debug] [ThreadPool]: SQL status: OK in 3.0 seconds
[0m14:03:28.164398 [debug] [ThreadPool]: On list_schemas: Close
[0m14:03:28.804872 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:03:28.819437 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:03:28.819437 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:03:28.819437 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:03:28.820514 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:03:30.184613 [debug] [ThreadPool]: SQL status: OK in 1.36 seconds
[0m14:03:30.194688 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:03:30.194688 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:03:30.199315 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:03:30.974719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '347e9af1-0543-4ff1-8ca3-88dd77c3f9ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F60FD750F0>]}
[0m14:03:30.974719 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:03:30.974719 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:03:30.974719 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:03:30.982922 [info ] [MainThread]: 
[0m14:03:31.040946 [debug] [Thread-1 (]: Began running node model.finance.finance_bronze_to_silver
[0m14:03:31.040946 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.finance_bronze_to_silver .................. [RUN]
[0m14:03:31.040946 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.finance_bronze_to_silver"
[0m14:03:31.040946 [debug] [Thread-1 (]: Began compiling node model.finance.finance_bronze_to_silver
[0m14:03:31.040946 [debug] [Thread-1 (]: Compiling model.finance.finance_bronze_to_silver
[0m14:03:31.044484 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.finance_bronze_to_silver"
[0m14:03:31.044484 [debug] [Thread-1 (]: finished collecting timing info
[0m14:03:31.049124 [debug] [Thread-1 (]: Began executing node model.finance.finance_bronze_to_silver
[0m14:03:31.099787 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.finance_bronze_to_silver"
[0m14:03:31.099787 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:03:31.099787 [debug] [Thread-1 (]: Using databricks connection "model.finance.finance_bronze_to_silver"
[0m14:03:31.099787 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as{
    select * from data.table_finance;
}
  
[0m14:03:31.099787 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:03:33.194457 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as{
    select * from data.table_finance;
}
  
[0m14:03:33.194457 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 16)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as{
----------------^^^
    select * from data.table_finance;
}
  

[0m14:03:33.194457 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 16)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as{
----------------^^^
    select * from data.table_finance;
}
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 16)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as{
----------------^^^
    select * from data.table_finance;
}
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:356)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:166)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:90)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:112)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:346)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:346)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:333)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:392)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:691)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:392)
	... 19 more

[0m14:03:33.198191 [debug] [Thread-1 (]: Databricks adapter: operation-id: b' \xa1\xd1\x1b\x9f\xabG\\\xa7\x02\x82\x9d\xa6Fp\xb8'
[0m14:03:33.198416 [debug] [Thread-1 (]: finished collecting timing info
[0m14:03:33.198762 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: ROLLBACK
[0m14:03:33.198904 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:03:33.198904 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: Close
[0m14:03:34.194727 [debug] [Thread-1 (]: Runtime Error in model finance_bronze_to_silver (models\finance_bronze_to_silver.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 16)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */
  
    
      
        create or replace table finance.finance_bronze_to_silver
      
      
      using delta
      
      
      
      
      
      
      as
        
  
  with customer as{
  ----------------^^^
      select * from data.table_finance;
  }
    
  
[0m14:03:34.194727 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '347e9af1-0543-4ff1-8ca3-88dd77c3f9ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F67F30DBA0>]}
[0m14:03:34.194727 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model finance.finance_bronze_to_silver ......... [[31mERROR[0m in 3.15s]
[0m14:03:34.194727 [debug] [Thread-1 (]: Finished running node model.finance.finance_bronze_to_silver
[0m14:03:34.204671 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:03:34.207704 [debug] [MainThread]: On master: ROLLBACK
[0m14:03:34.207704 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:03:34.814711 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:03:34.814711 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:03:34.814711 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:03:34.814711 [debug] [MainThread]: On master: ROLLBACK
[0m14:03:34.814711 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:03:34.814711 [debug] [MainThread]: On master: Close
[0m14:03:35.434452 [info ] [MainThread]: 
[0m14:03:35.434452 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 10.30 seconds (10.30s).
[0m14:03:35.434452 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:03:35.434452 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:03:35.434452 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:03:35.434452 [debug] [MainThread]: Connection 'model.finance.finance_bronze_to_silver' was properly closed.
[0m14:03:35.447992 [info ] [MainThread]: 
[0m14:03:35.447992 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:03:35.454501 [info ] [MainThread]: 
[0m14:03:35.454501 [error] [MainThread]: [33mRuntime Error in model finance_bronze_to_silver (models\finance_bronze_to_silver.sql)[0m
[0m14:03:35.458069 [error] [MainThread]:   
[0m14:03:35.458069 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 16)
[0m14:03:35.474435 [error] [MainThread]:   
[0m14:03:35.481607 [error] [MainThread]:   == SQL ==
[0m14:03:35.494642 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */
[0m14:03:35.494642 [error] [MainThread]:   
[0m14:03:35.515044 [error] [MainThread]:     
[0m14:03:35.521818 [error] [MainThread]:       
[0m14:03:35.534681 [error] [MainThread]:         create or replace table finance.finance_bronze_to_silver
[0m14:03:35.534681 [error] [MainThread]:       
[0m14:03:35.534681 [error] [MainThread]:       
[0m14:03:35.534681 [error] [MainThread]:       using delta
[0m14:03:35.547957 [error] [MainThread]:       
[0m14:03:35.554757 [error] [MainThread]:       
[0m14:03:35.564457 [error] [MainThread]:       
[0m14:03:35.564457 [error] [MainThread]:       
[0m14:03:35.584871 [error] [MainThread]:       
[0m14:03:35.584871 [error] [MainThread]:       
[0m14:03:35.604431 [error] [MainThread]:       as
[0m14:03:35.604431 [error] [MainThread]:         
[0m14:03:35.627731 [error] [MainThread]:   
[0m14:03:35.628750 [error] [MainThread]:   with customer as{
[0m14:03:35.642017 [error] [MainThread]:   ----------------^^^
[0m14:03:35.644102 [error] [MainThread]:       select * from data.table_finance;
[0m14:03:35.645648 [error] [MainThread]:   }
[0m14:03:35.648909 [error] [MainThread]:     
[0m14:03:35.650440 [error] [MainThread]:   
[0m14:03:35.666398 [info ] [MainThread]: 
[0m14:03:35.666398 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m14:03:35.684455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F67946EE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F60FD77580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F60FDE31C0>]}
[0m14:03:35.684455 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:07:11.514609 | aabc8387-274f-432e-8ec7-1e8c2b4d86ae ==============================
[0m14:07:11.514609 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:07:11.514609 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/finance_bronze_to_silver.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:07:11.517203 [debug] [MainThread]: Tracking: tracking
[0m14:07:11.531444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000172DF279C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000172DF279870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000172DF279AB0>]}
[0m14:07:11.619056 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:07:11.619056 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:07:11.628406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'aabc8387-274f-432e-8ec7-1e8c2b4d86ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000172DF45C220>]}
[0m14:07:11.634302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'aabc8387-274f-432e-8ec7-1e8c2b4d86ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000172DF434610>]}
[0m14:07:11.639873 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:07:11.639873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aabc8387-274f-432e-8ec7-1e8c2b4d86ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000172DF434A00>]}
[0m14:07:11.641457 [info ] [MainThread]: 
[0m14:07:11.644014 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:07:11.654213 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m14:07:11.669108 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:07:11.669108 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:07:11.669108 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:07:14.680220 [debug] [ThreadPool]: SQL status: OK in 3.01 seconds
[0m14:07:14.689015 [debug] [ThreadPool]: On list_schemas: Close
[0m14:07:15.817981 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:07:15.826797 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:07:15.826797 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:07:15.827914 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:07:15.828930 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:07:17.435594 [debug] [ThreadPool]: SQL status: OK in 1.61 seconds
[0m14:07:17.436402 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:07:17.436402 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:07:17.436402 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:07:18.083941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aabc8387-274f-432e-8ec7-1e8c2b4d86ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000172DF434490>]}
[0m14:07:18.086525 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:07:18.088151 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:07:18.088646 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:07:18.090254 [info ] [MainThread]: 
[0m14:07:18.103912 [debug] [Thread-1 (]: Began running node model.finance.finance_bronze_to_silver
[0m14:07:18.103912 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.finance_bronze_to_silver .................. [RUN]
[0m14:07:18.111563 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.finance_bronze_to_silver"
[0m14:07:18.111563 [debug] [Thread-1 (]: Began compiling node model.finance.finance_bronze_to_silver
[0m14:07:18.111563 [debug] [Thread-1 (]: Compiling model.finance.finance_bronze_to_silver
[0m14:07:18.115256 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.finance_bronze_to_silver"
[0m14:07:18.115256 [debug] [Thread-1 (]: finished collecting timing info
[0m14:07:18.119660 [debug] [Thread-1 (]: Began executing node model.finance.finance_bronze_to_silver
[0m14:07:18.169032 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.finance_bronze_to_silver"
[0m14:07:18.169032 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:07:18.169032 [debug] [Thread-1 (]: Using databricks connection "model.finance.finance_bronze_to_silver"
[0m14:07:18.172300 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as{
    select * from data.table_finance;
}
  
[0m14:07:18.172300 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:07:19.684094 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as{
    select * from data.table_finance;
}
  
[0m14:07:19.684094 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 16)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as{
----------------^^^
    select * from data.table_finance;
}
  

[0m14:07:19.684094 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 16)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as{
----------------^^^
    select * from data.table_finance;
}
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 16)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as{
----------------^^^
    select * from data.table_finance;
}
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:356)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:166)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:90)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:112)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:346)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:346)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:333)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:392)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:691)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:392)
	... 19 more

[0m14:07:19.684094 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'X\x89k\xb6twJ\xe4\xa5\x9d\xa0\xa12\x88\xdfW'
[0m14:07:19.684094 [debug] [Thread-1 (]: finished collecting timing info
[0m14:07:19.684094 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: ROLLBACK
[0m14:07:19.684094 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:07:19.684094 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: Close
[0m14:07:20.623872 [debug] [Thread-1 (]: Runtime Error in model finance_bronze_to_silver (models\finance_bronze_to_silver.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 16)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */
  
    
      
        create or replace table finance.finance_bronze_to_silver
      
      
      using delta
      
      
      
      
      
      
      as
        
  
  with customer as{
  ----------------^^^
      select * from data.table_finance;
  }
    
  
[0m14:07:20.623872 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aabc8387-274f-432e-8ec7-1e8c2b4d86ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000172E04C0E80>]}
[0m14:07:20.623872 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model finance.finance_bronze_to_silver ......... [[31mERROR[0m in 2.51s]
[0m14:07:20.623872 [debug] [Thread-1 (]: Finished running node model.finance.finance_bronze_to_silver
[0m14:07:20.623872 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:07:20.623872 [debug] [MainThread]: On master: ROLLBACK
[0m14:07:20.623872 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:07:21.344110 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:07:21.344110 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:07:21.344110 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:07:21.344110 [debug] [MainThread]: On master: ROLLBACK
[0m14:07:21.344110 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:07:21.344110 [debug] [MainThread]: On master: Close
[0m14:07:22.045784 [info ] [MainThread]: 
[0m14:07:22.045784 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 10.40 seconds (10.40s).
[0m14:07:22.051421 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:07:22.051421 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:07:22.054071 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:07:22.054071 [debug] [MainThread]: Connection 'model.finance.finance_bronze_to_silver' was properly closed.
[0m14:07:22.073930 [info ] [MainThread]: 
[0m14:07:22.075621 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:07:22.075621 [info ] [MainThread]: 
[0m14:07:22.135988 [error] [MainThread]: [33mRuntime Error in model finance_bronze_to_silver (models\finance_bronze_to_silver.sql)[0m
[0m14:07:22.138028 [error] [MainThread]:   
[0m14:07:22.190078 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 16)
[0m14:07:22.192246 [error] [MainThread]:   
[0m14:07:22.241926 [error] [MainThread]:   == SQL ==
[0m14:07:22.243040 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */
[0m14:07:22.295531 [error] [MainThread]:   
[0m14:07:22.297309 [error] [MainThread]:     
[0m14:07:22.350022 [error] [MainThread]:       
[0m14:07:22.352353 [error] [MainThread]:         create or replace table finance.finance_bronze_to_silver
[0m14:07:22.388463 [error] [MainThread]:       
[0m14:07:22.442572 [error] [MainThread]:       
[0m14:07:22.468171 [error] [MainThread]:       using delta
[0m14:07:22.470323 [error] [MainThread]:       
[0m14:07:22.522643 [error] [MainThread]:       
[0m14:07:22.524149 [error] [MainThread]:       
[0m14:07:22.572759 [error] [MainThread]:       
[0m14:07:22.574904 [error] [MainThread]:       
[0m14:07:22.628948 [error] [MainThread]:       
[0m14:07:22.704366 [error] [MainThread]:       as
[0m14:07:22.751563 [error] [MainThread]:         
[0m14:07:22.802855 [error] [MainThread]:   
[0m14:07:22.804891 [error] [MainThread]:   with customer as{
[0m14:07:22.852756 [error] [MainThread]:   ----------------^^^
[0m14:07:22.854728 [error] [MainThread]:       select * from data.table_finance;
[0m14:07:22.904728 [error] [MainThread]:   }
[0m14:07:22.954617 [error] [MainThread]:     
[0m14:07:22.956919 [error] [MainThread]:   
[0m14:07:23.006981 [info ] [MainThread]: 
[0m14:07:23.008476 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m14:07:23.056486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000172DF288250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000172DF434280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000172E04B9510>]}
[0m14:07:23.058532 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:07:46.984333 | 5028a569-2939-4dbe-bf2a-aed741603165 ==============================
[0m14:07:46.984333 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:07:46.984333 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/finance_bronze_to_silver.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:07:46.984333 [debug] [MainThread]: Tracking: tracking
[0m14:07:47.003749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E276B29C60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E276B298A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E276B29FC0>]}
[0m14:07:47.085789 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:07:47.085789 [debug] [MainThread]: Partial parsing: updated file: finance://models\finance_bronze_to_silver.sql
[0m14:07:47.099068 [debug] [MainThread]: 1603: static parser failed on finance_bronze_to_silver.sql
[0m14:07:47.117762 [debug] [MainThread]: 1602: parser fallback to jinja rendering on finance_bronze_to_silver.sql
[0m14:07:47.133878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5028a569-2939-4dbe-bf2a-aed741603165', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E277CEACE0>]}
[0m14:07:47.133878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5028a569-2939-4dbe-bf2a-aed741603165', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E277C6E530>]}
[0m14:07:47.143690 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:07:47.143690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5028a569-2939-4dbe-bf2a-aed741603165', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E277C8DDE0>]}
[0m14:07:47.143690 [info ] [MainThread]: 
[0m14:07:47.143690 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:07:47.154055 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m14:07:47.173806 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:07:47.173806 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:07:47.173806 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:07:49.374295 [debug] [ThreadPool]: SQL status: OK in 2.2 seconds
[0m14:07:49.386813 [debug] [ThreadPool]: On list_schemas: Close
[0m14:07:50.244123 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:07:50.254055 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:07:50.254055 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:07:50.254055 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:07:50.254055 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:07:52.374267 [debug] [ThreadPool]: SQL status: OK in 2.12 seconds
[0m14:07:52.383817 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:07:52.383817 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:07:52.383817 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:07:53.803564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5028a569-2939-4dbe-bf2a-aed741603165', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E277D4D0F0>]}
[0m14:07:53.808620 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:07:53.808620 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:07:53.809690 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:07:53.809690 [info ] [MainThread]: 
[0m14:07:53.874599 [debug] [Thread-1 (]: Began running node model.finance.finance_bronze_to_silver
[0m14:07:53.874599 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.finance_bronze_to_silver .................. [RUN]
[0m14:07:53.874599 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.finance_bronze_to_silver"
[0m14:07:53.874599 [debug] [Thread-1 (]: Began compiling node model.finance.finance_bronze_to_silver
[0m14:07:53.874599 [debug] [Thread-1 (]: Compiling model.finance.finance_bronze_to_silver
[0m14:07:53.884245 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.finance_bronze_to_silver"
[0m14:07:53.884245 [debug] [Thread-1 (]: finished collecting timing info
[0m14:07:53.884245 [debug] [Thread-1 (]: Began executing node model.finance.finance_bronze_to_silver
[0m14:07:53.933671 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.finance_bronze_to_silver"
[0m14:07:53.944316 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:07:53.944914 [debug] [Thread-1 (]: Using databricks connection "model.finance.finance_bronze_to_silver"
[0m14:07:53.945451 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as {
    select * from data.table_finance;
}
  
[0m14:07:53.945451 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:07:55.543893 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as {
    select * from data.table_finance;
}
  
[0m14:07:55.543893 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 17)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as {
-----------------^^^
    select * from data.table_finance;
}
  

[0m14:07:55.543893 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 17)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as {
-----------------^^^
    select * from data.table_finance;
}
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 17)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

with customer as {
-----------------^^^
    select * from data.table_finance;
}
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:356)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:166)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:90)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:112)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:346)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:346)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:333)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:392)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:691)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:392)
	... 19 more

[0m14:07:55.550286 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\xd5~\x83\x12\xf1iM!\xa8 \x95\xba r\xe9\xf4'
[0m14:07:55.550678 [debug] [Thread-1 (]: finished collecting timing info
[0m14:07:55.550678 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: ROLLBACK
[0m14:07:55.552679 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:07:55.552960 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: Close
[0m14:07:56.364193 [debug] [Thread-1 (]: Runtime Error in model finance_bronze_to_silver (models\finance_bronze_to_silver.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 17)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */
  
    
      
        create or replace table finance.finance_bronze_to_silver
      
      
      using delta
      
      
      
      
      
      
      as
        
  
  with customer as {
  -----------------^^^
      select * from data.table_finance;
  }
    
  
[0m14:07:56.364193 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5028a569-2939-4dbe-bf2a-aed741603165', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E276B202E0>]}
[0m14:07:56.364193 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model finance.finance_bronze_to_silver ......... [[31mERROR[0m in 2.49s]
[0m14:07:56.364193 [debug] [Thread-1 (]: Finished running node model.finance.finance_bronze_to_silver
[0m14:07:56.374655 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:07:56.374655 [debug] [MainThread]: On master: ROLLBACK
[0m14:07:56.374655 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:07:57.063969 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:07:57.063969 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:07:57.063969 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:07:57.063969 [debug] [MainThread]: On master: ROLLBACK
[0m14:07:57.063969 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:07:57.063969 [debug] [MainThread]: On master: Close
[0m14:07:58.395457 [info ] [MainThread]: 
[0m14:07:58.395457 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 11.25 seconds (11.25s).
[0m14:07:58.400338 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:07:58.400338 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:07:58.403954 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:07:58.403954 [debug] [MainThread]: Connection 'model.finance.finance_bronze_to_silver' was properly closed.
[0m14:07:58.403954 [info ] [MainThread]: 
[0m14:07:58.413629 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:07:58.413629 [info ] [MainThread]: 
[0m14:07:58.433913 [error] [MainThread]: [33mRuntime Error in model finance_bronze_to_silver (models\finance_bronze_to_silver.sql)[0m
[0m14:07:58.437758 [error] [MainThread]:   
[0m14:07:58.455361 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near '{'(line 18, pos 17)
[0m14:07:58.455361 [error] [MainThread]:   
[0m14:07:58.474171 [error] [MainThread]:   == SQL ==
[0m14:07:58.474171 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */
[0m14:07:58.493882 [error] [MainThread]:   
[0m14:07:58.493882 [error] [MainThread]:     
[0m14:07:58.504230 [error] [MainThread]:       
[0m14:07:58.504230 [error] [MainThread]:         create or replace table finance.finance_bronze_to_silver
[0m14:07:58.514075 [error] [MainThread]:       
[0m14:07:58.514075 [error] [MainThread]:       
[0m14:07:58.534090 [error] [MainThread]:       using delta
[0m14:07:58.534090 [error] [MainThread]:       
[0m14:07:58.553944 [error] [MainThread]:       
[0m14:07:58.553944 [error] [MainThread]:       
[0m14:07:58.576070 [error] [MainThread]:       
[0m14:07:58.576070 [error] [MainThread]:       
[0m14:07:58.600123 [error] [MainThread]:       
[0m14:07:58.601756 [error] [MainThread]:       as
[0m14:07:58.618810 [error] [MainThread]:         
[0m14:07:58.620828 [error] [MainThread]:   
[0m14:07:58.623169 [error] [MainThread]:   with customer as {
[0m14:07:58.624193 [error] [MainThread]:   -----------------^^^
[0m14:07:58.637011 [error] [MainThread]:       select * from data.table_finance;
[0m14:07:58.637011 [error] [MainThread]:   }
[0m14:07:58.654248 [error] [MainThread]:     
[0m14:07:58.663919 [error] [MainThread]:   
[0m14:07:58.680597 [info ] [MainThread]: 
[0m14:07:58.682614 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m14:07:58.683625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26158EE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26528C400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E277DC1960>]}
[0m14:07:58.683625 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:08:28.133773 | 5c0464f5-151b-4c61-bf9e-52315296d3c5 ==============================
[0m14:08:28.133773 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:08:28.133773 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/finance_bronze_to_silver.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:08:28.143937 [debug] [MainThread]: Tracking: tracking
[0m14:08:28.158619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B0A219C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B0A219870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B0A219AB0>]}
[0m14:08:28.243721 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:08:28.250988 [debug] [MainThread]: Partial parsing: updated file: finance://models\finance_bronze_to_silver.sql
[0m14:08:28.258736 [debug] [MainThread]: 1699: static parser successfully parsed finance_bronze_to_silver.sql
[0m14:08:28.283920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5c0464f5-151b-4c61-bf9e-52315296d3c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B0B3F36A0>]}
[0m14:08:28.293693 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5c0464f5-151b-4c61-bf9e-52315296d3c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B0A38E4D0>]}
[0m14:08:28.293693 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:08:28.301184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5c0464f5-151b-4c61-bf9e-52315296d3c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B0A3ADDB0>]}
[0m14:08:28.303722 [info ] [MainThread]: 
[0m14:08:28.306123 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:08:28.313917 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m14:08:28.323629 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:08:28.323629 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:08:28.328576 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:08:29.820181 [debug] [ThreadPool]: SQL status: OK in 1.49 seconds
[0m14:08:29.823843 [debug] [ThreadPool]: On list_schemas: Close
[0m14:08:30.463812 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:08:30.473618 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:08:30.473618 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:08:30.473618 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:08:30.482781 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:08:31.873838 [debug] [ThreadPool]: SQL status: OK in 1.39 seconds
[0m14:08:31.878846 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:08:31.878846 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:08:31.878846 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:08:32.603860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5c0464f5-151b-4c61-bf9e-52315296d3c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B0A206710>]}
[0m14:08:32.603860 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:08:32.603860 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:08:32.608129 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:08:32.608129 [info ] [MainThread]: 
[0m14:08:32.623773 [debug] [Thread-1 (]: Began running node model.finance.finance_bronze_to_silver
[0m14:08:32.623773 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.finance_bronze_to_silver .................. [RUN]
[0m14:08:32.628372 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.finance_bronze_to_silver"
[0m14:08:32.630424 [debug] [Thread-1 (]: Began compiling node model.finance.finance_bronze_to_silver
[0m14:08:32.630922 [debug] [Thread-1 (]: Compiling model.finance.finance_bronze_to_silver
[0m14:08:32.674049 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.finance_bronze_to_silver"
[0m14:08:32.676968 [debug] [Thread-1 (]: finished collecting timing info
[0m14:08:32.676968 [debug] [Thread-1 (]: Began executing node model.finance.finance_bronze_to_silver
[0m14:08:32.713673 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.finance_bronze_to_silver"
[0m14:08:32.724909 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:08:32.725334 [debug] [Thread-1 (]: Using databricks connection "model.finance.finance_bronze_to_silver"
[0m14:08:32.725850 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

select * from data.table_finance;
  
[0m14:08:32.726236 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:08:37.823853 [debug] [Thread-1 (]: SQL status: OK in 5.1 seconds
[0m14:08:37.839068 [debug] [Thread-1 (]: finished collecting timing info
[0m14:08:37.839068 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: ROLLBACK
[0m14:08:37.839068 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:08:37.839068 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: Close
[0m14:08:38.467014 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5c0464f5-151b-4c61-bf9e-52315296d3c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B0A22AEC0>]}
[0m14:08:38.467014 [info ] [Thread-1 (]: 1 of 1 OK created sql table model finance.finance_bronze_to_silver ............. [[32mOK[0m in 5.84s]
[0m14:08:38.484051 [debug] [Thread-1 (]: Finished running node model.finance.finance_bronze_to_silver
[0m14:08:38.495128 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:08:38.495128 [debug] [MainThread]: On master: ROLLBACK
[0m14:08:38.495128 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:08:40.723934 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:08:40.723934 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:08:40.723934 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:08:40.723934 [debug] [MainThread]: On master: ROLLBACK
[0m14:08:40.723934 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:08:40.723934 [debug] [MainThread]: On master: Close
[0m14:08:41.385216 [info ] [MainThread]: 
[0m14:08:41.385216 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 13.08 seconds (13.08s).
[0m14:08:41.385216 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:08:41.385216 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:08:41.385216 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:08:41.385216 [debug] [MainThread]: Connection 'model.finance.finance_bronze_to_silver' was properly closed.
[0m14:08:41.400002 [info ] [MainThread]: 
[0m14:08:41.403808 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:08:41.403808 [info ] [MainThread]: 
[0m14:08:41.416977 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m14:08:41.423984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B0A22A680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B0A22AEF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B0B445420>]}
[0m14:08:41.423984 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:14:29.252727 | 1bb737e4-0b72-4011-b0a4-144adbb4a685 ==============================
[0m14:14:29.252727 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:14:29.252727 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/finance_bronze_to_silver.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:14:29.252727 [debug] [MainThread]: Tracking: tracking
[0m14:14:29.266634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240D5409C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240D5409870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240D5409AB0>]}
[0m14:14:29.362764 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:14:29.362764 [debug] [MainThread]: Partial parsing: updated file: finance://models\finance_bronze_to_silver.sql
[0m14:14:29.372964 [debug] [MainThread]: 1699: static parser successfully parsed finance_bronze_to_silver.sql
[0m14:14:29.406962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1bb737e4-0b72-4011-b0a4-144adbb4a685', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240D65DF6A0>]}
[0m14:14:29.413393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1bb737e4-0b72-4011-b0a4-144adbb4a685', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240D557E4D0>]}
[0m14:14:29.413393 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:14:29.413393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1bb737e4-0b72-4011-b0a4-144adbb4a685', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240D559DDB0>]}
[0m14:14:29.413393 [info ] [MainThread]: 
[0m14:14:29.423044 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:14:29.433102 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m14:14:29.442979 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:14:29.447997 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:14:29.447997 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:14:36.322936 [debug] [ThreadPool]: SQL status: OK in 6.87 seconds
[0m14:14:36.329473 [debug] [ThreadPool]: On list_schemas: Close
[0m14:14:39.192702 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:14:39.203127 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:14:39.203127 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:14:39.203127 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:14:39.203127 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:14:41.342707 [debug] [ThreadPool]: SQL status: OK in 2.14 seconds
[0m14:14:41.352893 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:14:41.352893 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:14:41.352893 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:14:43.183177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1bb737e4-0b72-4011-b0a4-144adbb4a685', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240D53C6710>]}
[0m14:14:43.183177 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:14:43.183177 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:14:43.183177 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:14:43.183177 [info ] [MainThread]: 
[0m14:14:43.202749 [debug] [Thread-1 (]: Began running node model.finance.finance_bronze_to_silver
[0m14:14:43.202749 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.finance_bronze_to_silver .................. [RUN]
[0m14:14:43.213061 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.finance_bronze_to_silver"
[0m14:14:43.213061 [debug] [Thread-1 (]: Began compiling node model.finance.finance_bronze_to_silver
[0m14:14:43.213061 [debug] [Thread-1 (]: Compiling model.finance.finance_bronze_to_silver
[0m14:14:43.252825 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.finance_bronze_to_silver"
[0m14:14:43.260570 [debug] [Thread-1 (]: finished collecting timing info
[0m14:14:43.260783 [debug] [Thread-1 (]: Began executing node model.finance.finance_bronze_to_silver
[0m14:14:43.308933 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.finance_bronze_to_silver"
[0m14:14:43.308933 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:14:43.308933 [debug] [Thread-1 (]: Using databricks connection "model.finance.finance_bronze_to_silver"
[0m14:14:43.312039 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.finance_bronze_to_silver"} */

  
    
      create or replace table finance.finance_bronze_to_silver
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual,COGS, Inventory, Segment from data.table_finance;
  
[0m14:14:43.312664 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:14:51.592732 [debug] [Thread-1 (]: SQL status: OK in 8.28 seconds
[0m14:14:51.610416 [debug] [Thread-1 (]: finished collecting timing info
[0m14:14:51.611025 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: ROLLBACK
[0m14:14:51.611519 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:14:51.611757 [debug] [Thread-1 (]: On model.finance.finance_bronze_to_silver: Close
[0m14:14:53.018445 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1bb737e4-0b72-4011-b0a4-144adbb4a685', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240D55EA140>]}
[0m14:14:53.018445 [info ] [Thread-1 (]: 1 of 1 OK created sql table model finance.finance_bronze_to_silver ............. [[32mOK[0m in 9.81s]
[0m14:14:53.023038 [debug] [Thread-1 (]: Finished running node model.finance.finance_bronze_to_silver
[0m14:14:53.023038 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:14:53.026894 [debug] [MainThread]: On master: ROLLBACK
[0m14:14:53.027510 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:14:56.326130 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:14:56.326640 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:14:56.327634 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:14:56.329041 [debug] [MainThread]: On master: ROLLBACK
[0m14:14:56.329537 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:14:56.330037 [debug] [MainThread]: On master: Close
[0m14:14:57.742752 [info ] [MainThread]: 
[0m14:14:57.742752 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 28.33 seconds (28.33s).
[0m14:14:57.742752 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:14:57.742752 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:14:57.742752 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:14:57.742752 [debug] [MainThread]: Connection 'model.finance.finance_bronze_to_silver' was properly closed.
[0m14:14:57.758099 [info ] [MainThread]: 
[0m14:14:57.758099 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:14:57.758099 [info ] [MainThread]: 
[0m14:14:57.763063 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m14:14:57.773827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240D66356F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240D6634310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240D6637190>]}
[0m14:14:57.773827 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:20:48.287140 | d2f284ae-f426-4921-8187-ad251083ed00 ==============================
[0m14:20:48.287140 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:20:48.291664 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/dim_ex[.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:20:48.291664 [debug] [MainThread]: Tracking: tracking
[0m14:20:48.307531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027891D69C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027891D69900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027891D6B340>]}
[0m14:20:48.402166 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 5 files added, 0 files changed.
[0m14:20:48.402166 [debug] [MainThread]: Partial parsing: added file: finance://models\dim_month.sql
[0m14:20:48.402166 [debug] [MainThread]: Partial parsing: added file: finance://models\dim_unit.sql
[0m14:20:48.402166 [debug] [MainThread]: Partial parsing: added file: finance://models\dim_info.sql
[0m14:20:48.402166 [debug] [MainThread]: Partial parsing: added file: finance://models\dim_profit.sql
[0m14:20:48.402166 [debug] [MainThread]: Partial parsing: added file: finance://models\dim_exp.sql
[0m14:20:48.402166 [debug] [MainThread]: Partial parsing: deleted file: finance://models\finance_bronze_to_silver.sql
[0m14:20:48.422105 [debug] [MainThread]: 1699: static parser successfully parsed dim_month.sql
[0m14:20:48.442029 [debug] [MainThread]: 1699: static parser successfully parsed dim_unit.sql
[0m14:20:48.442029 [debug] [MainThread]: 1699: static parser successfully parsed dim_info.sql
[0m14:20:48.442029 [debug] [MainThread]: 1699: static parser successfully parsed dim_profit.sql
[0m14:20:48.442029 [debug] [MainThread]: 1699: static parser successfully parsed dim_exp.sql
[0m14:20:48.472066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd2f284ae-f426-4921-8187-ad251083ed00', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027891F4F610>]}
[0m14:20:48.487182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd2f284ae-f426-4921-8187-ad251083ed00', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027891EDE4D0>]}
[0m14:20:48.490904 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:20:48.491935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd2f284ae-f426-4921-8187-ad251083ed00', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027891EFDD50>]}
[0m14:20:48.491935 [warn ] [MainThread]: The selection criterion 'models/dim_ex[.sql' does not match any nodes
[0m14:20:48.497199 [info ] [MainThread]: 
[0m14:20:48.503857 [warn ] [MainThread]: [[33mWARNING[0m]: Nothing to do. Try checking your model configs and model specification args
[0m14:20:48.529119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027891D69870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027891F0FFA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027891F0FE20>]}
[0m14:20:48.529119 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:20:59.261712 | 0a9a4a05-25fc-4422-8d74-673c99f329f0 ==============================
[0m14:20:59.261712 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:20:59.261712 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/dim_exp.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:20:59.265611 [debug] [MainThread]: Tracking: tracking
[0m14:20:59.274934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013B741B9C60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013B741B98A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013B741B9FC0>]}
[0m14:20:59.361915 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:20:59.361915 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:20:59.371941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0a9a4a05-25fc-4422-8d74-673c99f329f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013B75371180>]}
[0m14:20:59.381777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0a9a4a05-25fc-4422-8d74-673c99f329f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013B74379570>]}
[0m14:20:59.381777 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:20:59.381777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0a9a4a05-25fc-4422-8d74-673c99f329f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013B74379960>]}
[0m14:20:59.381777 [info ] [MainThread]: 
[0m14:20:59.392032 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:20:59.402004 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m14:20:59.421875 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:20:59.421875 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:20:59.424365 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:21:01.742309 [debug] [ThreadPool]: SQL status: OK in 2.32 seconds
[0m14:21:01.761949 [debug] [ThreadPool]: On list_schemas: Close
[0m14:21:02.462217 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:21:02.477177 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:21:02.482264 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:21:02.482264 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:21:02.482264 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:21:05.742261 [debug] [ThreadPool]: SQL status: OK in 3.25 seconds
[0m14:21:05.742261 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:21:05.742261 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:21:05.752261 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:21:06.563036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0a9a4a05-25fc-4422-8d74-673c99f329f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013B74376C50>]}
[0m14:21:06.563036 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:21:06.567117 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:21:06.567117 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:21:06.567117 [info ] [MainThread]: 
[0m14:21:06.602033 [debug] [Thread-1 (]: Began running node model.finance.dim_exp
[0m14:21:06.604694 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.dim_exp ................................... [RUN]
[0m14:21:06.605196 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_exp"
[0m14:21:06.605196 [debug] [Thread-1 (]: Began compiling node model.finance.dim_exp
[0m14:21:06.609273 [debug] [Thread-1 (]: Compiling model.finance.dim_exp
[0m14:21:06.612063 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_exp"
[0m14:21:06.617156 [debug] [Thread-1 (]: finished collecting timing info
[0m14:21:06.618973 [debug] [Thread-1 (]: Began executing node model.finance.dim_exp
[0m14:21:06.702065 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_exp"
[0m14:21:06.702065 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:21:06.702065 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_exp"
[0m14:21:06.706654 [debug] [Thread-1 (]: On model.finance.dim_exp: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_exp"} */

  
    
      create or replace table finance.dim_exp
    
    
    using delta
    
    
    
    
    
    
    as
      

select Expenses,Gross_sales, Manufacturing_price from data.table_finance;
  
[0m14:21:06.706654 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:21:11.501910 [debug] [Thread-1 (]: SQL status: OK in 4.8 seconds
[0m14:21:11.527073 [debug] [Thread-1 (]: finished collecting timing info
[0m14:21:11.527073 [debug] [Thread-1 (]: On model.finance.dim_exp: ROLLBACK
[0m14:21:11.527073 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:21:11.530660 [debug] [Thread-1 (]: On model.finance.dim_exp: Close
[0m14:21:12.291899 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0a9a4a05-25fc-4422-8d74-673c99f329f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013B7545CD60>]}
[0m14:21:12.291899 [info ] [Thread-1 (]: 1 of 1 OK created sql table model finance.dim_exp .............................. [[32mOK[0m in 5.69s]
[0m14:21:12.296319 [debug] [Thread-1 (]: Finished running node model.finance.dim_exp
[0m14:21:12.296319 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:21:12.296319 [debug] [MainThread]: On master: ROLLBACK
[0m14:21:12.301970 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:21:13.142267 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:21:13.142267 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:21:13.148062 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:21:13.149133 [debug] [MainThread]: On master: ROLLBACK
[0m14:21:13.149133 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:21:13.149133 [debug] [MainThread]: On master: Close
[0m14:21:14.172549 [info ] [MainThread]: 
[0m14:21:14.172549 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 14.78 seconds (14.78s).
[0m14:21:14.172549 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:21:14.172549 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:21:14.172549 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:21:14.172549 [debug] [MainThread]: Connection 'model.finance.dim_exp' was properly closed.
[0m14:21:14.202123 [info ] [MainThread]: 
[0m14:21:14.205067 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:21:14.205067 [info ] [MainThread]: 
[0m14:21:14.221917 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m14:21:14.221917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013B75408880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013B75408280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013B7540B490>]}
[0m14:21:14.228087 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:22:02.556923 | c6963689-6d96-4f31-9280-ff9f00b1e344 ==============================
[0m14:22:02.556923 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:22:02.556923 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/dim_unit.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:22:02.556923 [debug] [MainThread]: Tracking: tracking
[0m14:22:02.571835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1A0AD9C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1A0AD9870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1A0AD9AB0>]}
[0m14:22:02.661915 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:22:02.661915 [debug] [MainThread]: Partial parsing: updated file: finance://models\dim_unit.sql
[0m14:22:02.681903 [debug] [MainThread]: 1699: static parser successfully parsed dim_unit.sql
[0m14:22:02.711683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c6963689-6d96-4f31-9280-ff9f00b1e344', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1A1C8A8C0>]}
[0m14:22:02.721664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c6963689-6d96-4f31-9280-ff9f00b1e344', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1A1C66D70>]}
[0m14:22:02.721664 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:22:02.721664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c6963689-6d96-4f31-9280-ff9f00b1e344', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1A1C537F0>]}
[0m14:22:02.721664 [info ] [MainThread]: 
[0m14:22:02.721664 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:22:02.739633 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m14:22:02.754172 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:22:02.754172 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:22:02.754172 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:22:06.052204 [debug] [ThreadPool]: SQL status: OK in 3.3 seconds
[0m14:22:06.062054 [debug] [ThreadPool]: On list_schemas: Close
[0m14:22:07.732186 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:22:07.742080 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:22:07.742080 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:22:07.745723 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:22:07.745949 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:22:11.602132 [debug] [ThreadPool]: SQL status: OK in 3.86 seconds
[0m14:22:11.611744 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:22:11.611744 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:22:11.611744 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:22:13.549502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c6963689-6d96-4f31-9280-ff9f00b1e344', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1A1C6BBE0>]}
[0m14:22:13.551534 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:22:13.552260 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:22:13.553284 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:22:13.553797 [info ] [MainThread]: 
[0m14:22:13.565799 [debug] [Thread-1 (]: Began running node model.finance.dim_unit
[0m14:22:13.571917 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.dim_unit .................................. [RUN]
[0m14:22:13.573067 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_unit"
[0m14:22:13.575371 [debug] [Thread-1 (]: Began compiling node model.finance.dim_unit
[0m14:22:13.575371 [debug] [Thread-1 (]: Compiling model.finance.dim_unit
[0m14:22:13.622107 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_unit"
[0m14:22:13.622107 [debug] [Thread-1 (]: finished collecting timing info
[0m14:22:13.622107 [debug] [Thread-1 (]: Began executing node model.finance.dim_unit
[0m14:22:13.672101 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_unit"
[0m14:22:13.675620 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:22:13.675730 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_unit"
[0m14:22:13.675730 [debug] [Thread-1 (]: On model.finance.dim_unit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_unit"} */

  
    
      create or replace table finance.dim_unit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Target, Unit_price, Unit_sold from data.table_finance;
  
[0m14:22:13.677132 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:22:16.491946 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_unit"} */

  
    
      create or replace table finance.dim_unit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Target, Unit_price, Unit_sold from data.table_finance;
  
[0m14:22:16.491946 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Unit_sold` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`data`.`table_finance`.`date`, `spark_catalog`.`data`.`table_finance`.`COGS`, `spark_catalog`.`data`.`table_finance`.`Year`, `spark_catalog`.`data`.`table_finance`.`Sales`, `spark_catalog`.`data`.`table_finance`.`Units_Sold`]; line 18 pos 27
[0m14:22:16.491946 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Unit_sold` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`data`.`table_finance`.`date`, `spark_catalog`.`data`.`table_finance`.`COGS`, `spark_catalog`.`data`.`table_finance`.`Year`, `spark_catalog`.`data`.`table_finance`.`Sales`, `spark_catalog`.`data`.`table_finance`.`Units_Sold`]; line 18 pos 27
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Unit_sold` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`data`.`table_finance`.`date`, `spark_catalog`.`data`.`table_finance`.`COGS`, `spark_catalog`.`data`.`table_finance`.`Year`, `spark_catalog`.`data`.`table_finance`.`Sales`, `spark_catalog`.`data`.`table_finance`.`Units_Sold`]; line 18 pos 27
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m14:22:16.501681 [debug] [Thread-1 (]: Databricks adapter: operation-id: b':|."\xc4\x9dE\xda\x92\x85\x15a\x9a^\x08\xc5'
[0m14:22:16.503057 [debug] [Thread-1 (]: finished collecting timing info
[0m14:22:16.503057 [debug] [Thread-1 (]: On model.finance.dim_unit: ROLLBACK
[0m14:22:16.503848 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:22:16.503848 [debug] [Thread-1 (]: On model.finance.dim_unit: Close
[0m14:22:17.502213 [debug] [Thread-1 (]: Runtime Error in model dim_unit (models\dim_unit.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Unit_sold` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`data`.`table_finance`.`date`, `spark_catalog`.`data`.`table_finance`.`COGS`, `spark_catalog`.`data`.`table_finance`.`Year`, `spark_catalog`.`data`.`table_finance`.`Sales`, `spark_catalog`.`data`.`table_finance`.`Units_Sold`]; line 18 pos 27
[0m14:22:17.502213 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c6963689-6d96-4f31-9280-ff9f00b1e344', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B18F2334F0>]}
[0m14:22:17.511943 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model finance.dim_unit ......................... [[31mERROR[0m in 3.93s]
[0m14:22:17.511943 [debug] [Thread-1 (]: Finished running node model.finance.dim_unit
[0m14:22:17.511943 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:22:17.511943 [debug] [MainThread]: On master: ROLLBACK
[0m14:22:17.511943 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:22:19.732037 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:22:19.732037 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:22:19.732037 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:22:19.732037 [debug] [MainThread]: On master: ROLLBACK
[0m14:22:19.736187 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:22:19.736187 [debug] [MainThread]: On master: Close
[0m14:22:21.311825 [info ] [MainThread]: 
[0m14:22:21.321969 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 18.59 seconds (18.59s).
[0m14:22:21.321969 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:22:21.321969 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:22:21.321969 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:22:21.321969 [debug] [MainThread]: Connection 'model.finance.dim_unit' was properly closed.
[0m14:22:21.337961 [info ] [MainThread]: 
[0m14:22:21.337961 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:22:21.341985 [info ] [MainThread]: 
[0m14:22:21.352020 [error] [MainThread]: [33mRuntime Error in model dim_unit (models\dim_unit.sql)[0m
[0m14:22:21.352020 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Unit_sold` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`data`.`table_finance`.`date`, `spark_catalog`.`data`.`table_finance`.`COGS`, `spark_catalog`.`data`.`table_finance`.`Year`, `spark_catalog`.`data`.`table_finance`.`Sales`, `spark_catalog`.`data`.`table_finance`.`Units_Sold`]; line 18 pos 27
[0m14:22:21.362671 [info ] [MainThread]: 
[0m14:22:21.362671 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m14:22:21.381873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1A1C6B280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1A1C6B910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1A1C6A950>]}
[0m14:22:21.381873 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:24:05.376493 | 7e740a31-c7b1-4a5b-b18c-b92dbe54d9ab ==============================
[0m14:24:05.376493 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:24:05.376493 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/dim_unit.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:24:05.381500 [debug] [MainThread]: Tracking: tracking
[0m14:24:05.395000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191B63D9C60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191B63D98A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191B63D9FC0>]}
[0m14:24:05.491389 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:24:05.491389 [debug] [MainThread]: Partial parsing: updated file: finance://models\dim_unit.sql
[0m14:24:05.501514 [debug] [MainThread]: 1699: static parser successfully parsed dim_unit.sql
[0m14:24:05.531363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7e740a31-c7b1-4a5b-b18c-b92dbe54d9ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191B758A8F0>]}
[0m14:24:05.541394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7e740a31-c7b1-4a5b-b18c-b92dbe54d9ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191B7566DA0>]}
[0m14:24:05.541394 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:24:05.544373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7e740a31-c7b1-4a5b-b18c-b92dbe54d9ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191B7553820>]}
[0m14:24:05.544373 [info ] [MainThread]: 
[0m14:24:05.544373 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:24:05.551733 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m14:24:05.571612 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:24:05.571612 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:24:05.571612 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:24:11.151426 [debug] [ThreadPool]: SQL status: OK in 5.58 seconds
[0m14:24:11.165415 [debug] [ThreadPool]: On list_schemas: Close
[0m14:24:13.653597 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:24:13.661721 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:13.661721 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:24:13.661721 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:24:13.663153 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:24:21.531554 [debug] [ThreadPool]: SQL status: OK in 7.87 seconds
[0m14:24:21.536594 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:24:21.536594 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:24:21.536594 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:24:23.371622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7e740a31-c7b1-4a5b-b18c-b92dbe54d9ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191B7568A00>]}
[0m14:24:23.371622 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:23.371622 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:24:23.371622 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:24:23.371622 [info ] [MainThread]: 
[0m14:24:23.394018 [debug] [Thread-1 (]: Began running node model.finance.dim_unit
[0m14:24:23.394018 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.dim_unit .................................. [RUN]
[0m14:24:23.394018 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_unit"
[0m14:24:23.394018 [debug] [Thread-1 (]: Began compiling node model.finance.dim_unit
[0m14:24:23.394018 [debug] [Thread-1 (]: Compiling model.finance.dim_unit
[0m14:24:23.455048 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_unit"
[0m14:24:23.455048 [debug] [Thread-1 (]: finished collecting timing info
[0m14:24:23.455048 [debug] [Thread-1 (]: Began executing node model.finance.dim_unit
[0m14:24:23.501553 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_unit"
[0m14:24:23.501553 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:23.501553 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_unit"
[0m14:24:23.505416 [debug] [Thread-1 (]: On model.finance.dim_unit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_unit"} */

  
    
      create or replace table finance.dim_unit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Target, Unit_Price, Unit_Sold from data.table_finance;
  
[0m14:24:23.505905 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:24:27.992204 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_unit"} */

  
    
      create or replace table finance.dim_unit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Target, Unit_Price, Unit_Sold from data.table_finance;
  
[0m14:24:27.992204 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Unit_Sold` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`data`.`table_finance`.`date`, `spark_catalog`.`data`.`table_finance`.`COGS`, `spark_catalog`.`data`.`table_finance`.`Sales`, `spark_catalog`.`data`.`table_finance`.`Units_Sold`, `spark_catalog`.`data`.`table_finance`.`Year`]; line 18 pos 27
[0m14:24:27.992204 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Unit_Sold` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`data`.`table_finance`.`date`, `spark_catalog`.`data`.`table_finance`.`COGS`, `spark_catalog`.`data`.`table_finance`.`Sales`, `spark_catalog`.`data`.`table_finance`.`Units_Sold`, `spark_catalog`.`data`.`table_finance`.`Year`]; line 18 pos 27
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Unit_Sold` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`data`.`table_finance`.`date`, `spark_catalog`.`data`.`table_finance`.`COGS`, `spark_catalog`.`data`.`table_finance`.`Sales`, `spark_catalog`.`data`.`table_finance`.`Units_Sold`, `spark_catalog`.`data`.`table_finance`.`Year`]; line 18 pos 27
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m14:24:27.992204 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'd\r<\r>MG\x83\x8e\x0c\x89\x99\x07u\x12W'
[0m14:24:28.001470 [debug] [Thread-1 (]: finished collecting timing info
[0m14:24:28.001470 [debug] [Thread-1 (]: On model.finance.dim_unit: ROLLBACK
[0m14:24:28.001470 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:24:28.001470 [debug] [Thread-1 (]: On model.finance.dim_unit: Close
[0m14:24:29.431518 [debug] [Thread-1 (]: Runtime Error in model dim_unit (models\dim_unit.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Unit_Sold` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`data`.`table_finance`.`date`, `spark_catalog`.`data`.`table_finance`.`COGS`, `spark_catalog`.`data`.`table_finance`.`Sales`, `spark_catalog`.`data`.`table_finance`.`Units_Sold`, `spark_catalog`.`data`.`table_finance`.`Year`]; line 18 pos 27
[0m14:24:29.431518 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7e740a31-c7b1-4a5b-b18c-b92dbe54d9ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191B63E9B10>]}
[0m14:24:29.441414 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model finance.dim_unit ......................... [[31mERROR[0m in 6.04s]
[0m14:24:29.441414 [debug] [Thread-1 (]: Finished running node model.finance.dim_unit
[0m14:24:29.441414 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:24:29.441414 [debug] [MainThread]: On master: ROLLBACK
[0m14:24:29.441414 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:24:30.341464 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:24:30.341464 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:30.341464 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:24:30.341464 [debug] [MainThread]: On master: ROLLBACK
[0m14:24:30.349337 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:24:30.349337 [debug] [MainThread]: On master: Close
[0m14:24:31.191775 [info ] [MainThread]: 
[0m14:24:31.191775 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 25.65 seconds (25.65s).
[0m14:24:31.191775 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:24:31.191775 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:24:31.191775 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:24:31.191775 [debug] [MainThread]: Connection 'model.finance.dim_unit' was properly closed.
[0m14:24:31.202774 [info ] [MainThread]: 
[0m14:24:31.211353 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:24:31.211353 [info ] [MainThread]: 
[0m14:24:31.211353 [error] [MainThread]: [33mRuntime Error in model dim_unit (models\dim_unit.sql)[0m
[0m14:24:31.211353 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Unit_Sold` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`data`.`table_finance`.`date`, `spark_catalog`.`data`.`table_finance`.`COGS`, `spark_catalog`.`data`.`table_finance`.`Sales`, `spark_catalog`.`data`.`table_finance`.`Units_Sold`, `spark_catalog`.`data`.`table_finance`.`Year`]; line 18 pos 27
[0m14:24:31.211353 [info ] [MainThread]: 
[0m14:24:31.211353 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m14:24:31.221497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191B756B7C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191B7568E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191B7569B10>]}
[0m14:24:31.221497 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:25:08.481396 | 592427df-7dd4-4c5b-8dee-981942757b86 ==============================
[0m14:25:08.481396 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:25:08.481396 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/dim_unit.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:25:08.481396 [debug] [MainThread]: Tracking: tracking
[0m14:25:08.503961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CA86B3DC90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CA86B3D870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CA86B3DAB0>]}
[0m14:25:08.621569 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:25:08.621569 [debug] [MainThread]: Partial parsing: updated file: finance://models\dim_unit.sql
[0m14:25:08.641096 [debug] [MainThread]: 1699: static parser successfully parsed dim_unit.sql
[0m14:25:08.681307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '592427df-7dd4-4c5b-8dee-981942757b86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CA86D2A8C0>]}
[0m14:25:08.691040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '592427df-7dd4-4c5b-8dee-981942757b86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CA86D06D70>]}
[0m14:25:08.691040 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:25:08.691040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '592427df-7dd4-4c5b-8dee-981942757b86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CA86CF37F0>]}
[0m14:25:08.702248 [info ] [MainThread]: 
[0m14:25:08.713053 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:25:08.731690 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m14:25:08.748428 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:25:08.748428 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:25:08.748428 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:14.332465 [debug] [ThreadPool]: SQL status: OK in 5.58 seconds
[0m14:25:14.351494 [debug] [ThreadPool]: On list_schemas: Close
[0m14:25:15.118464 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:25:15.140835 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:25:15.140835 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:25:15.144625 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:25:15.144625 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:20.821035 [debug] [ThreadPool]: SQL status: OK in 5.68 seconds
[0m14:25:20.829069 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:25:20.829069 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:25:20.829069 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:25:22.421277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '592427df-7dd4-4c5b-8dee-981942757b86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CA86D09300>]}
[0m14:25:22.421277 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:25:22.421277 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:25:22.428754 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:25:22.428754 [info ] [MainThread]: 
[0m14:25:22.461595 [debug] [Thread-1 (]: Began running node model.finance.dim_unit
[0m14:25:22.461595 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.dim_unit .................................. [RUN]
[0m14:25:22.461595 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_unit"
[0m14:25:22.471736 [debug] [Thread-1 (]: Began compiling node model.finance.dim_unit
[0m14:25:22.471736 [debug] [Thread-1 (]: Compiling model.finance.dim_unit
[0m14:25:22.551609 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_unit"
[0m14:25:22.551609 [debug] [Thread-1 (]: finished collecting timing info
[0m14:25:22.555692 [debug] [Thread-1 (]: Began executing node model.finance.dim_unit
[0m14:25:22.700828 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_unit"
[0m14:25:22.708746 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:25:22.708931 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_unit"
[0m14:25:22.708931 [debug] [Thread-1 (]: On model.finance.dim_unit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_unit"} */

  
    
      create or replace table finance.dim_unit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Target, Unit_Price, Units_Sold from data.table_finance;
  
[0m14:25:22.708931 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:25:28.700919 [debug] [Thread-1 (]: SQL status: OK in 5.99 seconds
[0m14:25:28.738080 [debug] [Thread-1 (]: finished collecting timing info
[0m14:25:28.738080 [debug] [Thread-1 (]: On model.finance.dim_unit: ROLLBACK
[0m14:25:28.741103 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:25:28.742072 [debug] [Thread-1 (]: On model.finance.dim_unit: Close
[0m14:25:35.771290 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '592427df-7dd4-4c5b-8dee-981942757b86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CA86D0B490>]}
[0m14:25:35.771290 [info ] [Thread-1 (]: 1 of 1 OK created sql table model finance.dim_unit ............................. [[32mOK[0m in 13.31s]
[0m14:25:35.779192 [debug] [Thread-1 (]: Finished running node model.finance.dim_unit
[0m14:25:35.782031 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:25:35.784272 [debug] [MainThread]: On master: ROLLBACK
[0m14:25:35.785401 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:25:36.901329 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:25:36.901329 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:25:36.901329 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:25:36.904336 [debug] [MainThread]: On master: ROLLBACK
[0m14:25:36.904879 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:25:36.904879 [debug] [MainThread]: On master: Close
[0m14:25:39.765619 [info ] [MainThread]: 
[0m14:25:39.766115 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 31.05 seconds (31.05s).
[0m14:25:39.771253 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:25:39.773108 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:25:39.773770 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:25:39.774266 [debug] [MainThread]: Connection 'model.finance.dim_unit' was properly closed.
[0m14:25:39.791841 [info ] [MainThread]: 
[0m14:25:39.791841 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:25:39.791841 [info ] [MainThread]: 
[0m14:25:39.811182 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m14:25:39.811182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CA86D0AEF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CA86D08C10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CA86D090F0>]}
[0m14:25:39.811182 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:26:24.691197 | cecbfb48-f723-4c86-83ca-450c4b8115ca ==============================
[0m14:26:24.691197 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:26:24.691197 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/dim_profit.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:26:24.691197 [debug] [MainThread]: Tracking: tracking
[0m14:26:24.704990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012DFF309C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012DFF309900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012DFF30B340>]}
[0m14:26:24.801762 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m14:26:24.801762 [debug] [MainThread]: Partial parsing: updated file: finance://models\dim_profit.sql
[0m14:26:24.805689 [debug] [MainThread]: Partial parsing: updated file: finance://models\dim_month.sql
[0m14:26:24.819823 [debug] [MainThread]: 1699: static parser successfully parsed dim_profit.sql
[0m14:26:24.844141 [debug] [MainThread]: 1699: static parser successfully parsed dim_month.sql
[0m14:26:24.860592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cecbfb48-f723-4c86-83ca-450c4b8115ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012DFF4E24D0>]}
[0m14:26:24.871293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cecbfb48-f723-4c86-83ca-450c4b8115ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012DFF4B39A0>]}
[0m14:26:24.871293 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:26:24.871293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cecbfb48-f723-4c86-83ca-450c4b8115ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012DFF4B3730>]}
[0m14:26:24.871293 [info ] [MainThread]: 
[0m14:26:24.881253 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:26:24.891322 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m14:26:24.901053 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:26:24.901053 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:24.901053 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:28.771541 [debug] [ThreadPool]: SQL status: OK in 3.87 seconds
[0m14:26:28.781165 [debug] [ThreadPool]: On list_schemas: Close
[0m14:26:30.874558 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:26:30.891011 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:26:30.891011 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:26:30.891011 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:26:30.891011 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:34.191531 [debug] [ThreadPool]: SQL status: OK in 3.3 seconds
[0m14:26:34.202555 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:26:34.202555 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:26:34.208855 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:26:34.881666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cecbfb48-f723-4c86-83ca-450c4b8115ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012DFF2F5480>]}
[0m14:26:34.891192 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:26:34.891192 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:26:34.891192 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:26:34.891192 [info ] [MainThread]: 
[0m14:26:34.991531 [debug] [Thread-1 (]: Began running node model.finance.dim_profit
[0m14:26:34.991531 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.dim_profit ................................ [RUN]
[0m14:26:35.001349 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_profit"
[0m14:26:35.001349 [debug] [Thread-1 (]: Began compiling node model.finance.dim_profit
[0m14:26:35.001349 [debug] [Thread-1 (]: Compiling model.finance.dim_profit
[0m14:26:35.011603 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_profit"
[0m14:26:35.016551 [debug] [Thread-1 (]: finished collecting timing info
[0m14:26:35.017042 [debug] [Thread-1 (]: Began executing node model.finance.dim_profit
[0m14:26:35.160970 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_profit"
[0m14:26:35.164574 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:26:35.165567 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_profit"
[0m14:26:35.166318 [debug] [Thread-1 (]: On model.finance.dim_profit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_profit"} */

  
    
      create or replace table finance.dim_profit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Discounts, Profits, Sales, Country, Discount_Band, Product from data.table_finance;
  
[0m14:26:35.167123 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:26:37.353349 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_profit"} */

  
    
      create or replace table finance.dim_profit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Discounts, Profits, Sales, Country, Discount_Band, Product from data.table_finance;
  
[0m14:26:37.353349 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Profits` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`data`.`table_finance`.`date`, `spark_catalog`.`data`.`table_finance`.`COGS`, `spark_catalog`.`data`.`table_finance`.`Sales`, `spark_catalog`.`data`.`table_finance`.`Year`, `spark_catalog`.`data`.`table_finance`.`Actual`]; line 18 pos 18
[0m14:26:37.353349 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Profits` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`data`.`table_finance`.`date`, `spark_catalog`.`data`.`table_finance`.`COGS`, `spark_catalog`.`data`.`table_finance`.`Sales`, `spark_catalog`.`data`.`table_finance`.`Year`, `spark_catalog`.`data`.`table_finance`.`Actual`]; line 18 pos 18
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Profits` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`data`.`table_finance`.`date`, `spark_catalog`.`data`.`table_finance`.`COGS`, `spark_catalog`.`data`.`table_finance`.`Sales`, `spark_catalog`.`data`.`table_finance`.`Year`, `spark_catalog`.`data`.`table_finance`.`Actual`]; line 18 pos 18
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m14:26:37.353349 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\x9b\xd3\xadq\xc1\xe2F\xb2\xa7]\x97\x8f\n3\xb8\xf9'
[0m14:26:37.353349 [debug] [Thread-1 (]: finished collecting timing info
[0m14:26:37.353349 [debug] [Thread-1 (]: On model.finance.dim_profit: ROLLBACK
[0m14:26:37.358887 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:26:37.358887 [debug] [Thread-1 (]: On model.finance.dim_profit: Close
[0m14:26:39.971417 [debug] [Thread-1 (]: Runtime Error in model dim_profit (models\dim_profit.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Profits` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`data`.`table_finance`.`date`, `spark_catalog`.`data`.`table_finance`.`COGS`, `spark_catalog`.`data`.`table_finance`.`Sales`, `spark_catalog`.`data`.`table_finance`.`Year`, `spark_catalog`.`data`.`table_finance`.`Actual`]; line 18 pos 18
[0m14:26:39.971417 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cecbfb48-f723-4c86-83ca-450c4b8115ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012DFF50E140>]}
[0m14:26:39.971417 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model finance.dim_profit ....................... [[31mERROR[0m in 4.97s]
[0m14:26:39.981324 [debug] [Thread-1 (]: Finished running node model.finance.dim_profit
[0m14:26:39.987106 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:26:39.987106 [debug] [MainThread]: On master: ROLLBACK
[0m14:26:39.989614 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:26:43.150911 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:26:43.150911 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:26:43.150911 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:26:43.154513 [debug] [MainThread]: On master: ROLLBACK
[0m14:26:43.154922 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:26:43.155448 [debug] [MainThread]: On master: Close
[0m14:26:47.651249 [info ] [MainThread]: 
[0m14:26:47.651249 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 22.77 seconds (22.77s).
[0m14:26:47.651249 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:26:47.656774 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:26:47.656774 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:26:47.656774 [debug] [MainThread]: Connection 'model.finance.dim_profit' was properly closed.
[0m14:26:47.672261 [info ] [MainThread]: 
[0m14:26:47.678709 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:26:47.681086 [info ] [MainThread]: 
[0m14:26:47.691391 [error] [MainThread]: [33mRuntime Error in model dim_profit (models\dim_profit.sql)[0m
[0m14:26:47.695765 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Profits` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`data`.`table_finance`.`date`, `spark_catalog`.`data`.`table_finance`.`COGS`, `spark_catalog`.`data`.`table_finance`.`Sales`, `spark_catalog`.`data`.`table_finance`.`Year`, `spark_catalog`.`data`.`table_finance`.`Actual`]; line 18 pos 18
[0m14:26:47.710928 [info ] [MainThread]: 
[0m14:26:47.710928 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m14:26:47.731162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012DFF50D000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012DFF5E8760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012DFF5E8460>]}
[0m14:26:47.733965 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:27:08.541657 | f9e060fe-b35f-47fc-ace2-f49ebade8091 ==============================
[0m14:27:08.541657 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:27:08.541657 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/dim_profit.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:27:08.545953 [debug] [MainThread]: Tracking: tracking
[0m14:27:08.561909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D731D61C60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D731D618A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D731D61FC0>]}
[0m14:27:08.691921 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m14:27:08.691921 [debug] [MainThread]: Partial parsing: updated file: finance://models\dim_month.sql
[0m14:27:08.691921 [debug] [MainThread]: Partial parsing: updated file: finance://models\dim_profit.sql
[0m14:27:08.700724 [debug] [MainThread]: 1699: static parser successfully parsed dim_month.sql
[0m14:27:08.721157 [debug] [MainThread]: 1699: static parser successfully parsed dim_profit.sql
[0m14:27:08.741204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f9e060fe-b35f-47fc-ace2-f49ebade8091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D731F42560>]}
[0m14:27:08.750636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f9e060fe-b35f-47fc-ace2-f49ebade8091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D731F13760>]}
[0m14:27:08.755496 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:27:08.755496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f9e060fe-b35f-47fc-ace2-f49ebade8091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D731F13A90>]}
[0m14:27:08.755496 [info ] [MainThread]: 
[0m14:27:08.760822 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:27:08.770730 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m14:27:08.778642 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:27:08.778642 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:27:08.778642 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:27:11.831349 [debug] [ThreadPool]: SQL status: OK in 3.05 seconds
[0m14:27:11.831349 [debug] [ThreadPool]: On list_schemas: Close
[0m14:27:17.961150 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:27:17.976035 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:17.978038 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:27:17.979030 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:27:17.979573 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:27:34.554509 [debug] [ThreadPool]: SQL status: OK in 16.57 seconds
[0m14:27:34.554509 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:27:34.554509 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:27:34.554509 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:27:35.711033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f9e060fe-b35f-47fc-ace2-f49ebade8091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D732F61450>]}
[0m14:27:35.711033 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:35.711033 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:27:35.714838 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:27:35.716392 [info ] [MainThread]: 
[0m14:27:35.776931 [debug] [Thread-1 (]: Began running node model.finance.dim_profit
[0m14:27:35.776931 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.dim_profit ................................ [RUN]
[0m14:27:35.781449 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_profit"
[0m14:27:35.781449 [debug] [Thread-1 (]: Began compiling node model.finance.dim_profit
[0m14:27:35.781449 [debug] [Thread-1 (]: Compiling model.finance.dim_profit
[0m14:27:35.781449 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_profit"
[0m14:27:35.781449 [debug] [Thread-1 (]: finished collecting timing info
[0m14:27:35.781449 [debug] [Thread-1 (]: Began executing node model.finance.dim_profit
[0m14:27:35.834540 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_profit"
[0m14:27:35.834540 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:35.834540 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_profit"
[0m14:27:35.834540 [debug] [Thread-1 (]: On model.finance.dim_profit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_profit"} */

  
    
      create or replace table finance.dim_profit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Discounts, Profit, Sales, Country, Discount_Band, Product from data.table_finance;
  
[0m14:27:35.834540 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:27:40.091097 [debug] [Thread-1 (]: SQL status: OK in 4.26 seconds
[0m14:27:40.100612 [debug] [Thread-1 (]: finished collecting timing info
[0m14:27:40.100612 [debug] [Thread-1 (]: On model.finance.dim_profit: ROLLBACK
[0m14:27:40.100612 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:27:40.100612 [debug] [Thread-1 (]: On model.finance.dim_profit: Close
[0m14:27:40.901113 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9e060fe-b35f-47fc-ace2-f49ebade8091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D733004FA0>]}
[0m14:27:40.911941 [info ] [Thread-1 (]: 1 of 1 OK created sql table model finance.dim_profit ........................... [[32mOK[0m in 5.12s]
[0m14:27:40.911941 [debug] [Thread-1 (]: Finished running node model.finance.dim_profit
[0m14:27:40.911941 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:27:40.911941 [debug] [MainThread]: On master: ROLLBACK
[0m14:27:40.911941 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:27:41.841433 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:27:41.841433 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:41.841433 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:27:41.850300 [debug] [MainThread]: On master: ROLLBACK
[0m14:27:41.850916 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:27:41.851306 [debug] [MainThread]: On master: Close
[0m14:27:42.881367 [info ] [MainThread]: 
[0m14:27:42.881367 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 34.13 seconds (34.13s).
[0m14:27:42.881367 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:27:42.881367 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:27:42.881367 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:27:42.881367 [debug] [MainThread]: Connection 'model.finance.dim_profit' was properly closed.
[0m14:27:42.891055 [info ] [MainThread]: 
[0m14:27:42.900373 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:27:42.900373 [info ] [MainThread]: 
[0m14:27:42.920521 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m14:27:42.923920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D732F609D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7330043A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D733004B50>]}
[0m14:27:42.923920 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:27:57.491415 | 84b28b78-f347-4631-bcc5-81e98a010674 ==============================
[0m14:27:57.491415 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:27:57.491415 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/dim_month.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:27:57.494227 [debug] [MainThread]: Tracking: tracking
[0m14:27:57.510680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020E02019C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020E02019870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020E02019AB0>]}
[0m14:27:57.596023 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:27:57.596023 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:27:57.601119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '84b28b78-f347-4631-bcc5-81e98a010674', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020E031D1120>]}
[0m14:27:57.604446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '84b28b78-f347-4631-bcc5-81e98a010674', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020E021D9510>]}
[0m14:27:57.610955 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:27:57.611912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '84b28b78-f347-4631-bcc5-81e98a010674', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020E021D9900>]}
[0m14:27:57.612407 [info ] [MainThread]: 
[0m14:27:57.612407 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:27:57.620496 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m14:27:57.630838 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:27:57.630838 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:27:57.630838 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:28:04.920872 [debug] [ThreadPool]: SQL status: OK in 7.29 seconds
[0m14:28:04.931575 [debug] [ThreadPool]: On list_schemas: Close
[0m14:28:09.781357 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:28:09.791205 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:09.791205 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:28:09.791205 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:28:09.791205 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:28:14.070854 [debug] [ThreadPool]: SQL status: OK in 4.28 seconds
[0m14:28:14.081407 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:28:14.081407 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:28:14.081407 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:28:15.851361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '84b28b78-f347-4631-bcc5-81e98a010674', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020E021D6B90>]}
[0m14:28:15.851361 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:15.851361 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:28:15.851361 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:28:15.851361 [info ] [MainThread]: 
[0m14:28:15.869028 [debug] [Thread-1 (]: Began running node model.finance.dim_month
[0m14:28:15.869028 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.dim_month ................................. [RUN]
[0m14:28:15.871254 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_month"
[0m14:28:15.871254 [debug] [Thread-1 (]: Began compiling node model.finance.dim_month
[0m14:28:15.871254 [debug] [Thread-1 (]: Compiling model.finance.dim_month
[0m14:28:15.875448 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_month"
[0m14:28:15.875448 [debug] [Thread-1 (]: finished collecting timing info
[0m14:28:15.879517 [debug] [Thread-1 (]: Began executing node model.finance.dim_month
[0m14:28:15.931152 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_month"
[0m14:28:15.931152 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:15.931152 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_month"
[0m14:28:15.931152 [debug] [Thread-1 (]: On model.finance.dim_month: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_month"} */

  
    
      create or replace table finance.dim_month
    
    
    using delta
    
    
    
    
    
    
    as
      

select Month_Number, Year, MonthName, Date from data.table_finance;
  
[0m14:28:15.934948 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:28:20.951466 [debug] [Thread-1 (]: SQL status: OK in 5.02 seconds
[0m14:28:20.961830 [debug] [Thread-1 (]: finished collecting timing info
[0m14:28:20.961830 [debug] [Thread-1 (]: On model.finance.dim_month: ROLLBACK
[0m14:28:20.963725 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:20.963835 [debug] [Thread-1 (]: On model.finance.dim_month: Close
[0m14:28:46.462843 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '84b28b78-f347-4631-bcc5-81e98a010674', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020E03278B80>]}
[0m14:28:46.464333 [info ] [Thread-1 (]: 1 of 1 OK created sql table model finance.dim_month ............................ [[32mOK[0m in 30.59s]
[0m14:28:46.465826 [debug] [Thread-1 (]: Finished running node model.finance.dim_month
[0m14:28:46.470674 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:28:46.470674 [debug] [MainThread]: On master: ROLLBACK
[0m14:28:46.470674 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:28:50.180571 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:28:50.188578 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:50.188976 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:28:50.188976 [debug] [MainThread]: On master: ROLLBACK
[0m14:28:50.190983 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:28:50.190983 [debug] [MainThread]: On master: Close
[0m14:28:52.040728 [info ] [MainThread]: 
[0m14:28:52.041314 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 54.43 seconds (54.43s).
[0m14:28:52.041314 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:28:52.041314 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:28:52.041314 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:28:52.041314 [debug] [MainThread]: Connection 'model.finance.dim_month' was properly closed.
[0m14:28:52.059108 [info ] [MainThread]: 
[0m14:28:52.059108 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:28:52.060709 [info ] [MainThread]: 
[0m14:28:52.060709 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m14:28:52.060709 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020E03278130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020E0327A6E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020E03278700>]}
[0m14:28:52.060709 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:49:03.637674 | 61e2e94e-773c-47a4-a196-5112c4a569de ==============================
[0m14:49:03.637674 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:49:03.640188 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/dim_check_month.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:49:03.640188 [debug] [MainThread]: Tracking: tracking
[0m14:49:03.655820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210BEF59C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210BEF59870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210BEF59AB0>]}
[0m14:49:03.758030 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 2 files changed.
[0m14:49:03.758030 [debug] [MainThread]: Partial parsing: added file: finance://models\schema.yml
[0m14:49:03.763350 [debug] [MainThread]: Partial parsing: added file: finance://models\dim_check_month.sql
[0m14:49:03.763916 [debug] [MainThread]: Partial parsing: updated file: finance://models\dim_info.sql
[0m14:49:03.764270 [debug] [MainThread]: Partial parsing: updated file: finance://models\dim_exp.sql
[0m14:49:03.778142 [debug] [MainThread]: 1699: static parser successfully parsed dim_check_month.sql
[0m14:49:03.792920 [debug] [MainThread]: 1699: static parser successfully parsed dim_info.sql
[0m14:49:03.798069 [debug] [MainThread]: 1699: static parser successfully parsed dim_exp.sql
[0m14:49:03.928111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '61e2e94e-773c-47a4-a196-5112c4a569de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210BF128850>]}
[0m14:49:03.939245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '61e2e94e-773c-47a4-a196-5112c4a569de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210BF1038B0>]}
[0m14:49:03.947649 [info ] [MainThread]: Found 8 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:49:03.947931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61e2e94e-773c-47a4-a196-5112c4a569de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210BF103940>]}
[0m14:49:03.950174 [info ] [MainThread]: 
[0m14:49:03.950174 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:49:03.950174 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m14:49:03.967731 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:49:03.967731 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:49:03.967731 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:49:10.598015 [debug] [ThreadPool]: SQL status: OK in 6.63 seconds
[0m14:49:10.608216 [debug] [ThreadPool]: On list_schemas: Close
[0m14:49:13.208240 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:49:13.218070 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:13.218070 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:49:13.223033 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:49:13.223530 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:49:17.747964 [debug] [ThreadPool]: SQL status: OK in 4.52 seconds
[0m14:49:17.747964 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:49:17.747964 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:49:17.747964 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:49:20.268225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61e2e94e-773c-47a4-a196-5112c4a569de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210BEF58BE0>]}
[0m14:49:20.268225 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:20.268225 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:49:20.278153 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:49:20.278153 [info ] [MainThread]: 
[0m14:49:20.308109 [debug] [Thread-1 (]: Began running node model.finance.dim_check_month
[0m14:49:20.308109 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.dim_check_month ........................... [RUN]
[0m14:49:20.308109 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_check_month"
[0m14:49:20.308109 [debug] [Thread-1 (]: Began compiling node model.finance.dim_check_month
[0m14:49:20.308109 [debug] [Thread-1 (]: Compiling model.finance.dim_check_month
[0m14:49:20.319686 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_check_month"
[0m14:49:20.319686 [debug] [Thread-1 (]: finished collecting timing info
[0m14:49:20.326864 [debug] [Thread-1 (]: Began executing node model.finance.dim_check_month
[0m14:49:20.398233 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_check_month"
[0m14:49:20.401172 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:20.401172 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_check_month"
[0m14:49:20.401172 [debug] [Thread-1 (]: On model.finance.dim_check_month: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_check_month"} */

  
    
      create or replace table finance.dim_check_month
    
    
    using delta
    
    
    
    
    
    
    as
      

select date from finance.dim_month where date > '2018-02-01' and date < '2018-03-01';
  
[0m14:49:20.401172 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:49:26.248145 [debug] [Thread-1 (]: SQL status: OK in 5.85 seconds
[0m14:49:26.258154 [debug] [Thread-1 (]: finished collecting timing info
[0m14:49:26.258154 [debug] [Thread-1 (]: On model.finance.dim_check_month: ROLLBACK
[0m14:49:26.262898 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:49:26.263108 [debug] [Thread-1 (]: On model.finance.dim_check_month: Close
[0m14:49:27.337890 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '61e2e94e-773c-47a4-a196-5112c4a569de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210C0278730>]}
[0m14:49:27.337890 [info ] [Thread-1 (]: 1 of 1 OK created sql table model finance.dim_check_month ...................... [[32mOK[0m in 7.03s]
[0m14:49:27.348615 [debug] [Thread-1 (]: Finished running node model.finance.dim_check_month
[0m14:49:27.352284 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:49:27.352284 [debug] [MainThread]: On master: ROLLBACK
[0m14:49:27.352284 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:49:28.467992 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:49:28.467992 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:28.467992 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:49:28.467992 [debug] [MainThread]: On master: ROLLBACK
[0m14:49:28.478400 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:49:28.478896 [debug] [MainThread]: On master: Close
[0m14:49:29.701177 [info ] [MainThread]: 
[0m14:49:29.701177 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 25.75 seconds (25.75s).
[0m14:49:29.701177 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:49:29.707299 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:49:29.707803 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:49:29.707803 [debug] [MainThread]: Connection 'model.finance.dim_check_month' was properly closed.
[0m14:49:29.719595 [info ] [MainThread]: 
[0m14:49:29.720164 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:49:29.720164 [info ] [MainThread]: 
[0m14:49:29.738200 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m14:49:29.738200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210BEF68DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210BEF681F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210BEF69090>]}
[0m14:49:29.738200 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:51:05.967551 | 00315b97-7181-4c7e-8c76-d29b1559677d ==============================
[0m14:51:05.967551 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:51:05.967551 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'select': ['dim_info'], 'which': 'test', 'rpc_method': 'test'}
[0m14:51:05.967551 [debug] [MainThread]: Tracking: tracking
[0m14:51:05.979737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC2E579C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC2E579900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC2E57B340>]}
[0m14:51:06.067417 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:51:06.067417 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:51:06.078299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '00315b97-7181-4c7e-8c76-d29b1559677d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC2E7982B0>]}
[0m14:51:06.078794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '00315b97-7181-4c7e-8c76-d29b1559677d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC2E768880>]}
[0m14:51:06.087479 [info ] [MainThread]: Found 8 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:51:06.089012 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '00315b97-7181-4c7e-8c76-d29b1559677d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC2E7686A0>]}
[0m14:51:06.089505 [info ] [MainThread]: 
[0m14:51:06.092527 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:51:06.097773 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:51:06.113706 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:51:06.117472 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:51:06.118048 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:51:06.118048 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:51:09.557879 [debug] [ThreadPool]: SQL status: OK in 3.44 seconds
[0m14:51:09.571211 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:51:09.571211 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:51:09.571211 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:51:11.281041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '00315b97-7181-4c7e-8c76-d29b1559677d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC2E579CC0>]}
[0m14:51:11.281041 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:51:11.281041 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:51:11.285088 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:51:11.285088 [info ] [MainThread]: 
[0m14:51:11.297891 [debug] [Thread-1 (]: Began running node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m14:51:11.297891 [info ] [Thread-1 (]: 1 of 5 START test accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business  [RUN]
[0m14:51:11.307599 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m14:51:11.307599 [debug] [Thread-1 (]: Began compiling node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m14:51:11.307599 [debug] [Thread-1 (]: Compiling test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m14:51:11.317928 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m14:51:11.327256 [debug] [Thread-1 (]: finished collecting timing info
[0m14:51:11.327800 [debug] [Thread-1 (]: Began executing node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m14:51:11.338305 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m14:51:11.346900 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:51:11.347313 [debug] [Thread-1 (]: Using databricks connection "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m14:51:11.347809 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        Segment as value_field,
        count(*) as n_records

    from finance.dim_info
    group by Segment

)

select *
from all_values
where value_field not in (
    'Government','Channel Partners','Midmarket','Enterprise','Small Business'
)



      
    ) dbt_internal_test
[0m14:51:11.348318 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:51:14.347957 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        Segment as value_field,
        count(*) as n_records

    from finance.dim_info
    group by Segment

)

select *
from all_values
where value_field not in (
    'Government','Channel Partners','Midmarket','Enterprise','Small Business'
)



      
    ) dbt_internal_test
[0m14:51:14.347957 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: finance.dim_info; line 17 pos 9
[0m14:51:14.347957 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: finance.dim_info; line 17 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: finance.dim_info; line 17 pos 9
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m14:51:14.351605 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\xc2\xba\xe7\x9f\xbb\xdfL\x0b\x9d\xa9\xd0(\xae\xa6\xf5\x1a'
[0m14:51:14.352102 [debug] [Thread-1 (]: finished collecting timing info
[0m14:51:14.352102 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: ROLLBACK
[0m14:51:14.352727 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:51:14.352727 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: Close
[0m14:51:15.598087 [debug] [Thread-1 (]: Runtime Error in test accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business (models\schema.yml)
  Table or view not found: finance.dim_info; line 17 pos 9
[0m14:51:15.598087 [error] [Thread-1 (]: 1 of 5 ERROR accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business  [[31mERROR[0m in 4.30s]
[0m14:51:15.598087 [debug] [Thread-1 (]: Finished running node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m14:51:15.598087 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m14:51:15.605308 [info ] [Thread-1 (]: 2 of 5 START test not_null_dim_info_Actual ..................................... [RUN]
[0m14:51:15.607627 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m14:51:15.607627 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m14:51:15.607627 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m14:51:15.657901 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m14:51:15.657901 [debug] [Thread-1 (]: finished collecting timing info
[0m14:51:15.657901 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m14:51:15.657901 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m14:51:15.657901 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:51:15.657901 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m14:51:15.657901 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Actual.5cb2ffbb95"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_info
where Actual is null



      
    ) dbt_internal_test
[0m14:51:15.666820 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:51:18.040187 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Actual.5cb2ffbb95"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_info
where Actual is null



      
    ) dbt_internal_test
[0m14:51:18.040187 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: finance.dim_info; line 14 pos 5
[0m14:51:18.040187 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: finance.dim_info; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: finance.dim_info; line 14 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m14:51:18.043219 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\r\xd4\x8cC\xa1 N\xc1\xbf\xfd\xdb\xb3\xb8\xcd\xef\x95'
[0m14:51:18.043219 [debug] [Thread-1 (]: finished collecting timing info
[0m14:51:18.043219 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: ROLLBACK
[0m14:51:18.043219 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:51:18.043219 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: Close
[0m14:51:19.190551 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_info_Actual (models\schema.yml)
  Table or view not found: finance.dim_info; line 14 pos 5
[0m14:51:19.190551 [error] [Thread-1 (]: 2 of 5 ERROR not_null_dim_info_Actual .......................................... [[31mERROR[0m in 3.58s]
[0m14:51:19.194057 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m14:51:19.194057 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_COGS.efb9b66052
[0m14:51:19.196131 [info ] [Thread-1 (]: 3 of 5 START test not_null_dim_info_COGS ....................................... [RUN]
[0m14:51:19.197511 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m14:51:19.197511 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_COGS.efb9b66052
[0m14:51:19.198949 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_COGS.efb9b66052
[0m14:51:19.202539 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m14:51:19.205719 [debug] [Thread-1 (]: finished collecting timing info
[0m14:51:19.205719 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_COGS.efb9b66052
[0m14:51:19.209806 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m14:51:19.209806 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:51:19.209806 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m14:51:19.214007 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_COGS.efb9b66052"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select COGS
from finance.dim_info
where COGS is null



      
    ) dbt_internal_test
[0m14:51:19.214007 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:51:21.465061 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_COGS.efb9b66052"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select COGS
from finance.dim_info
where COGS is null



      
    ) dbt_internal_test
[0m14:51:21.465061 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: finance.dim_info; line 14 pos 5
[0m14:51:21.467758 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: finance.dim_info; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: finance.dim_info; line 14 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m14:51:21.467758 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\xfa\xa5\xb8y\x05@I\x80\x95\xf1\xb2\x8a\xff\xb4\x94\xdb'
[0m14:51:21.467758 [debug] [Thread-1 (]: finished collecting timing info
[0m14:51:21.467758 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: ROLLBACK
[0m14:51:21.467758 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:51:21.467758 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: Close
[0m14:51:22.547542 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_info_COGS (models\schema.yml)
  Table or view not found: finance.dim_info; line 14 pos 5
[0m14:51:22.547542 [error] [Thread-1 (]: 3 of 5 ERROR not_null_dim_info_COGS ............................................ [[31mERROR[0m in 3.35s]
[0m14:51:22.550087 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_COGS.efb9b66052
[0m14:51:22.550087 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Inventory.45016db171
[0m14:51:22.550087 [info ] [Thread-1 (]: 4 of 5 START test not_null_dim_info_Inventory .................................. [RUN]
[0m14:51:22.550087 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Inventory.45016db171"
[0m14:51:22.550087 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Inventory.45016db171
[0m14:51:22.550087 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Inventory.45016db171
[0m14:51:22.557787 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Inventory.45016db171"
[0m14:51:22.557787 [debug] [Thread-1 (]: finished collecting timing info
[0m14:51:22.562074 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Inventory.45016db171
[0m14:51:22.562855 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Inventory.45016db171"
[0m14:51:22.562855 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:51:22.562855 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Inventory.45016db171"
[0m14:51:22.562855 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Inventory.45016db171"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Inventory
from finance.dim_info
where Inventory is null



      
    ) dbt_internal_test
[0m14:51:22.562855 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:51:25.177909 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Inventory.45016db171"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Inventory
from finance.dim_info
where Inventory is null



      
    ) dbt_internal_test
[0m14:51:25.177909 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: finance.dim_info; line 14 pos 5
[0m14:51:25.177909 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: finance.dim_info; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: finance.dim_info; line 14 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m14:51:25.177909 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\xdf\xc0\xc6\n\xcbpIe\xb8\x92\x82\xed\xf7\x08\xa7\x0f'
[0m14:51:25.177909 [debug] [Thread-1 (]: finished collecting timing info
[0m14:51:25.177909 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: ROLLBACK
[0m14:51:25.177909 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:51:25.177909 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: Close
[0m14:51:26.397527 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_info_Inventory (models\schema.yml)
  Table or view not found: finance.dim_info; line 14 pos 5
[0m14:51:26.397527 [error] [Thread-1 (]: 4 of 5 ERROR not_null_dim_info_Inventory ....................................... [[31mERROR[0m in 3.85s]
[0m14:51:26.403575 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Inventory.45016db171
[0m14:51:26.403575 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m14:51:26.403575 [info ] [Thread-1 (]: 5 of 5 START test not_null_dim_info_Segment .................................... [RUN]
[0m14:51:26.409568 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m14:51:26.409568 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m14:51:26.409568 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m14:51:26.414955 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m14:51:26.417871 [debug] [Thread-1 (]: finished collecting timing info
[0m14:51:26.417967 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m14:51:26.419201 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m14:51:26.419201 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:51:26.419201 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m14:51:26.424317 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Segment.57cc6a2d98"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Segment
from finance.dim_info
where Segment is null



      
    ) dbt_internal_test
[0m14:51:26.424317 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:51:31.228130 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Segment.57cc6a2d98"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Segment
from finance.dim_info
where Segment is null



      
    ) dbt_internal_test
[0m14:51:31.237804 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: finance.dim_info; line 14 pos 5
[0m14:51:31.237804 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: finance.dim_info; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: finance.dim_info; line 14 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m14:51:31.237804 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\xdf\xa5\xd2\xfd\xa3\xfeEc\xa4\x9cu1\x1eQ\xaf\xb9'
[0m14:51:31.237804 [debug] [Thread-1 (]: finished collecting timing info
[0m14:51:31.241959 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: ROLLBACK
[0m14:51:31.241959 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:51:31.243224 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: Close
[0m14:51:32.487549 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_info_Segment (models\schema.yml)
  Table or view not found: finance.dim_info; line 14 pos 5
[0m14:51:32.487549 [error] [Thread-1 (]: 5 of 5 ERROR not_null_dim_info_Segment ......................................... [[31mERROR[0m in 6.08s]
[0m14:51:32.487549 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m14:51:32.498062 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:51:32.498062 [debug] [MainThread]: On master: ROLLBACK
[0m14:51:32.499747 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:51:33.521909 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:51:33.521909 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:51:33.521909 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:51:33.525033 [debug] [MainThread]: On master: ROLLBACK
[0m14:51:33.525538 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:51:33.526441 [debug] [MainThread]: On master: Close
[0m14:51:34.818183 [info ] [MainThread]: 
[0m14:51:34.818183 [info ] [MainThread]: Finished running 5 tests in 0 hours 0 minutes and 28.73 seconds (28.73s).
[0m14:51:34.821560 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:51:34.821560 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:51:34.821560 [debug] [MainThread]: Connection 'test.finance.not_null_dim_info_Segment.57cc6a2d98' was properly closed.
[0m14:51:34.832874 [info ] [MainThread]: 
[0m14:51:34.837894 [info ] [MainThread]: [31mCompleted with 5 errors and 0 warnings:[0m
[0m14:51:34.840553 [info ] [MainThread]: 
[0m14:51:34.893489 [error] [MainThread]: [33mRuntime Error in test accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business (models\schema.yml)[0m
[0m14:51:34.934048 [error] [MainThread]:   Table or view not found: finance.dim_info; line 17 pos 9
[0m14:51:34.985848 [info ] [MainThread]: 
[0m14:51:34.987878 [error] [MainThread]: [33mRuntime Error in test not_null_dim_info_Actual (models\schema.yml)[0m
[0m14:51:35.020846 [error] [MainThread]:   Table or view not found: finance.dim_info; line 14 pos 5
[0m14:51:35.026682 [info ] [MainThread]: 
[0m14:51:35.047813 [error] [MainThread]: [33mRuntime Error in test not_null_dim_info_COGS (models\schema.yml)[0m
[0m14:51:35.047813 [error] [MainThread]:   Table or view not found: finance.dim_info; line 14 pos 5
[0m14:51:35.067783 [info ] [MainThread]: 
[0m14:51:35.070148 [error] [MainThread]: [33mRuntime Error in test not_null_dim_info_Inventory (models\schema.yml)[0m
[0m14:51:35.088079 [error] [MainThread]:   Table or view not found: finance.dim_info; line 14 pos 5
[0m14:51:35.088079 [info ] [MainThread]: 
[0m14:51:35.107731 [error] [MainThread]: [33mRuntime Error in test not_null_dim_info_Segment (models\schema.yml)[0m
[0m14:51:35.107731 [error] [MainThread]:   Table or view not found: finance.dim_info; line 14 pos 5
[0m14:51:35.107731 [info ] [MainThread]: 
[0m14:51:35.107731 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=5 SKIP=0 TOTAL=5
[0m14:51:35.119402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC2E7131F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC2E8737F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC2E873B50>]}
[0m14:51:35.119402 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:52:22.267733 | ca89c376-6d46-41c1-b8a9-8462359f89f8 ==============================
[0m14:52:22.267733 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:52:22.272741 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'select': ['dim_exp'], 'which': 'test', 'rpc_method': 'test'}
[0m14:52:22.272741 [debug] [MainThread]: Tracking: tracking
[0m14:52:22.287267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151FF609C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151FF609870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151FF609AB0>]}
[0m14:52:22.367538 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:52:22.367538 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:52:22.367538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ca89c376-6d46-41c1-b8a9-8462359f89f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151FF828310>]}
[0m14:52:22.382797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ca89c376-6d46-41c1-b8a9-8462359f89f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151FF7F88E0>]}
[0m14:52:22.382797 [info ] [MainThread]: Found 8 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:52:22.382797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ca89c376-6d46-41c1-b8a9-8462359f89f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151FF7F8520>]}
[0m14:52:22.387369 [info ] [MainThread]: 
[0m14:52:22.387369 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:52:22.403390 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:52:22.407366 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:52:22.407366 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:52:22.407366 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:52:22.407366 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:52:26.142166 [debug] [ThreadPool]: SQL status: OK in 3.73 seconds
[0m14:52:26.147352 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:52:26.147352 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:52:26.147352 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:52:27.368414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ca89c376-6d46-41c1-b8a9-8462359f89f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151FF5F6710>]}
[0m14:52:27.368414 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:52:27.368414 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:52:27.368414 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:52:27.368414 [info ] [MainThread]: 
[0m14:52:27.388399 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m14:52:27.388399 [info ] [Thread-1 (]: 1 of 3 START test not_null_dim_exp_Expenses .................................... [RUN]
[0m14:52:27.395994 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m14:52:27.395994 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m14:52:27.397770 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m14:52:27.412022 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m14:52:27.413534 [debug] [Thread-1 (]: finished collecting timing info
[0m14:52:27.413534 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m14:52:27.427766 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m14:52:27.432132 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:52:27.432132 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m14:52:27.432132 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Expenses.c94546fc2a: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Expenses.c94546fc2a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Expenses
from finance.dim_exp
where Expenses is null



      
    ) dbt_internal_test
[0m14:52:27.432132 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:52:32.497902 [debug] [Thread-1 (]: SQL status: OK in 5.07 seconds
[0m14:52:32.517835 [debug] [Thread-1 (]: finished collecting timing info
[0m14:52:32.517835 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Expenses.c94546fc2a: ROLLBACK
[0m14:52:32.519977 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:52:32.519977 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Expenses.c94546fc2a: Close
[0m14:52:36.157686 [info ] [Thread-1 (]: 1 of 3 PASS not_null_dim_exp_Expenses .......................................... [[32mPASS[0m in 8.76s]
[0m14:52:36.167385 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m14:52:36.167385 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m14:52:36.167385 [info ] [Thread-1 (]: 2 of 3 START test not_null_dim_exp_Gross_sales ................................. [RUN]
[0m14:52:36.167385 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m14:52:36.167385 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m14:52:36.167385 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m14:52:36.237862 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m14:52:36.242608 [debug] [Thread-1 (]: finished collecting timing info
[0m14:52:36.243153 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m14:52:36.243876 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m14:52:36.243876 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:52:36.243876 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m14:52:36.243876 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Gross_sales.07439d5e88: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Gross_sales
from finance.dim_exp
where Gross_sales is null



      
    ) dbt_internal_test
[0m14:52:36.247603 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:52:41.117387 [debug] [Thread-1 (]: SQL status: OK in 4.87 seconds
[0m14:52:41.117387 [debug] [Thread-1 (]: finished collecting timing info
[0m14:52:41.117387 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Gross_sales.07439d5e88: ROLLBACK
[0m14:52:41.127605 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:52:41.127605 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Gross_sales.07439d5e88: Close
[0m14:52:42.937239 [info ] [Thread-1 (]: 2 of 3 PASS not_null_dim_exp_Gross_sales ....................................... [[32mPASS[0m in 6.77s]
[0m14:52:42.937239 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m14:52:42.937239 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m14:52:42.942625 [info ] [Thread-1 (]: 3 of 3 START test not_null_dim_exp_Manufacturing_price ......................... [RUN]
[0m14:52:42.942625 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m14:52:42.942625 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m14:52:42.942625 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m14:52:42.947318 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m14:52:42.947318 [debug] [Thread-1 (]: finished collecting timing info
[0m14:52:42.947318 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m14:52:42.953548 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m14:52:42.953548 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:52:42.953548 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m14:52:42.957616 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Manufacturing_price
from finance.dim_exp
where Manufacturing_price is null



      
    ) dbt_internal_test
[0m14:52:42.957616 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:52:46.023909 [debug] [Thread-1 (]: SQL status: OK in 3.07 seconds
[0m14:52:46.027515 [debug] [Thread-1 (]: finished collecting timing info
[0m14:52:46.027515 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f: ROLLBACK
[0m14:52:46.027515 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:52:46.032555 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f: Close
[0m14:52:47.227349 [info ] [Thread-1 (]: 3 of 3 PASS not_null_dim_exp_Manufacturing_price ............................... [[32mPASS[0m in 4.28s]
[0m14:52:47.238886 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m14:52:47.238886 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:52:47.242605 [debug] [MainThread]: On master: ROLLBACK
[0m14:52:47.242605 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:52:48.457369 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:52:48.457369 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:52:48.457369 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:52:48.466302 [debug] [MainThread]: On master: ROLLBACK
[0m14:52:48.466302 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:52:48.467348 [debug] [MainThread]: On master: Close
[0m14:52:50.457373 [info ] [MainThread]: 
[0m14:52:50.457373 [info ] [MainThread]: Finished running 3 tests in 0 hours 0 minutes and 28.07 seconds (28.07s).
[0m14:52:50.457373 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:52:50.457373 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:52:50.467272 [debug] [MainThread]: Connection 'test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f' was properly closed.
[0m14:52:50.477433 [info ] [MainThread]: 
[0m14:52:50.477433 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:52:50.477433 [info ] [MainThread]: 
[0m14:52:50.477433 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m14:52:50.484335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151FF7A3250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151FF619900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151FF8D3940>]}
[0m14:52:50.484833 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:53:19.442580 | 853de458-a455-4d02-8f6b-ef529ca862fc ==============================
[0m14:53:19.442580 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:53:19.443929 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'select': ['dim_unit'], 'which': 'test', 'rpc_method': 'test'}
[0m14:53:19.443929 [debug] [MainThread]: Tracking: tracking
[0m14:53:19.459760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA6DBA9C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA6DBA9900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA6DBAB340>]}
[0m14:53:19.537113 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:53:19.537113 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:53:19.547421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '853de458-a455-4d02-8f6b-ef529ca862fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA6ED982B0>]}
[0m14:53:19.557335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '853de458-a455-4d02-8f6b-ef529ca862fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA6ED68880>]}
[0m14:53:19.557335 [info ] [MainThread]: Found 8 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:53:19.559775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '853de458-a455-4d02-8f6b-ef529ca862fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA6ED686A0>]}
[0m14:53:19.559775 [info ] [MainThread]: 
[0m14:53:19.559775 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:53:19.575276 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:53:19.577261 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:53:19.577261 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:53:19.587253 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:53:19.587253 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:53:22.577233 [debug] [ThreadPool]: SQL status: OK in 2.99 seconds
[0m14:53:22.597433 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:53:22.602416 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:53:22.602867 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:53:23.687100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '853de458-a455-4d02-8f6b-ef529ca862fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA6DB956F0>]}
[0m14:53:23.687234 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:53:23.687234 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:53:23.688712 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:53:23.689205 [info ] [MainThread]: 
[0m14:53:23.707434 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m14:53:23.707434 [info ] [Thread-1 (]: 1 of 3 START test not_null_dim_unit_Target ..................................... [RUN]
[0m14:53:23.707434 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m14:53:23.707434 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m14:53:23.707434 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Target.33e9c88d57
[0m14:53:23.717542 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m14:53:23.727371 [debug] [Thread-1 (]: finished collecting timing info
[0m14:53:23.727899 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m14:53:23.737525 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m14:53:23.737525 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:53:23.737525 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m14:53:23.746713 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Target.33e9c88d57: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Target.33e9c88d57"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Target
from finance.dim_unit
where Target is null



      
    ) dbt_internal_test
[0m14:53:23.746841 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:53:26.654988 [debug] [Thread-1 (]: SQL status: OK in 2.91 seconds
[0m14:53:26.667566 [debug] [Thread-1 (]: finished collecting timing info
[0m14:53:26.667566 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Target.33e9c88d57: ROLLBACK
[0m14:53:26.671202 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:53:26.671202 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Target.33e9c88d57: Close
[0m14:53:28.214887 [info ] [Thread-1 (]: 1 of 3 PASS not_null_dim_unit_Target ........................................... [[32mPASS[0m in 4.51s]
[0m14:53:28.217440 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m14:53:28.221068 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m14:53:28.221564 [info ] [Thread-1 (]: 2 of 3 START test not_null_dim_unit_Unit_Price ................................. [RUN]
[0m14:53:28.221564 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m14:53:28.225181 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m14:53:28.225181 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m14:53:28.297692 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m14:53:28.305998 [debug] [Thread-1 (]: finished collecting timing info
[0m14:53:28.306508 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m14:53:28.312233 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m14:53:28.314741 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:53:28.314741 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m14:53:28.316991 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Unit_Price.912d40d17b: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Unit_Price
from finance.dim_unit
where Unit_Price is null



      
    ) dbt_internal_test
[0m14:53:28.317839 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:30.758195 [debug] [Thread-1 (]: SQL status: OK in 2.44 seconds
[0m14:53:30.765955 [debug] [Thread-1 (]: finished collecting timing info
[0m14:53:30.767464 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Unit_Price.912d40d17b: ROLLBACK
[0m14:53:30.769517 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:53:30.769517 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Unit_Price.912d40d17b: Close
[0m14:53:32.997805 [info ] [Thread-1 (]: 2 of 3 PASS not_null_dim_unit_Unit_Price ....................................... [[32mPASS[0m in 4.78s]
[0m14:53:32.997805 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m14:53:32.997805 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m14:53:32.997805 [info ] [Thread-1 (]: 3 of 3 START test not_null_dim_unit_Units_Sold ................................. [RUN]
[0m14:53:32.997805 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m14:53:32.997805 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m14:53:33.007500 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m14:53:33.012504 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m14:53:33.013507 [debug] [Thread-1 (]: finished collecting timing info
[0m14:53:33.013507 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m14:53:33.017512 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m14:53:33.021726 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:53:33.021726 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m14:53:33.022221 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Units_Sold.fd5939c596: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Units_Sold
from finance.dim_unit
where Units_Sold is null



      
    ) dbt_internal_test
[0m14:53:33.022221 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:36.147431 [debug] [Thread-1 (]: SQL status: OK in 3.13 seconds
[0m14:53:36.153223 [debug] [Thread-1 (]: finished collecting timing info
[0m14:53:36.153223 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Units_Sold.fd5939c596: ROLLBACK
[0m14:53:36.154733 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:53:36.154970 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Units_Sold.fd5939c596: Close
[0m14:53:37.607533 [info ] [Thread-1 (]: 3 of 3 PASS not_null_dim_unit_Units_Sold ....................................... [[32mPASS[0m in 4.61s]
[0m14:53:37.607533 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m14:53:37.617463 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:53:37.617463 [debug] [MainThread]: On master: ROLLBACK
[0m14:53:37.617463 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:53:38.907439 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:53:38.907439 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:53:38.907439 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:53:38.917107 [debug] [MainThread]: On master: ROLLBACK
[0m14:53:38.917909 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:53:38.918170 [debug] [MainThread]: On master: Close
[0m14:53:40.167200 [info ] [MainThread]: 
[0m14:53:40.167200 [info ] [MainThread]: Finished running 3 tests in 0 hours 0 minutes and 20.61 seconds (20.61s).
[0m14:53:40.178598 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:53:40.178598 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:53:40.178598 [debug] [MainThread]: Connection 'test.finance.not_null_dim_unit_Units_Sold.fd5939c596' was properly closed.
[0m14:53:40.187101 [info ] [MainThread]: 
[0m14:53:40.197489 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:53:40.198634 [info ] [MainThread]: 
[0m14:53:40.198634 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m14:53:40.217482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA6ED131F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA6DBBB4C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA6EE3F940>]}
[0m14:53:40.217482 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:54:00.547089 | 7e9c62e6-79c6-4c26-8dd3-ab28dd92fbe6 ==============================
[0m14:54:00.547089 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:54:00.547193 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'select': ['dim_month'], 'which': 'test', 'rpc_method': 'test'}
[0m14:54:00.547193 [debug] [MainThread]: Tracking: tracking
[0m14:54:00.558404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A58C41DC90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A58C41D900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A58C41F340>]}
[0m14:54:00.642281 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:54:00.642281 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:54:00.647451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7e9c62e6-79c6-4c26-8dd3-ab28dd92fbe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A58D60C280>]}
[0m14:54:00.657059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7e9c62e6-79c6-4c26-8dd3-ab28dd92fbe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A58D5DC850>]}
[0m14:54:00.661874 [info ] [MainThread]: Found 8 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:54:00.662113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7e9c62e6-79c6-4c26-8dd3-ab28dd92fbe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A58D5DC670>]}
[0m14:54:00.662113 [info ] [MainThread]: 
[0m14:54:00.666532 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:54:00.677370 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:54:00.692058 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:54:00.692058 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:54:00.692058 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:54:00.693062 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:54:04.897464 [debug] [ThreadPool]: SQL status: OK in 4.2 seconds
[0m14:54:04.901904 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:54:04.907519 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:54:04.907519 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:54:06.088275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7e9c62e6-79c6-4c26-8dd3-ab28dd92fbe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A58C41DCC0>]}
[0m14:54:06.088275 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:54:06.088275 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:54:06.088275 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:54:06.088275 [info ] [MainThread]: 
[0m14:54:06.107067 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Date.ce09cea79f
[0m14:54:06.107067 [info ] [Thread-1 (]: 1 of 4 START test not_null_dim_month_Date ...................................... [RUN]
[0m14:54:06.113681 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m14:54:06.113681 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Date.ce09cea79f
[0m14:54:06.115540 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Date.ce09cea79f
[0m14:54:06.127405 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m14:54:06.131937 [debug] [Thread-1 (]: finished collecting timing info
[0m14:54:06.132070 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Date.ce09cea79f
[0m14:54:06.147728 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m14:54:06.151092 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:54:06.151092 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m14:54:06.151092 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Date.ce09cea79f: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Date.ce09cea79f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Date
from finance.dim_month
where Date is null



      
    ) dbt_internal_test
[0m14:54:06.151092 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:54:13.247775 [debug] [Thread-1 (]: SQL status: OK in 7.1 seconds
[0m14:54:13.257296 [debug] [Thread-1 (]: finished collecting timing info
[0m14:54:13.257296 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Date.ce09cea79f: ROLLBACK
[0m14:54:13.257296 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:54:13.260601 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Date.ce09cea79f: Close
[0m14:54:19.177181 [info ] [Thread-1 (]: 1 of 4 PASS not_null_dim_month_Date ............................................ [[32mPASS[0m in 13.06s]
[0m14:54:19.177181 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Date.ce09cea79f
[0m14:54:19.177181 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m14:54:19.177181 [info ] [Thread-1 (]: 2 of 4 START test not_null_dim_month_MonthName ................................. [RUN]
[0m14:54:19.186686 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m14:54:19.187302 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m14:54:19.187302 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m14:54:19.227390 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m14:54:19.227390 [debug] [Thread-1 (]: finished collecting timing info
[0m14:54:19.227390 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m14:54:19.237062 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m14:54:19.237062 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:54:19.237062 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m14:54:19.241649 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_MonthName.c1d116d50b: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_MonthName.c1d116d50b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select MonthName
from finance.dim_month
where MonthName is null



      
    ) dbt_internal_test
[0m14:54:19.241649 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:54:22.669838 [debug] [Thread-1 (]: SQL status: OK in 3.43 seconds
[0m14:54:22.669838 [debug] [Thread-1 (]: finished collecting timing info
[0m14:54:22.677033 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_MonthName.c1d116d50b: ROLLBACK
[0m14:54:22.677033 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:54:22.677033 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_MonthName.c1d116d50b: Close
[0m14:54:24.526981 [info ] [Thread-1 (]: 2 of 4 PASS not_null_dim_month_MonthName ....................................... [[32mPASS[0m in 5.34s]
[0m14:54:24.526981 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m14:54:24.526981 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m14:54:24.531687 [info ] [Thread-1 (]: 3 of 4 START test not_null_dim_month_Month_Number .............................. [RUN]
[0m14:54:24.533000 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m14:54:24.533000 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m14:54:24.533000 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m14:54:24.537215 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m14:54:24.541926 [debug] [Thread-1 (]: finished collecting timing info
[0m14:54:24.542500 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m14:54:24.542500 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m14:54:24.545859 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:54:24.545859 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m14:54:24.545859 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Month_Number.be337b93c3: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Month_Number.be337b93c3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Month_Number
from finance.dim_month
where Month_Number is null



      
    ) dbt_internal_test
[0m14:54:24.546775 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:54:28.027555 [debug] [Thread-1 (]: SQL status: OK in 3.48 seconds
[0m14:54:28.027555 [debug] [Thread-1 (]: finished collecting timing info
[0m14:54:28.027555 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Month_Number.be337b93c3: ROLLBACK
[0m14:54:28.027555 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:54:28.036880 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Month_Number.be337b93c3: Close
[0m14:54:31.157159 [info ] [Thread-1 (]: 3 of 4 PASS not_null_dim_month_Month_Number .................................... [[32mPASS[0m in 6.62s]
[0m14:54:31.157159 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m14:54:31.157159 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m14:54:31.157159 [info ] [Thread-1 (]: 4 of 4 START test not_null_dim_month_Year ...................................... [RUN]
[0m14:54:31.162726 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m14:54:31.162726 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m14:54:31.162726 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Year.5d11bba8bb
[0m14:54:31.167097 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m14:54:31.167097 [debug] [Thread-1 (]: finished collecting timing info
[0m14:54:31.167097 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m14:54:31.167097 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m14:54:31.178268 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:54:31.178585 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m14:54:31.178585 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Year.5d11bba8bb: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Year.5d11bba8bb"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Year
from finance.dim_month
where Year is null



      
    ) dbt_internal_test
[0m14:54:31.178585 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:54:34.343754 [debug] [Thread-1 (]: SQL status: OK in 3.17 seconds
[0m14:54:34.350209 [debug] [Thread-1 (]: finished collecting timing info
[0m14:54:34.350209 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Year.5d11bba8bb: ROLLBACK
[0m14:54:34.352487 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:54:34.352593 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Year.5d11bba8bb: Close
[0m14:54:35.677230 [info ] [Thread-1 (]: 4 of 4 PASS not_null_dim_month_Year ............................................ [[32mPASS[0m in 4.52s]
[0m14:54:35.677230 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m14:54:35.687253 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:54:35.687253 [debug] [MainThread]: On master: ROLLBACK
[0m14:54:35.687253 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:54:36.917413 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:54:36.917567 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:54:36.917567 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:54:36.917567 [debug] [MainThread]: On master: ROLLBACK
[0m14:54:36.917567 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:54:36.921533 [debug] [MainThread]: On master: Close
[0m14:54:38.647242 [info ] [MainThread]: 
[0m14:54:38.647242 [info ] [MainThread]: Finished running 4 tests in 0 hours 0 minutes and 37.98 seconds (37.98s).
[0m14:54:38.647242 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:54:38.647242 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:54:38.647242 [debug] [MainThread]: Connection 'test.finance.not_null_dim_month_Year.5d11bba8bb' was properly closed.
[0m14:54:38.677971 [info ] [MainThread]: 
[0m14:54:38.677971 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:54:38.677971 [info ] [MainThread]: 
[0m14:54:38.697233 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m14:54:38.697233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A58C5B31C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A58D6D71C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A58D6D6B60>]}
[0m14:54:38.697233 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:55:09.281912 | 3f1017ef-0efb-4198-9654-95ca557dda65 ==============================
[0m14:55:09.281912 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:55:09.281912 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'select': ['dim_profit'], 'which': 'test', 'rpc_method': 'test'}
[0m14:55:09.281912 [debug] [MainThread]: Tracking: tracking
[0m14:55:09.297044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E654199C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E654199870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E654199AB0>]}
[0m14:55:09.373101 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:55:09.373101 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:55:09.377095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3f1017ef-0efb-4198-9654-95ca557dda65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6553882E0>]}
[0m14:55:09.387017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3f1017ef-0efb-4198-9654-95ca557dda65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6553588B0>]}
[0m14:55:09.387017 [info ] [MainThread]: Found 8 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:55:09.394756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3f1017ef-0efb-4198-9654-95ca557dda65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6553584F0>]}
[0m14:55:09.397064 [info ] [MainThread]: 
[0m14:55:09.399412 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:55:09.410446 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:55:09.417175 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:09.417175 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:55:09.417175 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:55:09.424673 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:55:11.877575 [debug] [ThreadPool]: SQL status: OK in 2.45 seconds
[0m14:55:11.887786 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:55:11.887786 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:55:11.887786 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:55:13.047458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3f1017ef-0efb-4198-9654-95ca557dda65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6553584C0>]}
[0m14:55:13.047458 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:13.047458 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:55:13.056871 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:55:13.056871 [info ] [MainThread]: 
[0m14:55:13.067108 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m14:55:13.067108 [info ] [Thread-1 (]: 1 of 6 START test not_null_dim_profit_Country .................................. [RUN]
[0m14:55:13.076821 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m14:55:13.077327 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m14:55:13.077847 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Country.363de9fbd5
[0m14:55:13.092269 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m14:55:13.093891 [debug] [Thread-1 (]: finished collecting timing info
[0m14:55:13.093891 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m14:55:13.107448 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m14:55:13.112772 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:13.112772 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m14:55:13.112772 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Country.363de9fbd5: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Country.363de9fbd5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Country
from finance.dim_profit
where Country is null



      
    ) dbt_internal_test
[0m14:55:13.112772 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:55:17.387410 [debug] [Thread-1 (]: SQL status: OK in 4.27 seconds
[0m14:55:17.397148 [debug] [Thread-1 (]: finished collecting timing info
[0m14:55:17.397148 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Country.363de9fbd5: ROLLBACK
[0m14:55:17.397148 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:55:17.402594 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Country.363de9fbd5: Close
[0m14:55:18.587024 [info ] [Thread-1 (]: 1 of 6 PASS not_null_dim_profit_Country ........................................ [[32mPASS[0m in 5.52s]
[0m14:55:18.587024 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m14:55:18.587024 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m14:55:18.587024 [info ] [Thread-1 (]: 2 of 6 START test not_null_dim_profit_Discount_Band ............................ [RUN]
[0m14:55:18.587024 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m14:55:18.587024 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m14:55:18.587024 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m14:55:18.637215 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m14:55:18.637215 [debug] [Thread-1 (]: finished collecting timing info
[0m14:55:18.646530 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m14:55:18.651385 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m14:55:18.653559 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:18.653977 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m14:55:18.654476 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Discount_Band
from finance.dim_profit
where Discount_Band is null



      
    ) dbt_internal_test
[0m14:55:18.655028 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:55:21.037567 [debug] [Thread-1 (]: SQL status: OK in 2.38 seconds
[0m14:55:21.047116 [debug] [Thread-1 (]: finished collecting timing info
[0m14:55:21.047116 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa: ROLLBACK
[0m14:55:21.047116 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:55:21.051518 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa: Close
[0m14:55:22.607272 [info ] [Thread-1 (]: 2 of 6 PASS not_null_dim_profit_Discount_Band .................................. [[32mPASS[0m in 4.02s]
[0m14:55:22.607272 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m14:55:22.607272 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m14:55:22.613979 [info ] [Thread-1 (]: 3 of 6 START test not_null_dim_profit_Discounts ................................ [RUN]
[0m14:55:22.615005 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m14:55:22.615005 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m14:55:22.617172 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Discounts.03e349480e
[0m14:55:22.617172 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m14:55:22.622176 [debug] [Thread-1 (]: finished collecting timing info
[0m14:55:22.622176 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m14:55:22.623435 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m14:55:22.623435 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:22.623435 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m14:55:22.627261 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discounts.03e349480e: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Discounts.03e349480e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Discounts
from finance.dim_profit
where Discounts is null



      
    ) dbt_internal_test
[0m14:55:22.627552 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:55:28.837215 [debug] [Thread-1 (]: SQL status: OK in 6.21 seconds
[0m14:55:28.841997 [debug] [Thread-1 (]: finished collecting timing info
[0m14:55:28.841997 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discounts.03e349480e: ROLLBACK
[0m14:55:28.842944 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:55:28.843224 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discounts.03e349480e: Close
[0m14:55:31.279223 [info ] [Thread-1 (]: 3 of 6 PASS not_null_dim_profit_Discounts ...................................... [[32mPASS[0m in 8.66s]
[0m14:55:31.279223 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m14:55:31.279223 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Product.1422e74993
[0m14:55:31.279223 [info ] [Thread-1 (]: 4 of 6 START test not_null_dim_profit_Product .................................. [RUN]
[0m14:55:31.286893 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Product.1422e74993"
[0m14:55:31.286893 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Product.1422e74993
[0m14:55:31.286893 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Product.1422e74993
[0m14:55:31.286893 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Product.1422e74993"
[0m14:55:31.286893 [debug] [Thread-1 (]: finished collecting timing info
[0m14:55:31.286893 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Product.1422e74993
[0m14:55:31.297199 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Product.1422e74993"
[0m14:55:31.298388 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:31.298388 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Product.1422e74993"
[0m14:55:31.298388 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Product.1422e74993: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Product.1422e74993"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Product
from finance.dim_profit
where Product is null



      
    ) dbt_internal_test
[0m14:55:31.298388 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:55:33.858596 [debug] [Thread-1 (]: SQL status: OK in 2.56 seconds
[0m14:55:33.867228 [debug] [Thread-1 (]: finished collecting timing info
[0m14:55:33.867228 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Product.1422e74993: ROLLBACK
[0m14:55:33.871180 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:55:33.871316 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Product.1422e74993: Close
[0m14:55:35.067022 [info ] [Thread-1 (]: 4 of 6 PASS not_null_dim_profit_Product ........................................ [[32mPASS[0m in 3.79s]
[0m14:55:35.067022 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Product.1422e74993
[0m14:55:35.067022 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m14:55:35.077231 [info ] [Thread-1 (]: 5 of 6 START test not_null_dim_profit_Profit ................................... [RUN]
[0m14:55:35.077231 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m14:55:35.077231 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m14:55:35.080823 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m14:55:35.087144 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m14:55:35.087144 [debug] [Thread-1 (]: finished collecting timing info
[0m14:55:35.087144 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m14:55:35.087144 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m14:55:35.094252 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:35.094252 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m14:55:35.094252 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Profit.3c5f35db12: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Profit.3c5f35db12"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Profit
from finance.dim_profit
where Profit is null



      
    ) dbt_internal_test
[0m14:55:35.096763 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:55:37.777021 [debug] [Thread-1 (]: SQL status: OK in 2.68 seconds
[0m14:55:37.787182 [debug] [Thread-1 (]: finished collecting timing info
[0m14:55:37.792187 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Profit.3c5f35db12: ROLLBACK
[0m14:55:37.792187 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:55:37.793195 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Profit.3c5f35db12: Close
[0m14:55:39.118971 [info ] [Thread-1 (]: 5 of 6 PASS not_null_dim_profit_Profit ......................................... [[32mPASS[0m in 4.04s]
[0m14:55:39.118971 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m14:55:39.118971 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m14:55:39.118971 [info ] [Thread-1 (]: 6 of 6 START test not_null_dim_profit_Sales .................................... [RUN]
[0m14:55:39.127112 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m14:55:39.127112 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m14:55:39.129376 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Sales.05c16c548a
[0m14:55:39.143567 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m14:55:39.147203 [debug] [Thread-1 (]: finished collecting timing info
[0m14:55:39.147203 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m14:55:39.147203 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m14:55:39.157146 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:39.157146 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m14:55:39.159276 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Sales.05c16c548a: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Sales.05c16c548a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Sales
from finance.dim_profit
where Sales is null



      
    ) dbt_internal_test
[0m14:55:39.159806 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:55:43.257444 [debug] [Thread-1 (]: SQL status: OK in 4.1 seconds
[0m14:55:43.257444 [debug] [Thread-1 (]: finished collecting timing info
[0m14:55:43.257444 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Sales.05c16c548a: ROLLBACK
[0m14:55:43.266995 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:55:43.267181 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Sales.05c16c548a: Close
[0m14:55:44.589989 [info ] [Thread-1 (]: 6 of 6 PASS not_null_dim_profit_Sales .......................................... [[32mPASS[0m in 5.46s]
[0m14:55:44.589989 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m14:55:44.597023 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:55:44.597023 [debug] [MainThread]: On master: ROLLBACK
[0m14:55:44.597023 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:55:45.821840 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:55:45.821840 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:45.821840 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:55:45.826977 [debug] [MainThread]: On master: ROLLBACK
[0m14:55:45.828248 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:55:45.828818 [debug] [MainThread]: On master: Close
[0m14:55:47.048432 [info ] [MainThread]: 
[0m14:55:47.048432 [info ] [MainThread]: Finished running 6 tests in 0 hours 0 minutes and 37.65 seconds (37.65s).
[0m14:55:47.051730 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:55:47.051730 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:55:47.051730 [debug] [MainThread]: Connection 'test.finance.not_null_dim_profit_Sales.05c16c548a' was properly closed.
[0m14:55:47.062420 [info ] [MainThread]: 
[0m14:55:47.067041 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:55:47.067041 [info ] [MainThread]: 
[0m14:55:47.087186 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m14:55:47.087186 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E654333220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E65542FE80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E65542F6A0>]}
[0m14:55:47.087186 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:56:06.567057 | 46ebae10-7edb-47a2-8bac-e009f0d030ce ==============================
[0m14:56:06.567057 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:56:06.567057 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/dim_info.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:56:06.567057 [debug] [MainThread]: Tracking: tracking
[0m14:56:06.593704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022294E99C60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022294E998A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022294E99FC0>]}
[0m14:56:06.672037 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:56:06.677136 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:56:06.681779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '46ebae10-7edb-47a2-8bac-e009f0d030ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022296088340>]}
[0m14:56:06.687920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '46ebae10-7edb-47a2-8bac-e009f0d030ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022296058910>]}
[0m14:56:06.687920 [info ] [MainThread]: Found 8 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:56:06.695356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '46ebae10-7edb-47a2-8bac-e009f0d030ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022296058550>]}
[0m14:56:06.696686 [info ] [MainThread]: 
[0m14:56:06.696686 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:56:06.707090 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m14:56:06.717017 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:56:06.717017 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:56:06.717017 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:56:12.377136 [debug] [ThreadPool]: SQL status: OK in 5.66 seconds
[0m14:56:12.386930 [debug] [ThreadPool]: On list_schemas: Close
[0m14:56:17.826779 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:56:17.826779 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:56:17.826779 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:56:17.826779 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:56:17.826779 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:56:23.387216 [debug] [ThreadPool]: SQL status: OK in 5.56 seconds
[0m14:56:23.397233 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:56:23.397233 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:56:23.397233 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:56:24.671745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '46ebae10-7edb-47a2-8bac-e009f0d030ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022294E98970>]}
[0m14:56:24.671745 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:56:24.671745 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:56:24.671745 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:56:24.671745 [info ] [MainThread]: 
[0m14:56:24.687069 [debug] [Thread-1 (]: Began running node model.finance.dim_info
[0m14:56:24.687069 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.dim_info .................................. [RUN]
[0m14:56:24.693307 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_info"
[0m14:56:24.693307 [debug] [Thread-1 (]: Began compiling node model.finance.dim_info
[0m14:56:24.695252 [debug] [Thread-1 (]: Compiling model.finance.dim_info
[0m14:56:24.697784 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_info"
[0m14:56:24.697784 [debug] [Thread-1 (]: finished collecting timing info
[0m14:56:24.697784 [debug] [Thread-1 (]: Began executing node model.finance.dim_info
[0m14:56:24.797852 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_info"
[0m14:56:24.797852 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:56:24.797852 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_info"
[0m14:56:24.799864 [debug] [Thread-1 (]: On model.finance.dim_info: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_info"} */

  
    
      create or replace table finance.dim_info
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, COGS, Inventory, Segment from data.table_finance;
  
[0m14:56:24.799864 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:56:32.013615 [debug] [Thread-1 (]: SQL status: OK in 7.21 seconds
[0m14:56:32.021882 [debug] [Thread-1 (]: finished collecting timing info
[0m14:56:32.021882 [debug] [Thread-1 (]: On model.finance.dim_info: ROLLBACK
[0m14:56:32.021882 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:56:32.026886 [debug] [Thread-1 (]: On model.finance.dim_info: Close
[0m14:56:34.256711 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '46ebae10-7edb-47a2-8bac-e009f0d030ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002229611A0B0>]}
[0m14:56:34.256711 [info ] [Thread-1 (]: 1 of 1 OK created sql table model finance.dim_info ............................. [[32mOK[0m in 9.56s]
[0m14:56:34.256711 [debug] [Thread-1 (]: Finished running node model.finance.dim_info
[0m14:56:34.256711 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:56:34.256711 [debug] [MainThread]: On master: ROLLBACK
[0m14:56:34.266071 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:56:35.467307 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:56:35.467307 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:56:35.467307 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:56:35.467307 [debug] [MainThread]: On master: ROLLBACK
[0m14:56:35.467307 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:56:35.471657 [debug] [MainThread]: On master: Close
[0m14:56:36.927063 [info ] [MainThread]: 
[0m14:56:36.930575 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 30.23 seconds (30.23s).
[0m14:56:36.930575 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:56:36.930575 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:56:36.930575 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:56:36.934960 [debug] [MainThread]: Connection 'model.finance.dim_info' was properly closed.
[0m14:56:36.946746 [info ] [MainThread]: 
[0m14:56:36.946746 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:56:36.946746 [info ] [MainThread]: 
[0m14:56:36.950915 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m14:56:36.956921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022295033280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022296058940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022296118D90>]}
[0m14:56:36.956921 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:56:47.596620 | 2f619dc2-0cc4-48e6-b15f-9112bbb7bf2f ==============================
[0m14:56:47.596620 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:56:47.596620 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'select': ['dim_inro'], 'which': 'test', 'rpc_method': 'test'}
[0m14:56:47.596620 [debug] [MainThread]: Tracking: tracking
[0m14:56:47.617807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C6109C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C6109900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C610B340>]}
[0m14:56:47.701967 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:56:47.701967 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:56:47.706999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2f619dc2-0cc4-48e6-b15f-9112bbb7bf2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C72F82B0>]}
[0m14:56:47.721661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2f619dc2-0cc4-48e6-b15f-9112bbb7bf2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C72C8880>]}
[0m14:56:47.721661 [info ] [MainThread]: Found 8 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:56:47.723414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f619dc2-0cc4-48e6-b15f-9112bbb7bf2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C72C86A0>]}
[0m14:56:47.723414 [warn ] [MainThread]: The selection criterion 'dim_inro' does not match any nodes
[0m14:56:47.726706 [info ] [MainThread]: 
[0m14:56:47.737038 [warn ] [MainThread]: [[33mWARNING[0m]: Nothing to do. Try checking your model configs and model specification args
[0m14:56:47.748769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C62A31F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C72C8340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C72C8580>]}
[0m14:56:47.753304 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:56:56.907364 | 03197fe5-b337-445d-b763-f34280d5ca5b ==============================
[0m14:56:56.907364 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:56:56.907364 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'select': ['dim_info'], 'which': 'test', 'rpc_method': 'test'}
[0m14:56:56.907364 [debug] [MainThread]: Tracking: tracking
[0m14:56:56.923805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002861FC49C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002861FC49870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002861FC49AB0>]}
[0m14:56:57.000672 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:56:57.000672 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:56:57.006775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '03197fe5-b337-445d-b763-f34280d5ca5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028620E382E0>]}
[0m14:56:57.016675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '03197fe5-b337-445d-b763-f34280d5ca5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028620E088B0>]}
[0m14:56:57.016675 [info ] [MainThread]: Found 8 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:56:57.016675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '03197fe5-b337-445d-b763-f34280d5ca5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028620E086D0>]}
[0m14:56:57.022635 [info ] [MainThread]: 
[0m14:56:57.022635 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:56:57.026732 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:56:57.042198 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:56:57.042198 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:56:57.042198 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:56:57.042198 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:57:02.008388 [debug] [ThreadPool]: SQL status: OK in 4.97 seconds
[0m14:57:02.017083 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:57:02.017083 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:57:02.017083 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:57:04.036953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '03197fe5-b337-445d-b763-f34280d5ca5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002861FC49CC0>]}
[0m14:57:04.036953 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:57:04.036953 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:57:04.040719 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:57:04.040719 [info ] [MainThread]: 
[0m14:57:04.056854 [debug] [Thread-1 (]: Began running node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m14:57:04.056854 [info ] [Thread-1 (]: 1 of 5 START test accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business  [RUN]
[0m14:57:04.062857 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m14:57:04.062857 [debug] [Thread-1 (]: Began compiling node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m14:57:04.062857 [debug] [Thread-1 (]: Compiling test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m14:57:04.082050 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m14:57:04.082050 [debug] [Thread-1 (]: finished collecting timing info
[0m14:57:04.082050 [debug] [Thread-1 (]: Began executing node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m14:57:04.100572 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m14:57:04.104592 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:57:04.105102 [debug] [Thread-1 (]: Using databricks connection "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m14:57:04.105102 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        Segment as value_field,
        count(*) as n_records

    from finance.dim_info
    group by Segment

)

select *
from all_values
where value_field not in (
    'Government','Channel Partners','Midmarket','Enterprise','Small Business'
)



      
    ) dbt_internal_test
[0m14:57:04.105102 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:57:08.746964 [debug] [Thread-1 (]: SQL status: OK in 4.64 seconds
[0m14:57:08.761787 [debug] [Thread-1 (]: finished collecting timing info
[0m14:57:08.761787 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: ROLLBACK
[0m14:57:08.761787 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:57:08.761787 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: Close
[0m14:57:09.892976 [info ] [Thread-1 (]: 1 of 5 PASS accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business  [[32mPASS[0m in 5.83s]
[0m14:57:09.896613 [debug] [Thread-1 (]: Finished running node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m14:57:09.897517 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m14:57:09.898849 [info ] [Thread-1 (]: 2 of 5 START test not_null_dim_info_Actual ..................................... [RUN]
[0m14:57:09.898849 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m14:57:09.898849 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m14:57:09.898849 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m14:57:09.971546 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m14:57:09.971546 [debug] [Thread-1 (]: finished collecting timing info
[0m14:57:09.971546 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m14:57:09.971546 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m14:57:09.971546 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:57:09.977118 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m14:57:09.977868 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Actual.5cb2ffbb95"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_info
where Actual is null



      
    ) dbt_internal_test
[0m14:57:09.977868 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:57:12.470566 [debug] [Thread-1 (]: SQL status: OK in 2.49 seconds
[0m14:57:12.478331 [debug] [Thread-1 (]: finished collecting timing info
[0m14:57:12.478331 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: ROLLBACK
[0m14:57:12.478331 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:57:12.478331 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: Close
[0m14:57:14.706938 [info ] [Thread-1 (]: 2 of 5 PASS not_null_dim_info_Actual ........................................... [[32mPASS[0m in 4.81s]
[0m14:57:14.706938 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m14:57:14.706938 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_COGS.efb9b66052
[0m14:57:14.706938 [info ] [Thread-1 (]: 3 of 5 START test not_null_dim_info_COGS ....................................... [RUN]
[0m14:57:14.716919 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m14:57:14.716919 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_COGS.efb9b66052
[0m14:57:14.716919 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_COGS.efb9b66052
[0m14:57:14.731315 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m14:57:14.731584 [debug] [Thread-1 (]: finished collecting timing info
[0m14:57:14.731584 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_COGS.efb9b66052
[0m14:57:14.736930 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m14:57:14.736930 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:57:14.743719 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m14:57:14.744528 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_COGS.efb9b66052"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select COGS
from finance.dim_info
where COGS is null



      
    ) dbt_internal_test
[0m14:57:14.744528 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:57:18.107282 [debug] [Thread-1 (]: SQL status: OK in 3.36 seconds
[0m14:57:18.112953 [debug] [Thread-1 (]: finished collecting timing info
[0m14:57:18.112953 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: ROLLBACK
[0m14:57:18.112953 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:57:18.117358 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: Close
[0m14:57:19.098506 [info ] [Thread-1 (]: 3 of 5 PASS not_null_dim_info_COGS ............................................. [[32mPASS[0m in 4.39s]
[0m14:57:19.098506 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_COGS.efb9b66052
[0m14:57:19.098506 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Inventory.45016db171
[0m14:57:19.103748 [info ] [Thread-1 (]: 4 of 5 START test not_null_dim_info_Inventory .................................. [RUN]
[0m14:57:19.105883 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Inventory.45016db171"
[0m14:57:19.105883 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Inventory.45016db171
[0m14:57:19.107218 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Inventory.45016db171
[0m14:57:19.115928 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Inventory.45016db171"
[0m14:57:19.117069 [debug] [Thread-1 (]: finished collecting timing info
[0m14:57:19.117644 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Inventory.45016db171
[0m14:57:19.118695 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Inventory.45016db171"
[0m14:57:19.118695 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:57:19.126493 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Inventory.45016db171"
[0m14:57:19.126493 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Inventory.45016db171"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Inventory
from finance.dim_info
where Inventory is null



      
    ) dbt_internal_test
[0m14:57:19.126919 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:57:23.197164 [debug] [Thread-1 (]: SQL status: OK in 4.07 seconds
[0m14:57:23.202733 [debug] [Thread-1 (]: finished collecting timing info
[0m14:57:23.202733 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: ROLLBACK
[0m14:57:23.202733 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:57:23.206740 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: Close
[0m14:57:24.837125 [info ] [Thread-1 (]: 4 of 5 PASS not_null_dim_info_Inventory ........................................ [[32mPASS[0m in 5.73s]
[0m14:57:24.847109 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Inventory.45016db171
[0m14:57:24.847298 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m14:57:24.847298 [info ] [Thread-1 (]: 5 of 5 START test not_null_dim_info_Segment .................................... [RUN]
[0m14:57:24.847298 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m14:57:24.847298 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m14:57:24.847298 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m14:57:24.857125 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m14:57:24.857125 [debug] [Thread-1 (]: finished collecting timing info
[0m14:57:24.857125 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m14:57:24.857125 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m14:57:24.857125 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:57:24.857125 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m14:57:24.857125 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Segment.57cc6a2d98"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Segment
from finance.dim_info
where Segment is null



      
    ) dbt_internal_test
[0m14:57:24.857125 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:57:28.326963 [debug] [Thread-1 (]: SQL status: OK in 3.47 seconds
[0m14:57:28.336983 [debug] [Thread-1 (]: finished collecting timing info
[0m14:57:28.336983 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: ROLLBACK
[0m14:57:28.339247 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:57:28.339741 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: Close
[0m14:57:29.546955 [info ] [Thread-1 (]: 5 of 5 PASS not_null_dim_info_Segment .......................................... [[32mPASS[0m in 4.70s]
[0m14:57:29.546955 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m14:57:29.556850 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:57:29.556850 [debug] [MainThread]: On master: ROLLBACK
[0m14:57:29.560275 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:57:30.767070 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:57:30.767070 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:57:30.776580 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:57:30.777294 [debug] [MainThread]: On master: ROLLBACK
[0m14:57:30.777842 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:57:30.777842 [debug] [MainThread]: On master: Close
[0m14:57:32.009332 [info ] [MainThread]: 
[0m14:57:32.012561 [info ] [MainThread]: Finished running 5 tests in 0 hours 0 minutes and 34.99 seconds (34.99s).
[0m14:57:32.012561 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:57:32.016565 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m14:57:32.016565 [debug] [MainThread]: Connection 'test.finance.not_null_dim_info_Segment.57cc6a2d98' was properly closed.
[0m14:57:32.036694 [info ] [MainThread]: 
[0m14:57:32.036694 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:57:32.041001 [info ] [MainThread]: 
[0m14:57:32.057016 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m14:57:32.057016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028620F765F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028620F773D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028620F77250>]}
[0m14:57:32.064426 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 14:59:30.046247 | 40b5245c-dbc6-46e6-a85d-e67a60a58c27 ==============================
[0m14:59:30.046247 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:59:30.046247 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'which': 'test', 'rpc_method': 'test'}
[0m14:59:30.046247 [debug] [MainThread]: Tracking: tracking
[0m14:59:30.066563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCB4669C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCB4669870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCB4669AB0>]}
[0m14:59:30.140549 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:59:30.140549 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:59:30.146637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '40b5245c-dbc6-46e6-a85d-e67a60a58c27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCB585C310>]}
[0m14:59:30.156401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '40b5245c-dbc6-46e6-a85d-e67a60a58c27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCB58348E0>]}
[0m14:59:30.156401 [info ] [MainThread]: Found 8 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:59:30.156401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '40b5245c-dbc6-46e6-a85d-e67a60a58c27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCB5834520>]}
[0m14:59:30.162419 [info ] [MainThread]: 
[0m14:59:30.166477 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:59:30.176219 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m14:59:30.187015 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:59:30.187015 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m14:59:30.187015 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m14:59:30.187015 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:59:33.866826 [debug] [ThreadPool]: SQL status: OK in 3.68 seconds
[0m14:59:33.878947 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m14:59:33.878947 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:59:33.878947 [debug] [ThreadPool]: On list_None_finance: Close
[0m14:59:35.147752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '40b5245c-dbc6-46e6-a85d-e67a60a58c27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCB58344C0>]}
[0m14:59:35.147752 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:59:35.147752 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:59:35.147752 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:59:35.147752 [info ] [MainThread]: 
[0m14:59:35.166720 [debug] [Thread-1 (]: Began running node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m14:59:35.166720 [info ] [Thread-1 (]: 1 of 25 START test accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business  [RUN]
[0m14:59:35.166720 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m14:59:35.176334 [debug] [Thread-1 (]: Began compiling node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m14:59:35.176988 [debug] [Thread-1 (]: Compiling test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m14:59:35.186682 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m14:59:35.186682 [debug] [Thread-1 (]: finished collecting timing info
[0m14:59:35.194511 [debug] [Thread-1 (]: Began executing node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m14:59:35.206368 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m14:59:35.206368 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:59:35.206368 [debug] [Thread-1 (]: Using databricks connection "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m14:59:35.214706 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        Segment as value_field,
        count(*) as n_records

    from finance.dim_info
    group by Segment

)

select *
from all_values
where value_field not in (
    'Government','Channel Partners','Midmarket','Enterprise','Small Business'
)



      
    ) dbt_internal_test
[0m14:59:35.215202 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:59:38.066671 [debug] [Thread-1 (]: SQL status: OK in 2.85 seconds
[0m14:59:38.071153 [debug] [Thread-1 (]: finished collecting timing info
[0m14:59:38.071153 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: ROLLBACK
[0m14:59:38.076759 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:59:38.076759 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: Close
[0m14:59:39.606283 [info ] [Thread-1 (]: 1 of 25 PASS accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business  [[32mPASS[0m in 4.44s]
[0m14:59:39.606283 [debug] [Thread-1 (]: Finished running node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m14:59:39.606283 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m14:59:39.606283 [info ] [Thread-1 (]: 2 of 25 START test not_null_dim_exp_Expenses ................................... [RUN]
[0m14:59:39.606283 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m14:59:39.606283 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m14:59:39.606283 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m14:59:39.668532 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m14:59:39.668532 [debug] [Thread-1 (]: finished collecting timing info
[0m14:59:39.674718 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m14:59:39.676378 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m14:59:39.681484 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:59:39.683157 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m14:59:39.683157 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Expenses.c94546fc2a: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Expenses.c94546fc2a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Expenses
from finance.dim_exp
where Expenses is null



      
    ) dbt_internal_test
[0m14:59:39.683157 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:59:43.399437 [debug] [Thread-1 (]: SQL status: OK in 3.72 seconds
[0m14:59:43.406103 [debug] [Thread-1 (]: finished collecting timing info
[0m14:59:43.406780 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Expenses.c94546fc2a: ROLLBACK
[0m14:59:43.406780 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:59:43.407442 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Expenses.c94546fc2a: Close
[0m14:59:45.252999 [info ] [Thread-1 (]: 2 of 25 PASS not_null_dim_exp_Expenses ......................................... [[32mPASS[0m in 5.65s]
[0m14:59:45.256511 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m14:59:45.256511 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m14:59:45.258846 [info ] [Thread-1 (]: 3 of 25 START test not_null_dim_exp_Gross_sales ................................ [RUN]
[0m14:59:45.259408 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m14:59:45.259408 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m14:59:45.259408 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m14:59:45.266465 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m14:59:45.266465 [debug] [Thread-1 (]: finished collecting timing info
[0m14:59:45.269030 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m14:59:45.269528 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m14:59:45.269528 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:59:45.269528 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m14:59:45.269528 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Gross_sales.07439d5e88: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Gross_sales
from finance.dim_exp
where Gross_sales is null



      
    ) dbt_internal_test
[0m14:59:45.269528 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:59:49.836477 [debug] [Thread-1 (]: SQL status: OK in 4.57 seconds
[0m14:59:49.846479 [debug] [Thread-1 (]: finished collecting timing info
[0m14:59:49.846479 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Gross_sales.07439d5e88: ROLLBACK
[0m14:59:49.846479 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:59:49.846479 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Gross_sales.07439d5e88: Close
[0m14:59:51.174819 [info ] [Thread-1 (]: 3 of 25 PASS not_null_dim_exp_Gross_sales ...................................... [[32mPASS[0m in 5.92s]
[0m14:59:51.176277 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m14:59:51.176277 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m14:59:51.176277 [info ] [Thread-1 (]: 4 of 25 START test not_null_dim_exp_Manufacturing_price ........................ [RUN]
[0m14:59:51.176277 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m14:59:51.176277 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m14:59:51.176277 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m14:59:51.186442 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m14:59:51.186442 [debug] [Thread-1 (]: finished collecting timing info
[0m14:59:51.186442 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m14:59:51.190075 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m14:59:51.190075 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:59:51.190075 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m14:59:51.190075 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Manufacturing_price
from finance.dim_exp
where Manufacturing_price is null



      
    ) dbt_internal_test
[0m14:59:51.190075 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:59:54.143159 [debug] [Thread-1 (]: SQL status: OK in 2.95 seconds
[0m14:59:54.146217 [debug] [Thread-1 (]: finished collecting timing info
[0m14:59:54.146217 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f: ROLLBACK
[0m14:59:54.146217 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:59:54.147405 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f: Close
[0m14:59:55.373722 [info ] [Thread-1 (]: 4 of 25 PASS not_null_dim_exp_Manufacturing_price .............................. [[32mPASS[0m in 4.20s]
[0m14:59:55.376649 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m14:59:55.376649 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m14:59:55.376649 [info ] [Thread-1 (]: 5 of 25 START test not_null_dim_info_Actual .................................... [RUN]
[0m14:59:55.381394 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m14:59:55.381394 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m14:59:55.383493 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m14:59:55.389616 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m14:59:55.391161 [debug] [Thread-1 (]: finished collecting timing info
[0m14:59:55.391161 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m14:59:55.391161 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m14:59:55.391161 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:59:55.391161 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m14:59:55.395722 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Actual.5cb2ffbb95"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_info
where Actual is null



      
    ) dbt_internal_test
[0m14:59:55.396319 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:59:58.337162 [debug] [Thread-1 (]: SQL status: OK in 2.94 seconds
[0m14:59:58.337162 [debug] [Thread-1 (]: finished collecting timing info
[0m14:59:58.346500 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: ROLLBACK
[0m14:59:58.346973 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:59:58.347464 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: Close
[0m15:00:00.081397 [info ] [Thread-1 (]: 5 of 25 PASS not_null_dim_info_Actual .......................................... [[32mPASS[0m in 4.70s]
[0m15:00:00.090966 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m15:00:00.090966 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_COGS.efb9b66052
[0m15:00:00.090966 [info ] [Thread-1 (]: 6 of 25 START test not_null_dim_info_COGS ...................................... [RUN]
[0m15:00:00.096973 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m15:00:00.100024 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_COGS.efb9b66052
[0m15:00:00.100024 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_COGS.efb9b66052
[0m15:00:00.111400 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m15:00:00.111400 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:00.111400 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_COGS.efb9b66052
[0m15:00:00.116632 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m15:00:00.116632 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:00.116632 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m15:00:00.116632 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_COGS.efb9b66052"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select COGS
from finance.dim_info
where COGS is null



      
    ) dbt_internal_test
[0m15:00:00.116632 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:00:04.356660 [debug] [Thread-1 (]: SQL status: OK in 4.24 seconds
[0m15:00:04.356660 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:04.356660 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: ROLLBACK
[0m15:00:04.356660 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:00:04.366404 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: Close
[0m15:00:06.226499 [info ] [Thread-1 (]: 6 of 25 PASS not_null_dim_info_COGS ............................................ [[32mPASS[0m in 6.13s]
[0m15:00:06.226499 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_COGS.efb9b66052
[0m15:00:06.226499 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Inventory.45016db171
[0m15:00:06.236729 [info ] [Thread-1 (]: 7 of 25 START test not_null_dim_info_Inventory ................................. [RUN]
[0m15:00:06.236729 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Inventory.45016db171"
[0m15:00:06.236729 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Inventory.45016db171
[0m15:00:06.236729 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Inventory.45016db171
[0m15:00:06.236729 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Inventory.45016db171"
[0m15:00:06.246449 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:06.246449 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Inventory.45016db171
[0m15:00:06.247483 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Inventory.45016db171"
[0m15:00:06.247483 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:06.247483 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Inventory.45016db171"
[0m15:00:06.247483 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Inventory.45016db171"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Inventory
from finance.dim_info
where Inventory is null



      
    ) dbt_internal_test
[0m15:00:06.247483 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:00:10.323745 [debug] [Thread-1 (]: SQL status: OK in 4.08 seconds
[0m15:00:10.326520 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:10.326520 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: ROLLBACK
[0m15:00:10.326520 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:00:10.331534 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: Close
[0m15:00:12.176545 [info ] [Thread-1 (]: 7 of 25 PASS not_null_dim_info_Inventory ....................................... [[32mPASS[0m in 5.94s]
[0m15:00:12.176545 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Inventory.45016db171
[0m15:00:12.186263 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m15:00:12.187930 [info ] [Thread-1 (]: 8 of 25 START test not_null_dim_info_Segment ................................... [RUN]
[0m15:00:12.187930 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m15:00:12.187930 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m15:00:12.187930 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m15:00:12.201358 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m15:00:12.201358 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:12.206414 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m15:00:12.212766 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m15:00:12.212766 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:12.212766 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m15:00:12.212766 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Segment.57cc6a2d98"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Segment
from finance.dim_info
where Segment is null



      
    ) dbt_internal_test
[0m15:00:12.216751 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:00:18.256563 [debug] [Thread-1 (]: SQL status: OK in 6.04 seconds
[0m15:00:18.256563 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:18.256563 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: ROLLBACK
[0m15:00:18.266213 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:00:18.266213 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: Close
[0m15:00:22.306750 [info ] [Thread-1 (]: 8 of 25 PASS not_null_dim_info_Segment ......................................... [[32mPASS[0m in 10.12s]
[0m15:00:22.306750 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m15:00:22.316473 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Date.ce09cea79f
[0m15:00:22.317755 [info ] [Thread-1 (]: 9 of 25 START test not_null_dim_month_Date ..................................... [RUN]
[0m15:00:22.317755 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m15:00:22.321657 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Date.ce09cea79f
[0m15:00:22.322149 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Date.ce09cea79f
[0m15:00:22.332131 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m15:00:22.332131 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:22.336659 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Date.ce09cea79f
[0m15:00:22.338597 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m15:00:22.338597 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:22.346328 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m15:00:22.346328 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Date.ce09cea79f: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Date.ce09cea79f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Date
from finance.dim_month
where Date is null



      
    ) dbt_internal_test
[0m15:00:22.346328 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:00:25.886607 [debug] [Thread-1 (]: SQL status: OK in 3.54 seconds
[0m15:00:25.886607 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:25.886607 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Date.ce09cea79f: ROLLBACK
[0m15:00:25.893251 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:00:25.893251 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Date.ce09cea79f: Close
[0m15:00:27.456485 [info ] [Thread-1 (]: 9 of 25 PASS not_null_dim_month_Date ........................................... [[32mPASS[0m in 5.14s]
[0m15:00:27.456485 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Date.ce09cea79f
[0m15:00:27.456485 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m15:00:27.462967 [info ] [Thread-1 (]: 10 of 25 START test not_null_dim_month_MonthName ............................... [RUN]
[0m15:00:27.462967 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m15:00:27.462967 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m15:00:27.462967 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m15:00:27.466675 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m15:00:27.471874 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:27.472371 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m15:00:27.473621 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m15:00:27.473621 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:27.476598 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m15:00:27.476598 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_MonthName.c1d116d50b: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_MonthName.c1d116d50b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select MonthName
from finance.dim_month
where MonthName is null



      
    ) dbt_internal_test
[0m15:00:27.477904 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:00:30.386529 [debug] [Thread-1 (]: SQL status: OK in 2.91 seconds
[0m15:00:30.396332 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:30.396332 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_MonthName.c1d116d50b: ROLLBACK
[0m15:00:30.396332 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:00:30.396332 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_MonthName.c1d116d50b: Close
[0m15:00:31.911088 [info ] [Thread-1 (]: 10 of 25 PASS not_null_dim_month_MonthName ..................................... [[32mPASS[0m in 4.45s]
[0m15:00:31.916321 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m15:00:31.916321 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m15:00:31.916321 [info ] [Thread-1 (]: 11 of 25 START test not_null_dim_month_Month_Number ............................ [RUN]
[0m15:00:31.919162 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m15:00:31.919162 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m15:00:31.919162 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m15:00:31.926182 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m15:00:31.926182 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:31.926182 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m15:00:31.926182 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m15:00:31.926182 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:31.926182 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m15:00:31.926182 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Month_Number.be337b93c3: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Month_Number.be337b93c3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Month_Number
from finance.dim_month
where Month_Number is null



      
    ) dbt_internal_test
[0m15:00:31.926182 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:00:35.706541 [debug] [Thread-1 (]: SQL status: OK in 3.78 seconds
[0m15:00:35.716350 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:35.716350 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Month_Number.be337b93c3: ROLLBACK
[0m15:00:35.719204 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:00:35.719204 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Month_Number.be337b93c3: Close
[0m15:00:36.970343 [info ] [Thread-1 (]: 11 of 25 PASS not_null_dim_month_Month_Number .................................. [[32mPASS[0m in 5.05s]
[0m15:00:36.970343 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m15:00:36.976462 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m15:00:36.976462 [info ] [Thread-1 (]: 12 of 25 START test not_null_dim_month_Year .................................... [RUN]
[0m15:00:36.980461 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m15:00:36.980461 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m15:00:36.980461 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Year.5d11bba8bb
[0m15:00:36.986717 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m15:00:36.996580 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:36.998288 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m15:00:36.998782 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m15:00:37.006909 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:37.007379 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m15:00:37.007379 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Year.5d11bba8bb: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Year.5d11bba8bb"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Year
from finance.dim_month
where Year is null



      
    ) dbt_internal_test
[0m15:00:37.007379 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:00:39.613005 [debug] [Thread-1 (]: SQL status: OK in 2.61 seconds
[0m15:00:39.618916 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:39.618916 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Year.5d11bba8bb: ROLLBACK
[0m15:00:39.623136 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:00:39.623136 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Year.5d11bba8bb: Close
[0m15:00:40.836819 [info ] [Thread-1 (]: 12 of 25 PASS not_null_dim_month_Year .......................................... [[32mPASS[0m in 3.86s]
[0m15:00:40.836819 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m15:00:40.845726 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m15:00:40.846234 [info ] [Thread-1 (]: 13 of 25 START test not_null_dim_profit_Country ................................ [RUN]
[0m15:00:40.846234 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m15:00:40.846234 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m15:00:40.851484 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Country.363de9fbd5
[0m15:00:40.862042 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m15:00:40.862042 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:40.866344 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m15:00:40.872326 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m15:00:40.872326 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:40.872326 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m15:00:40.876864 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Country.363de9fbd5: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Country.363de9fbd5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Country
from finance.dim_profit
where Country is null



      
    ) dbt_internal_test
[0m15:00:40.877358 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:00:43.806915 [debug] [Thread-1 (]: SQL status: OK in 2.93 seconds
[0m15:00:43.806915 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:43.806915 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Country.363de9fbd5: ROLLBACK
[0m15:00:43.806915 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:00:43.806915 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Country.363de9fbd5: Close
[0m15:00:45.036605 [info ] [Thread-1 (]: 13 of 25 PASS not_null_dim_profit_Country ...................................... [[32mPASS[0m in 4.19s]
[0m15:00:45.047227 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m15:00:45.047227 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m15:00:45.047227 [info ] [Thread-1 (]: 14 of 25 START test not_null_dim_profit_Discount_Band .......................... [RUN]
[0m15:00:45.047227 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m15:00:45.047227 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m15:00:45.047227 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m15:00:45.056377 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m15:00:45.056377 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:45.056377 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m15:00:45.063114 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m15:00:45.066235 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:45.066235 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m15:00:45.067496 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Discount_Band
from finance.dim_profit
where Discount_Band is null



      
    ) dbt_internal_test
[0m15:00:45.067973 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:00:47.696461 [debug] [Thread-1 (]: SQL status: OK in 2.63 seconds
[0m15:00:47.706663 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:47.706663 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa: ROLLBACK
[0m15:00:47.706663 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:00:47.706663 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa: Close
[0m15:00:48.836300 [info ] [Thread-1 (]: 14 of 25 PASS not_null_dim_profit_Discount_Band ................................ [[32mPASS[0m in 3.79s]
[0m15:00:48.836300 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m15:00:48.836300 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m15:00:48.836300 [info ] [Thread-1 (]: 15 of 25 START test not_null_dim_profit_Discounts .............................. [RUN]
[0m15:00:48.836300 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m15:00:48.846471 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m15:00:48.846654 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Discounts.03e349480e
[0m15:00:48.856523 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m15:00:48.856523 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:48.856523 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m15:00:48.856523 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m15:00:48.856523 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:48.856523 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m15:00:48.856523 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discounts.03e349480e: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Discounts.03e349480e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Discounts
from finance.dim_profit
where Discounts is null



      
    ) dbt_internal_test
[0m15:00:48.856523 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:00:53.336206 [debug] [Thread-1 (]: SQL status: OK in 4.48 seconds
[0m15:00:53.336206 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:53.336206 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discounts.03e349480e: ROLLBACK
[0m15:00:53.343053 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:00:53.343053 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discounts.03e349480e: Close
[0m15:00:54.416617 [info ] [Thread-1 (]: 15 of 25 PASS not_null_dim_profit_Discounts .................................... [[32mPASS[0m in 5.58s]
[0m15:00:54.424672 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m15:00:54.426195 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Product.1422e74993
[0m15:00:54.426195 [info ] [Thread-1 (]: 16 of 25 START test not_null_dim_profit_Product ................................ [RUN]
[0m15:00:54.427933 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Product.1422e74993"
[0m15:00:54.430030 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Product.1422e74993
[0m15:00:54.430538 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Product.1422e74993
[0m15:00:54.438945 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Product.1422e74993"
[0m15:00:54.443098 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:54.443734 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Product.1422e74993
[0m15:00:54.446362 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Product.1422e74993"
[0m15:00:54.446362 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:54.446362 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Product.1422e74993"
[0m15:00:54.446362 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Product.1422e74993: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Product.1422e74993"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Product
from finance.dim_profit
where Product is null



      
    ) dbt_internal_test
[0m15:00:54.446362 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:00:57.096475 [debug] [Thread-1 (]: SQL status: OK in 2.65 seconds
[0m15:00:57.096475 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:57.096475 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Product.1422e74993: ROLLBACK
[0m15:00:57.102418 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:00:57.102418 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Product.1422e74993: Close
[0m15:00:58.336236 [info ] [Thread-1 (]: 16 of 25 PASS not_null_dim_profit_Product ...................................... [[32mPASS[0m in 3.91s]
[0m15:00:58.336236 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Product.1422e74993
[0m15:00:58.336236 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m15:00:58.336236 [info ] [Thread-1 (]: 17 of 25 START test not_null_dim_profit_Profit ................................. [RUN]
[0m15:00:58.336236 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m15:00:58.336236 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m15:00:58.344352 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m15:00:58.346391 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m15:00:58.346391 [debug] [Thread-1 (]: finished collecting timing info
[0m15:00:58.346391 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m15:00:58.356149 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m15:00:58.356270 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:58.356270 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m15:00:58.356270 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Profit.3c5f35db12: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Profit.3c5f35db12"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Profit
from finance.dim_profit
where Profit is null



      
    ) dbt_internal_test
[0m15:00:58.356270 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:01:01.106257 [debug] [Thread-1 (]: SQL status: OK in 2.75 seconds
[0m15:01:01.116688 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:01.116688 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Profit.3c5f35db12: ROLLBACK
[0m15:01:01.122624 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:01:01.122624 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Profit.3c5f35db12: Close
[0m15:01:02.548209 [info ] [Thread-1 (]: 17 of 25 PASS not_null_dim_profit_Profit ....................................... [[32mPASS[0m in 4.21s]
[0m15:01:02.548209 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m15:01:02.548209 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m15:01:02.548209 [info ] [Thread-1 (]: 18 of 25 START test not_null_dim_profit_Sales .................................. [RUN]
[0m15:01:02.556333 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m15:01:02.556333 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m15:01:02.556333 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Sales.05c16c548a
[0m15:01:02.558237 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m15:01:02.558237 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:02.558237 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m15:01:02.566398 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m15:01:02.566398 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:01:02.566398 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m15:01:02.566398 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Sales.05c16c548a: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Sales.05c16c548a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Sales
from finance.dim_profit
where Sales is null



      
    ) dbt_internal_test
[0m15:01:02.566398 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:01:05.246572 [debug] [Thread-1 (]: SQL status: OK in 2.68 seconds
[0m15:01:05.256471 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:05.256471 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Sales.05c16c548a: ROLLBACK
[0m15:01:05.256471 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:01:05.258924 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Sales.05c16c548a: Close
[0m15:01:06.446201 [info ] [Thread-1 (]: 18 of 25 PASS not_null_dim_profit_Sales ........................................ [[32mPASS[0m in 3.90s]
[0m15:01:06.446201 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m15:01:06.450817 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m15:01:06.451119 [info ] [Thread-1 (]: 19 of 25 START test not_null_dim_unit_Target ................................... [RUN]
[0m15:01:06.452316 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m15:01:06.453403 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m15:01:06.453403 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Target.33e9c88d57
[0m15:01:06.456589 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m15:01:06.460031 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:06.460031 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m15:01:06.460827 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m15:01:06.460827 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:01:06.460827 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m15:01:06.460827 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Target.33e9c88d57: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Target.33e9c88d57"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Target
from finance.dim_unit
where Target is null



      
    ) dbt_internal_test
[0m15:01:06.464442 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:01:09.436621 [debug] [Thread-1 (]: SQL status: OK in 2.97 seconds
[0m15:01:09.436621 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:09.436621 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Target.33e9c88d57: ROLLBACK
[0m15:01:09.436621 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:01:09.436621 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Target.33e9c88d57: Close
[0m15:01:10.846273 [info ] [Thread-1 (]: 19 of 25 PASS not_null_dim_unit_Target ......................................... [[32mPASS[0m in 4.39s]
[0m15:01:10.846273 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m15:01:10.846273 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m15:01:10.846273 [info ] [Thread-1 (]: 20 of 25 START test not_null_dim_unit_Unit_Price ............................... [RUN]
[0m15:01:10.851738 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m15:01:10.851738 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m15:01:10.851738 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m15:01:10.861457 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m15:01:10.866464 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:10.866464 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m15:01:10.866464 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m15:01:10.873907 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:01:10.873907 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m15:01:10.873907 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Unit_Price.912d40d17b: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Unit_Price
from finance.dim_unit
where Unit_Price is null



      
    ) dbt_internal_test
[0m15:01:10.873907 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:01:14.726256 [debug] [Thread-1 (]: SQL status: OK in 3.85 seconds
[0m15:01:14.736397 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:14.736397 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Unit_Price.912d40d17b: ROLLBACK
[0m15:01:14.744108 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:01:14.744108 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Unit_Price.912d40d17b: Close
[0m15:01:21.396161 [info ] [Thread-1 (]: 20 of 25 PASS not_null_dim_unit_Unit_Price ..................................... [[32mPASS[0m in 10.54s]
[0m15:01:21.396161 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m15:01:21.396161 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m15:01:21.396161 [info ] [Thread-1 (]: 21 of 25 START test not_null_dim_unit_Units_Sold ............................... [RUN]
[0m15:01:21.402682 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m15:01:21.402682 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m15:01:21.402682 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m15:01:21.406184 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m15:01:21.406184 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:21.406184 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m15:01:21.406184 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m15:01:21.406184 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:01:21.406184 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m15:01:21.406184 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Units_Sold.fd5939c596: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Units_Sold
from finance.dim_unit
where Units_Sold is null



      
    ) dbt_internal_test
[0m15:01:21.415119 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:01:24.053059 [debug] [Thread-1 (]: SQL status: OK in 2.64 seconds
[0m15:01:24.056233 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:24.056233 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Units_Sold.fd5939c596: ROLLBACK
[0m15:01:24.056233 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:01:24.058446 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Units_Sold.fd5939c596: Close
[0m15:01:25.276410 [info ] [Thread-1 (]: 21 of 25 PASS not_null_dim_unit_Units_Sold ..................................... [[32mPASS[0m in 3.87s]
[0m15:01:25.276410 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m15:01:25.276410 [debug] [Thread-1 (]: Began running node test.finance.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:01:25.286014 [info ] [Thread-1 (]: 22 of 25 START test not_null_my_first_dbt_model_id ............................. [RUN]
[0m15:01:25.286014 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:01:25.286014 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:01:25.286014 [debug] [Thread-1 (]: Compiling test.finance.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:01:25.286014 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:01:25.295329 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:25.295824 [debug] [Thread-1 (]: Began executing node test.finance.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:01:25.296816 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:01:25.302495 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:01:25.302744 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:01:25.302744 [debug] [Thread-1 (]: On test.finance.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from finance.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m15:01:25.302744 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:01:33.436337 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from finance.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m15:01:33.436337 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: finance.my_first_dbt_model; line 14 pos 5
[0m15:01:33.436337 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: finance.my_first_dbt_model; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: finance.my_first_dbt_model; line 14 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m15:01:33.440276 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\x00w\xfd\xbd{\xe9N\xf9\x80hR-6z\xd2\xf9'
[0m15:01:33.440276 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:33.440276 [debug] [Thread-1 (]: On test.finance.not_null_my_first_dbt_model_id.5fb22c2710: ROLLBACK
[0m15:01:33.440276 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:01:33.442602 [debug] [Thread-1 (]: On test.finance.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m15:01:35.676207 [debug] [Thread-1 (]: Runtime Error in test not_null_my_first_dbt_model_id (models\example\schema.yml)
  Table or view not found: finance.my_first_dbt_model; line 14 pos 5
[0m15:01:35.676207 [error] [Thread-1 (]: 22 of 25 ERROR not_null_my_first_dbt_model_id .................................. [[31mERROR[0m in 10.39s]
[0m15:01:35.685989 [debug] [Thread-1 (]: Finished running node test.finance.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:01:35.685989 [debug] [Thread-1 (]: Began running node test.finance.not_null_my_second_dbt_model_id.151b76d778
[0m15:01:35.685989 [info ] [Thread-1 (]: 23 of 25 START test not_null_my_second_dbt_model_id ............................ [RUN]
[0m15:01:35.685989 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_my_second_dbt_model_id.151b76d778"
[0m15:01:35.693692 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_my_second_dbt_model_id.151b76d778
[0m15:01:35.694185 [debug] [Thread-1 (]: Compiling test.finance.not_null_my_second_dbt_model_id.151b76d778
[0m15:01:35.706262 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_my_second_dbt_model_id.151b76d778"
[0m15:01:35.709746 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:35.711092 [debug] [Thread-1 (]: Began executing node test.finance.not_null_my_second_dbt_model_id.151b76d778
[0m15:01:35.716094 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_my_second_dbt_model_id.151b76d778"
[0m15:01:35.716094 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:01:35.716094 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_my_second_dbt_model_id.151b76d778"
[0m15:01:35.716094 [debug] [Thread-1 (]: On test.finance.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from finance.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m15:01:35.722424 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:01:39.436178 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from finance.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m15:01:39.436178 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: finance.my_second_dbt_model; line 14 pos 5
[0m15:01:39.444104 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: finance.my_second_dbt_model; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: finance.my_second_dbt_model; line 14 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m15:01:39.446149 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'*\x80G\xc9\xb8 E\xb0\x98\xf4\xee\xe1/T\xb3\x01'
[0m15:01:39.446149 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:39.449038 [debug] [Thread-1 (]: On test.finance.not_null_my_second_dbt_model_id.151b76d778: ROLLBACK
[0m15:01:39.449038 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:01:39.449038 [debug] [Thread-1 (]: On test.finance.not_null_my_second_dbt_model_id.151b76d778: Close
[0m15:01:41.856222 [debug] [Thread-1 (]: Runtime Error in test not_null_my_second_dbt_model_id (models\example\schema.yml)
  Table or view not found: finance.my_second_dbt_model; line 14 pos 5
[0m15:01:41.856222 [error] [Thread-1 (]: 23 of 25 ERROR not_null_my_second_dbt_model_id ................................. [[31mERROR[0m in 6.17s]
[0m15:01:41.862576 [debug] [Thread-1 (]: Finished running node test.finance.not_null_my_second_dbt_model_id.151b76d778
[0m15:01:41.862576 [debug] [Thread-1 (]: Began running node test.finance.unique_my_first_dbt_model_id.16e066b321
[0m15:01:41.862576 [info ] [Thread-1 (]: 24 of 25 START test unique_my_first_dbt_model_id ............................... [RUN]
[0m15:01:41.866217 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.unique_my_first_dbt_model_id.16e066b321"
[0m15:01:41.866217 [debug] [Thread-1 (]: Began compiling node test.finance.unique_my_first_dbt_model_id.16e066b321
[0m15:01:41.866217 [debug] [Thread-1 (]: Compiling test.finance.unique_my_first_dbt_model_id.16e066b321
[0m15:01:41.874307 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.unique_my_first_dbt_model_id.16e066b321"
[0m15:01:41.878800 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:41.878800 [debug] [Thread-1 (]: Began executing node test.finance.unique_my_first_dbt_model_id.16e066b321
[0m15:01:41.878800 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.unique_my_first_dbt_model_id.16e066b321"
[0m15:01:41.883760 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:01:41.883760 [debug] [Thread-1 (]: Using databricks connection "test.finance.unique_my_first_dbt_model_id.16e066b321"
[0m15:01:41.884972 [debug] [Thread-1 (]: On test.finance.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from finance.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m15:01:41.885503 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:01:45.866215 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from finance.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m15:01:45.866273 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: finance.my_first_dbt_model; line 15 pos 5
[0m15:01:45.866273 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: finance.my_first_dbt_model; line 15 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: finance.my_first_dbt_model; line 15 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m15:01:45.868600 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'r\xa2\x01\xb9x\x0bD\xc0\x96)\xa4\xf9\x7f~\xfb\x82'
[0m15:01:45.870163 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:45.870163 [debug] [Thread-1 (]: On test.finance.unique_my_first_dbt_model_id.16e066b321: ROLLBACK
[0m15:01:45.870163 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:01:45.871655 [debug] [Thread-1 (]: On test.finance.unique_my_first_dbt_model_id.16e066b321: Close
[0m15:01:47.196519 [debug] [Thread-1 (]: Runtime Error in test unique_my_first_dbt_model_id (models\example\schema.yml)
  Table or view not found: finance.my_first_dbt_model; line 15 pos 5
[0m15:01:47.196519 [error] [Thread-1 (]: 24 of 25 ERROR unique_my_first_dbt_model_id .................................... [[31mERROR[0m in 5.33s]
[0m15:01:47.196519 [debug] [Thread-1 (]: Finished running node test.finance.unique_my_first_dbt_model_id.16e066b321
[0m15:01:47.196519 [debug] [Thread-1 (]: Began running node test.finance.unique_my_second_dbt_model_id.57a0f8c493
[0m15:01:47.196519 [info ] [Thread-1 (]: 25 of 25 START test unique_my_second_dbt_model_id .............................. [RUN]
[0m15:01:47.196519 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.unique_my_second_dbt_model_id.57a0f8c493"
[0m15:01:47.196519 [debug] [Thread-1 (]: Began compiling node test.finance.unique_my_second_dbt_model_id.57a0f8c493
[0m15:01:47.206062 [debug] [Thread-1 (]: Compiling test.finance.unique_my_second_dbt_model_id.57a0f8c493
[0m15:01:47.207479 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.unique_my_second_dbt_model_id.57a0f8c493"
[0m15:01:47.207479 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:47.207479 [debug] [Thread-1 (]: Began executing node test.finance.unique_my_second_dbt_model_id.57a0f8c493
[0m15:01:47.216408 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.unique_my_second_dbt_model_id.57a0f8c493"
[0m15:01:47.216408 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:01:47.216408 [debug] [Thread-1 (]: Using databricks connection "test.finance.unique_my_second_dbt_model_id.57a0f8c493"
[0m15:01:47.216408 [debug] [Thread-1 (]: On test.finance.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from finance.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m15:01:47.216408 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:01:49.806164 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from finance.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m15:01:49.806164 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: finance.my_second_dbt_model; line 15 pos 5
[0m15:01:49.806164 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: finance.my_second_dbt_model; line 15 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: finance.my_second_dbt_model; line 15 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m15:01:49.808814 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\x80s\xeb\x12`4Gw\x91\xee\xf0\x1f\x18b4)'
[0m15:01:49.808814 [debug] [Thread-1 (]: finished collecting timing info
[0m15:01:49.810132 [debug] [Thread-1 (]: On test.finance.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m15:01:49.810132 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:01:49.810913 [debug] [Thread-1 (]: On test.finance.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m15:01:50.976576 [debug] [Thread-1 (]: Runtime Error in test unique_my_second_dbt_model_id (models\example\schema.yml)
  Table or view not found: finance.my_second_dbt_model; line 15 pos 5
[0m15:01:50.976576 [error] [Thread-1 (]: 25 of 25 ERROR unique_my_second_dbt_model_id ................................... [[31mERROR[0m in 3.78s]
[0m15:01:50.986281 [debug] [Thread-1 (]: Finished running node test.finance.unique_my_second_dbt_model_id.57a0f8c493
[0m15:01:50.986281 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m15:01:50.986281 [debug] [MainThread]: On master: ROLLBACK
[0m15:01:50.992636 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:01:53.028313 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:01:53.028313 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:01:53.028313 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:01:53.028313 [debug] [MainThread]: On master: ROLLBACK
[0m15:01:53.028313 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:01:53.028313 [debug] [MainThread]: On master: Close
[0m15:01:54.475925 [info ] [MainThread]: 
[0m15:01:54.479766 [info ] [MainThread]: Finished running 25 tests in 0 hours 2 minutes and 24.31 seconds (144.31s).
[0m15:01:54.483212 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:01:54.484093 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m15:01:54.484621 [debug] [MainThread]: Connection 'test.finance.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m15:01:54.513085 [info ] [MainThread]: 
[0m15:01:54.514726 [info ] [MainThread]: [31mCompleted with 4 errors and 0 warnings:[0m
[0m15:01:54.516688 [info ] [MainThread]: 
[0m15:01:54.529809 [error] [MainThread]: [33mRuntime Error in test not_null_my_first_dbt_model_id (models\example\schema.yml)[0m
[0m15:01:54.536094 [error] [MainThread]:   Table or view not found: finance.my_first_dbt_model; line 14 pos 5
[0m15:01:54.556387 [info ] [MainThread]: 
[0m15:01:54.559688 [error] [MainThread]: [33mRuntime Error in test not_null_my_second_dbt_model_id (models\example\schema.yml)[0m
[0m15:01:54.585358 [error] [MainThread]:   Table or view not found: finance.my_second_dbt_model; line 14 pos 5
[0m15:01:54.602674 [info ] [MainThread]: 
[0m15:01:54.604778 [error] [MainThread]: [33mRuntime Error in test unique_my_first_dbt_model_id (models\example\schema.yml)[0m
[0m15:01:54.617662 [error] [MainThread]:   Table or view not found: finance.my_first_dbt_model; line 15 pos 5
[0m15:01:54.617662 [info ] [MainThread]: 
[0m15:01:54.636598 [error] [MainThread]: [33mRuntime Error in test unique_my_second_dbt_model_id (models\example\schema.yml)[0m
[0m15:01:54.636598 [error] [MainThread]:   Table or view not found: finance.my_second_dbt_model; line 15 pos 5
[0m15:01:54.636598 [info ] [MainThread]: 
[0m15:01:54.646342 [info ] [MainThread]: Done. PASS=21 WARN=0 ERROR=4 SKIP=0 TOTAL=25
[0m15:01:54.652163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCB5A4F580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCB59D0BE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCB59D1210>]}
[0m15:01:54.653458 [debug] [MainThread]: Flushing usage events


============================== 2022-11-27 15:59:09.575360 | 65bf7934-a577-4618-8957-db18c4f77b5a ==============================
[0m15:59:09.575360 [info ] [MainThread]: Running with dbt=1.3.1
[0m15:59:09.576806 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m15:59:09.576806 [debug] [MainThread]: Tracking: tracking
[0m15:59:09.587479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C1CA69C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C1CA69870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C1CA69AB0>]}
[0m15:59:09.657012 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:59:09.658008 [debug] [MainThread]: Partial parsing: update schema file: finance://models\schema.yml
[0m15:59:09.668255 [debug] [MainThread]: 1699: static parser successfully parsed dim_month.sql
[0m15:59:09.679812 [debug] [MainThread]: 1699: static parser successfully parsed dim_unit.sql
[0m15:59:09.683304 [debug] [MainThread]: 1699: static parser successfully parsed dim_profit.sql
[0m15:59:09.685222 [debug] [MainThread]: 1699: static parser successfully parsed dim_exp.sql
[0m15:59:09.687286 [debug] [MainThread]: 1699: static parser successfully parsed dim_info.sql
[0m15:59:09.801195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '65bf7934-a577-4618-8957-db18c4f77b5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C1CC467A0>]}
[0m15:59:09.811056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '65bf7934-a577-4618-8957-db18c4f77b5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C1CBD0070>]}
[0m15:59:09.811056 [info ] [MainThread]: Found 8 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m15:59:09.811988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '65bf7934-a577-4618-8957-db18c4f77b5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C0D403940>]}
[0m15:59:09.814056 [info ] [MainThread]: 
[0m15:59:09.815040 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m15:59:09.820947 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m15:59:09.829597 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:59:09.829597 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m15:59:09.829597 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m15:59:09.829597 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:59:18.682457 [debug] [ThreadPool]: SQL status: OK in 8.85 seconds
[0m15:59:18.693675 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m15:59:18.693675 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:59:18.694599 [debug] [ThreadPool]: On list_None_finance: Close
[0m15:59:20.087885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '65bf7934-a577-4618-8957-db18c4f77b5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C1CC47BE0>]}
[0m15:59:20.088893 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:59:20.090102 [info ] [MainThread]: 
[0m15:59:20.103079 [debug] [Thread-1 (]: Began running node model.finance.dim_exp
[0m15:59:20.103079 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_exp"
[0m15:59:20.104077 [debug] [Thread-1 (]: Began compiling node model.finance.dim_exp
[0m15:59:20.104077 [debug] [Thread-1 (]: Compiling model.finance.dim_exp
[0m15:59:20.107256 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_exp"
[0m15:59:20.107256 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.108171 [debug] [Thread-1 (]: Began executing node model.finance.dim_exp
[0m15:59:20.108171 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.109169 [debug] [Thread-1 (]: Finished running node model.finance.dim_exp
[0m15:59:20.109169 [debug] [Thread-1 (]: Began running node model.finance.dim_info
[0m15:59:20.110164 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_info"
[0m15:59:20.111160 [debug] [Thread-1 (]: Began compiling node model.finance.dim_info
[0m15:59:20.111160 [debug] [Thread-1 (]: Compiling model.finance.dim_info
[0m15:59:20.113505 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_info"
[0m15:59:20.114825 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.114825 [debug] [Thread-1 (]: Began executing node model.finance.dim_info
[0m15:59:20.114825 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.115843 [debug] [Thread-1 (]: Finished running node model.finance.dim_info
[0m15:59:20.115843 [debug] [Thread-1 (]: Began running node model.finance.dim_month
[0m15:59:20.116845 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_month"
[0m15:59:20.116845 [debug] [Thread-1 (]: Began compiling node model.finance.dim_month
[0m15:59:20.117858 [debug] [Thread-1 (]: Compiling model.finance.dim_month
[0m15:59:20.121281 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_month"
[0m15:59:20.121281 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.122275 [debug] [Thread-1 (]: Began executing node model.finance.dim_month
[0m15:59:20.122275 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.122275 [debug] [Thread-1 (]: Finished running node model.finance.dim_month
[0m15:59:20.123272 [debug] [Thread-1 (]: Began running node model.finance.dim_profit
[0m15:59:20.123272 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_profit"
[0m15:59:20.124269 [debug] [Thread-1 (]: Began compiling node model.finance.dim_profit
[0m15:59:20.124269 [debug] [Thread-1 (]: Compiling model.finance.dim_profit
[0m15:59:20.126262 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_profit"
[0m15:59:20.127259 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.127259 [debug] [Thread-1 (]: Began executing node model.finance.dim_profit
[0m15:59:20.127259 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.128255 [debug] [Thread-1 (]: Finished running node model.finance.dim_profit
[0m15:59:20.128255 [debug] [Thread-1 (]: Began running node model.finance.dim_unit
[0m15:59:20.129435 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_unit"
[0m15:59:20.129866 [debug] [Thread-1 (]: Began compiling node model.finance.dim_unit
[0m15:59:20.129866 [debug] [Thread-1 (]: Compiling model.finance.dim_unit
[0m15:59:20.132330 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_unit"
[0m15:59:20.133333 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.133333 [debug] [Thread-1 (]: Began executing node model.finance.dim_unit
[0m15:59:20.133333 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.134241 [debug] [Thread-1 (]: Finished running node model.finance.dim_unit
[0m15:59:20.134241 [debug] [Thread-1 (]: Began running node model.finance.my_first_dbt_model
[0m15:59:20.135235 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.my_first_dbt_model"
[0m15:59:20.135235 [debug] [Thread-1 (]: Began compiling node model.finance.my_first_dbt_model
[0m15:59:20.135235 [debug] [Thread-1 (]: Compiling model.finance.my_first_dbt_model
[0m15:59:20.138312 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.my_first_dbt_model"
[0m15:59:20.139271 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.139271 [debug] [Thread-1 (]: Began executing node model.finance.my_first_dbt_model
[0m15:59:20.139271 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.140220 [debug] [Thread-1 (]: Finished running node model.finance.my_first_dbt_model
[0m15:59:20.140220 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m15:59:20.140220 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m15:59:20.141229 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m15:59:20.141229 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m15:59:20.148455 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m15:59:20.149468 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.149468 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m15:59:20.149468 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.150447 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m15:59:20.150447 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m15:59:20.151444 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m15:59:20.151444 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m15:59:20.151444 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m15:59:20.155427 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m15:59:20.156405 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.156405 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m15:59:20.156405 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.157331 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m15:59:20.157331 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m15:59:20.158092 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m15:59:20.158092 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m15:59:20.158092 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m15:59:20.161941 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m15:59:20.163093 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.163093 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m15:59:20.163093 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.164018 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m15:59:20.164018 [debug] [Thread-1 (]: Began running node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m15:59:20.164018 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m15:59:20.165089 [debug] [Thread-1 (]: Began compiling node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m15:59:20.165089 [debug] [Thread-1 (]: Compiling test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m15:59:20.170354 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m15:59:20.171280 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.171280 [debug] [Thread-1 (]: Began executing node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m15:59:20.171280 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.172279 [debug] [Thread-1 (]: Finished running node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m15:59:20.172279 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m15:59:20.173174 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m15:59:20.173174 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m15:59:20.173174 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m15:59:20.178458 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m15:59:20.178458 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.179389 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m15:59:20.179389 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.180381 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m15:59:20.180381 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_COGS.efb9b66052
[0m15:59:20.181378 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m15:59:20.181378 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_COGS.efb9b66052
[0m15:59:20.181378 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_COGS.efb9b66052
[0m15:59:20.184632 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m15:59:20.185663 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.185663 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_COGS.efb9b66052
[0m15:59:20.185663 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.186669 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_COGS.efb9b66052
[0m15:59:20.186669 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Inventory.45016db171
[0m15:59:20.187702 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Inventory.45016db171"
[0m15:59:20.187702 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Inventory.45016db171
[0m15:59:20.187702 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Inventory.45016db171
[0m15:59:20.190717 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Inventory.45016db171"
[0m15:59:20.193253 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.193431 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Inventory.45016db171
[0m15:59:20.193431 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.194186 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Inventory.45016db171
[0m15:59:20.194186 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m15:59:20.195183 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m15:59:20.195183 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m15:59:20.195183 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m15:59:20.199178 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m15:59:20.199178 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.200168 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m15:59:20.200168 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.200168 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m15:59:20.201180 [debug] [Thread-1 (]: Began running node model.finance.dim_check_month
[0m15:59:20.201284 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_check_month"
[0m15:59:20.201937 [debug] [Thread-1 (]: Began compiling node model.finance.dim_check_month
[0m15:59:20.201937 [debug] [Thread-1 (]: Compiling model.finance.dim_check_month
[0m15:59:20.203932 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_check_month"
[0m15:59:20.204929 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.204929 [debug] [Thread-1 (]: Began executing node model.finance.dim_check_month
[0m15:59:20.204929 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.205926 [debug] [Thread-1 (]: Finished running node model.finance.dim_check_month
[0m15:59:20.205926 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Date.ce09cea79f
[0m15:59:20.206923 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m15:59:20.207280 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Date.ce09cea79f
[0m15:59:20.207580 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Date.ce09cea79f
[0m15:59:20.210256 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m15:59:20.212178 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.212178 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Date.ce09cea79f
[0m15:59:20.212178 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.212178 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Date.ce09cea79f
[0m15:59:20.213175 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m15:59:20.213175 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m15:59:20.213175 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m15:59:20.214171 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m15:59:20.217161 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m15:59:20.217161 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.218158 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m15:59:20.218158 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.218158 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m15:59:20.219155 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m15:59:20.219155 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m15:59:20.219155 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m15:59:20.220151 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m15:59:20.224138 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m15:59:20.224138 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.224138 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m15:59:20.225256 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.225256 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m15:59:20.226351 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m15:59:20.226351 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m15:59:20.227275 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m15:59:20.227275 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Year.5d11bba8bb
[0m15:59:20.231262 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m15:59:20.231262 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.232355 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m15:59:20.232355 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.232355 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m15:59:20.232355 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m15:59:20.233470 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m15:59:20.234458 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m15:59:20.234458 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Country.363de9fbd5
[0m15:59:20.238278 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m15:59:20.239303 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.239303 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m15:59:20.239303 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.240299 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m15:59:20.240299 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m15:59:20.240299 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m15:59:20.241440 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m15:59:20.241440 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m15:59:20.244463 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m15:59:20.245461 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.245461 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m15:59:20.245461 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.246457 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m15:59:20.246457 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m15:59:20.246457 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m15:59:20.247453 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m15:59:20.247453 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Discounts.03e349480e
[0m15:59:20.250727 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m15:59:20.251724 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.251724 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m15:59:20.251724 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.252721 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m15:59:20.252721 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Product.1422e74993
[0m15:59:20.253717 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Product.1422e74993"
[0m15:59:20.253717 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Product.1422e74993
[0m15:59:20.253717 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Product.1422e74993
[0m15:59:20.257704 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Product.1422e74993"
[0m15:59:20.257704 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.258702 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Product.1422e74993
[0m15:59:20.258702 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.258702 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Product.1422e74993
[0m15:59:20.259752 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m15:59:20.259752 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m15:59:20.259752 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m15:59:20.260776 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m15:59:20.263684 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m15:59:20.264680 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.264680 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m15:59:20.264680 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.265887 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m15:59:20.265887 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m15:59:20.265887 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m15:59:20.266877 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m15:59:20.266877 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Sales.05c16c548a
[0m15:59:20.270864 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m15:59:20.270864 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.271861 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m15:59:20.271861 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.271861 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m15:59:20.272857 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m15:59:20.272857 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m15:59:20.272857 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m15:59:20.274004 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Target.33e9c88d57
[0m15:59:20.276586 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m15:59:20.277584 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.277584 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m15:59:20.277584 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.278580 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m15:59:20.278580 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m15:59:20.279585 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m15:59:20.279585 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m15:59:20.279585 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m15:59:20.283659 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m15:59:20.284655 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.284655 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m15:59:20.284655 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.285650 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m15:59:20.285650 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m15:59:20.286663 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m15:59:20.286663 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m15:59:20.287644 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m15:59:20.291018 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m15:59:20.291947 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.291947 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m15:59:20.291947 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.292977 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m15:59:20.292977 [debug] [Thread-1 (]: Began running node model.finance.my_second_dbt_model
[0m15:59:20.293974 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.my_second_dbt_model"
[0m15:59:20.293974 [debug] [Thread-1 (]: Began compiling node model.finance.my_second_dbt_model
[0m15:59:20.293974 [debug] [Thread-1 (]: Compiling model.finance.my_second_dbt_model
[0m15:59:20.297000 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.my_second_dbt_model"
[0m15:59:20.297745 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.297745 [debug] [Thread-1 (]: Began executing node model.finance.my_second_dbt_model
[0m15:59:20.297745 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.298745 [debug] [Thread-1 (]: Finished running node model.finance.my_second_dbt_model
[0m15:59:20.298745 [debug] [Thread-1 (]: Began running node test.finance.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:59:20.299741 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:59:20.299741 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:59:20.299741 [debug] [Thread-1 (]: Compiling test.finance.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:59:20.302731 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:59:20.303728 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.303728 [debug] [Thread-1 (]: Began executing node test.finance.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:59:20.304728 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.304728 [debug] [Thread-1 (]: Finished running node test.finance.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:59:20.304728 [debug] [Thread-1 (]: Began running node test.finance.unique_my_first_dbt_model_id.16e066b321
[0m15:59:20.305832 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.unique_my_first_dbt_model_id.16e066b321"
[0m15:59:20.305832 [debug] [Thread-1 (]: Began compiling node test.finance.unique_my_first_dbt_model_id.16e066b321
[0m15:59:20.305832 [debug] [Thread-1 (]: Compiling test.finance.unique_my_first_dbt_model_id.16e066b321
[0m15:59:20.311835 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.unique_my_first_dbt_model_id.16e066b321"
[0m15:59:20.311835 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.313028 [debug] [Thread-1 (]: Began executing node test.finance.unique_my_first_dbt_model_id.16e066b321
[0m15:59:20.313028 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.313028 [debug] [Thread-1 (]: Finished running node test.finance.unique_my_first_dbt_model_id.16e066b321
[0m15:59:20.313028 [debug] [Thread-1 (]: Began running node test.finance.not_null_my_second_dbt_model_id.151b76d778
[0m15:59:20.314065 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_my_second_dbt_model_id.151b76d778"
[0m15:59:20.314065 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_my_second_dbt_model_id.151b76d778
[0m15:59:20.314065 [debug] [Thread-1 (]: Compiling test.finance.not_null_my_second_dbt_model_id.151b76d778
[0m15:59:20.318107 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_my_second_dbt_model_id.151b76d778"
[0m15:59:20.319035 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.319035 [debug] [Thread-1 (]: Began executing node test.finance.not_null_my_second_dbt_model_id.151b76d778
[0m15:59:20.319035 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.319035 [debug] [Thread-1 (]: Finished running node test.finance.not_null_my_second_dbt_model_id.151b76d778
[0m15:59:20.320027 [debug] [Thread-1 (]: Began running node test.finance.unique_my_second_dbt_model_id.57a0f8c493
[0m15:59:20.320027 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.unique_my_second_dbt_model_id.57a0f8c493"
[0m15:59:20.320027 [debug] [Thread-1 (]: Began compiling node test.finance.unique_my_second_dbt_model_id.57a0f8c493
[0m15:59:20.321153 [debug] [Thread-1 (]: Compiling test.finance.unique_my_second_dbt_model_id.57a0f8c493
[0m15:59:20.324268 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.unique_my_second_dbt_model_id.57a0f8c493"
[0m15:59:20.324268 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.325175 [debug] [Thread-1 (]: Began executing node test.finance.unique_my_second_dbt_model_id.57a0f8c493
[0m15:59:20.325175 [debug] [Thread-1 (]: finished collecting timing info
[0m15:59:20.326174 [debug] [Thread-1 (]: Finished running node test.finance.unique_my_second_dbt_model_id.57a0f8c493
[0m15:59:20.327171 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:59:20.327171 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m15:59:20.327171 [debug] [MainThread]: Connection 'test.finance.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m15:59:20.335300 [info ] [MainThread]: Done.
[0m15:59:20.507970 [debug] [MainThread]: Acquiring new databricks connection "generate_catalog"
[0m15:59:20.508934 [info ] [MainThread]: Building catalog
[0m15:59:20.510925 [debug] [ThreadPool]: Acquiring new databricks connection "finance"
[0m15:59:20.511938 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation finance.dim_check_month
[0m15:59:20.511938 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation finance.dim_exp
[0m15:59:20.511938 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation finance.dim_info
[0m15:59:20.511938 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation finance.dim_month
[0m15:59:20.513017 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation finance.dim_profit
[0m15:59:20.513017 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation finance.dim_unit
[0m15:59:20.513017 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation finance.finance_bronze_to_silver
[0m15:59:20.525144 [info ] [MainThread]: Catalog written to C:\Users\XavierDonBosco\Documents\dbt_project\finance\target\catalog.json
[0m15:59:20.526327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C1DE27310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C1DE26EF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C1DE24FD0>]}
[0m15:59:20.526327 [debug] [MainThread]: Flushing usage events
[0m15:59:23.092141 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m15:59:23.093122 [debug] [MainThread]: Connection 'finance' was properly closed.


============================== 2022-11-27 16:00:11.571999 | 854b8f7c-52d3-4046-9318-1c36bca7e753 ==============================
[0m16:00:11.571999 [info ] [MainThread]: Running with dbt=1.3.1
[0m16:00:11.573000 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'port': 8080, 'open_browser': True, 'which': 'serve', 'indirect_selection': 'eager'}
[0m16:00:11.573922 [debug] [MainThread]: Tracking: tracking
[0m16:00:11.583771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B51AEE9CC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B51AEE98D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B51AEEB310>]}
[0m16:00:11.585917 [info ] [MainThread]: Serving docs at 0.0.0.0:8080
[0m16:00:11.586914 [info ] [MainThread]: To access from your browser, navigate to:  http://localhost:8080
[0m16:00:11.586914 [info ] [MainThread]: 
[0m16:00:11.594204 [info ] [MainThread]: 
[0m16:00:11.595185 [info ] [MainThread]: Press Ctrl+C to exit.
[0m16:02:51.233843 [debug] [MainThread]: Flushing usage events
[0m16:02:55.533679 [info ] [MainThread]: ctrl-c


============================== 2022-11-28 05:46:19.196915 | f6216eb7-5679-4d7a-98f2-3c3a220d1f36 ==============================
[0m05:46:19.196915 [info ] [MainThread]: Running with dbt=1.3.1
[0m05:46:19.196915 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m05:46:19.196915 [debug] [MainThread]: Tracking: tracking
[0m05:46:19.208584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BB73D9C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BB73D9900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BB73DB340>]}
[0m05:46:19.371626 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 0 files added, 4 files changed.
[0m05:46:19.371626 [debug] [MainThread]: Partial parsing: update schema file: finance://models\schema.yml
[0m05:46:19.371626 [debug] [MainThread]: Partial parsing: deleted file: finance://models\example\my_first_dbt_model.sql
[0m05:46:19.371626 [debug] [MainThread]: Partial parsing: deleted file: finance://models\example\my_second_dbt_model.sql
[0m05:46:19.375628 [debug] [MainThread]: Partial parsing: updated file: finance://models\dim_exp.sql
[0m05:46:19.375812 [debug] [MainThread]: Partial parsing: updated file: finance://models\dim_month.sql
[0m05:46:19.375812 [debug] [MainThread]: Partial parsing: updated file: finance://models\dim_unit.sql
[0m05:46:19.382010 [debug] [MainThread]: 1699: static parser successfully parsed dim_exp.sql
[0m05:46:19.402127 [debug] [MainThread]: 1699: static parser successfully parsed dim_unit.sql
[0m05:46:19.403135 [debug] [MainThread]: 1699: static parser successfully parsed dim_profit.sql
[0m05:46:19.406764 [debug] [MainThread]: 1699: static parser successfully parsed dim_month.sql
[0m05:46:19.502038 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.finance.example

[0m05:46:19.502038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f6216eb7-5679-4d7a-98f2-3c3a220d1f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BB85C58A0>]}
[0m05:46:19.511730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f6216eb7-5679-4d7a-98f2-3c3a220d1f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BB75A53C0>]}
[0m05:46:19.511730 [info ] [MainThread]: Found 6 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m05:46:19.511730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f6216eb7-5679-4d7a-98f2-3c3a220d1f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BB75A5360>]}
[0m05:46:19.521661 [info ] [MainThread]: 
[0m05:46:19.522605 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:46:19.522605 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m05:46:19.531848 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m05:46:19.531848 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m05:46:19.533898 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:46:23.082236 [debug] [ThreadPool]: SQL status: OK in 3.55 seconds
[0m05:46:23.093367 [debug] [ThreadPool]: On list_schemas: Close
[0m05:46:24.432283 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m05:46:24.442372 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m05:46:24.446955 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m05:46:24.446955 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m05:46:24.447694 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:46:26.921977 [debug] [ThreadPool]: SQL status: OK in 2.47 seconds
[0m05:46:26.922107 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m05:46:26.922107 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m05:46:26.922107 [debug] [ThreadPool]: On list_None_finance: Close
[0m05:46:27.822538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f6216eb7-5679-4d7a-98f2-3c3a220d1f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BB7547400>]}
[0m05:46:27.822538 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:46:27.822538 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:46:27.827632 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m05:46:27.829479 [info ] [MainThread]: 
[0m05:46:27.856222 [debug] [Thread-1 (]: Began running node model.finance.dim_exp
[0m05:46:27.856222 [info ] [Thread-1 (]: 1 of 6 START sql table model finance.dim_exp ................................... [RUN]
[0m05:46:27.858845 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_exp"
[0m05:46:27.858845 [debug] [Thread-1 (]: Began compiling node model.finance.dim_exp
[0m05:46:27.859931 [debug] [Thread-1 (]: Compiling model.finance.dim_exp
[0m05:46:27.862071 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_exp"
[0m05:46:27.863713 [debug] [Thread-1 (]: finished collecting timing info
[0m05:46:27.863976 [debug] [Thread-1 (]: Began executing node model.finance.dim_exp
[0m05:46:27.902226 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_exp"
[0m05:46:27.902226 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:46:27.902226 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_exp"
[0m05:46:27.902226 [debug] [Thread-1 (]: On model.finance.dim_exp: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_exp"} */

  
    
      create or replace table finance.dim_exp
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Expenses, Gross_sales, Manufacturing_price from data.table_finance;
  
[0m05:46:27.907232 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m05:46:50.811711 [debug] [Thread-1 (]: SQL status: OK in 22.9 seconds
[0m05:46:50.822029 [debug] [Thread-1 (]: finished collecting timing info
[0m05:46:50.822029 [debug] [Thread-1 (]: On model.finance.dim_exp: ROLLBACK
[0m05:46:50.825932 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:46:50.825932 [debug] [Thread-1 (]: On model.finance.dim_exp: Close
[0m05:46:52.653188 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f6216eb7-5679-4d7a-98f2-3c3a220d1f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BB86D0B80>]}
[0m05:46:52.653188 [info ] [Thread-1 (]: 1 of 6 OK created sql table model finance.dim_exp .............................. [[32mOK[0m in 24.79s]
[0m05:46:52.653188 [debug] [Thread-1 (]: Finished running node model.finance.dim_exp
[0m05:46:52.653188 [debug] [Thread-1 (]: Began running node model.finance.dim_info
[0m05:46:52.656532 [info ] [Thread-1 (]: 2 of 6 START sql table model finance.dim_info .................................. [RUN]
[0m05:46:52.656961 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_info"
[0m05:46:52.658140 [debug] [Thread-1 (]: Began compiling node model.finance.dim_info
[0m05:46:52.658140 [debug] [Thread-1 (]: Compiling model.finance.dim_info
[0m05:46:52.658592 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_info"
[0m05:46:52.663202 [debug] [Thread-1 (]: finished collecting timing info
[0m05:46:52.663202 [debug] [Thread-1 (]: Began executing node model.finance.dim_info
[0m05:46:52.664844 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_info"
[0m05:46:52.668865 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:46:52.668865 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_info"
[0m05:46:52.669489 [debug] [Thread-1 (]: On model.finance.dim_info: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_info"} */

  
    
      create or replace table finance.dim_info
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, COGS, Inventory, Segment from data.table_finance;
  
[0m05:46:52.669489 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:46:58.213274 [debug] [Thread-1 (]: SQL status: OK in 5.54 seconds
[0m05:46:58.216809 [debug] [Thread-1 (]: finished collecting timing info
[0m05:46:58.216809 [debug] [Thread-1 (]: On model.finance.dim_info: ROLLBACK
[0m05:46:58.220172 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:46:58.220172 [debug] [Thread-1 (]: On model.finance.dim_info: Close
[0m05:46:59.141913 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f6216eb7-5679-4d7a-98f2-3c3a220d1f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BB85AC3A0>]}
[0m05:46:59.141913 [info ] [Thread-1 (]: 2 of 6 OK created sql table model finance.dim_info ............................. [[32mOK[0m in 6.48s]
[0m05:46:59.148657 [debug] [Thread-1 (]: Finished running node model.finance.dim_info
[0m05:46:59.148657 [debug] [Thread-1 (]: Began running node model.finance.dim_month
[0m05:46:59.151161 [info ] [Thread-1 (]: 3 of 6 START sql table model finance.dim_month ................................. [RUN]
[0m05:46:59.152136 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_month"
[0m05:46:59.152136 [debug] [Thread-1 (]: Began compiling node model.finance.dim_month
[0m05:46:59.153846 [debug] [Thread-1 (]: Compiling model.finance.dim_month
[0m05:46:59.154334 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_month"
[0m05:46:59.160050 [debug] [Thread-1 (]: finished collecting timing info
[0m05:46:59.160361 [debug] [Thread-1 (]: Began executing node model.finance.dim_month
[0m05:46:59.162667 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_month"
[0m05:46:59.165058 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:46:59.165767 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_month"
[0m05:46:59.165767 [debug] [Thread-1 (]: On model.finance.dim_month: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_month"} */

  
    
      create or replace table finance.dim_month
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Month_Number, Year, MonthName, Date from data.table_finance;
  
[0m05:46:59.166200 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:47:04.671835 [debug] [Thread-1 (]: SQL status: OK in 5.51 seconds
[0m05:47:04.671835 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:04.671835 [debug] [Thread-1 (]: On model.finance.dim_month: ROLLBACK
[0m05:47:04.671835 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:04.676387 [debug] [Thread-1 (]: On model.finance.dim_month: Close
[0m05:47:05.572368 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f6216eb7-5679-4d7a-98f2-3c3a220d1f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BB85AD960>]}
[0m05:47:05.572368 [info ] [Thread-1 (]: 3 of 6 OK created sql table model finance.dim_month ............................ [[32mOK[0m in 6.42s]
[0m05:47:05.581938 [debug] [Thread-1 (]: Finished running node model.finance.dim_month
[0m05:47:05.581938 [debug] [Thread-1 (]: Began running node model.finance.dim_profit
[0m05:47:05.581938 [info ] [Thread-1 (]: 4 of 6 START sql table model finance.dim_profit ................................ [RUN]
[0m05:47:05.581938 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_profit"
[0m05:47:05.581938 [debug] [Thread-1 (]: Began compiling node model.finance.dim_profit
[0m05:47:05.581938 [debug] [Thread-1 (]: Compiling model.finance.dim_profit
[0m05:47:05.581938 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_profit"
[0m05:47:05.592011 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:05.592011 [debug] [Thread-1 (]: Began executing node model.finance.dim_profit
[0m05:47:05.593336 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_profit"
[0m05:47:05.593336 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:05.597447 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_profit"
[0m05:47:05.597662 [debug] [Thread-1 (]: On model.finance.dim_profit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_profit"} */

  
    
      create or replace table finance.dim_profit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Discounts, Profit, Sales, Country, Discount_Band, Product from data.table_finance;
  
[0m05:47:05.597871 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:47:10.253168 [debug] [Thread-1 (]: SQL status: OK in 4.66 seconds
[0m05:47:10.253168 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:10.253168 [debug] [Thread-1 (]: On model.finance.dim_profit: ROLLBACK
[0m05:47:10.253168 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:10.256421 [debug] [Thread-1 (]: On model.finance.dim_profit: Close
[0m05:47:11.161914 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f6216eb7-5679-4d7a-98f2-3c3a220d1f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BB889B2E0>]}
[0m05:47:11.161914 [info ] [Thread-1 (]: 4 of 6 OK created sql table model finance.dim_profit ........................... [[32mOK[0m in 5.57s]
[0m05:47:11.161914 [debug] [Thread-1 (]: Finished running node model.finance.dim_profit
[0m05:47:11.161914 [debug] [Thread-1 (]: Began running node model.finance.dim_unit
[0m05:47:11.161914 [info ] [Thread-1 (]: 5 of 6 START sql table model finance.dim_unit .................................. [RUN]
[0m05:47:11.161914 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_unit"
[0m05:47:11.161914 [debug] [Thread-1 (]: Began compiling node model.finance.dim_unit
[0m05:47:11.161914 [debug] [Thread-1 (]: Compiling model.finance.dim_unit
[0m05:47:11.172375 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_unit"
[0m05:47:11.172375 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:11.172375 [debug] [Thread-1 (]: Began executing node model.finance.dim_unit
[0m05:47:11.178090 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_unit"
[0m05:47:11.182208 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:11.182535 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_unit"
[0m05:47:11.182535 [debug] [Thread-1 (]: On model.finance.dim_unit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_unit"} */

  
    
      create or replace table finance.dim_unit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Target, Unit_Price, Units_Sold from data.table_finance;
  
[0m05:47:11.183033 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:47:15.452179 [debug] [Thread-1 (]: SQL status: OK in 4.27 seconds
[0m05:47:15.452179 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:15.452179 [debug] [Thread-1 (]: On model.finance.dim_unit: ROLLBACK
[0m05:47:15.461311 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:15.461822 [debug] [Thread-1 (]: On model.finance.dim_unit: Close
[0m05:47:16.401931 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f6216eb7-5679-4d7a-98f2-3c3a220d1f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BB8753B80>]}
[0m05:47:16.401931 [info ] [Thread-1 (]: 5 of 6 OK created sql table model finance.dim_unit ............................. [[32mOK[0m in 5.24s]
[0m05:47:16.401931 [debug] [Thread-1 (]: Finished running node model.finance.dim_unit
[0m05:47:16.401931 [debug] [Thread-1 (]: Began running node model.finance.dim_check_month
[0m05:47:16.401931 [info ] [Thread-1 (]: 6 of 6 START sql table model finance.dim_check_month ........................... [RUN]
[0m05:47:16.401931 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_check_month"
[0m05:47:16.401931 [debug] [Thread-1 (]: Began compiling node model.finance.dim_check_month
[0m05:47:16.401931 [debug] [Thread-1 (]: Compiling model.finance.dim_check_month
[0m05:47:16.411843 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_check_month"
[0m05:47:16.411843 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:16.418406 [debug] [Thread-1 (]: Began executing node model.finance.dim_check_month
[0m05:47:16.421906 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_check_month"
[0m05:47:16.421906 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:16.423852 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_check_month"
[0m05:47:16.424078 [debug] [Thread-1 (]: On model.finance.dim_check_month: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_check_month"} */

  
    
      create or replace table finance.dim_check_month
    
    
    using delta
    
    
    
    
    
    
    as
      

select date from finance.dim_month where date > '2018-02-01' and date < '2018-03-01';
  
[0m05:47:16.424199 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:47:22.134805 [debug] [Thread-1 (]: SQL status: OK in 5.71 seconds
[0m05:47:22.134805 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:22.134805 [debug] [Thread-1 (]: On model.finance.dim_check_month: ROLLBACK
[0m05:47:22.134805 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:22.140388 [debug] [Thread-1 (]: On model.finance.dim_check_month: Close
[0m05:47:23.072268 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f6216eb7-5679-4d7a-98f2-3c3a220d1f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BB876F8B0>]}
[0m05:47:23.072268 [info ] [Thread-1 (]: 6 of 6 OK created sql table model finance.dim_check_month ...................... [[32mOK[0m in 6.67s]
[0m05:47:23.081891 [debug] [Thread-1 (]: Finished running node model.finance.dim_check_month
[0m05:47:23.083263 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:47:23.085434 [debug] [MainThread]: On master: ROLLBACK
[0m05:47:23.085859 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:47:24.021873 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:47:24.021873 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:24.021873 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:47:24.031472 [debug] [MainThread]: On master: ROLLBACK
[0m05:47:24.032437 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:47:24.032588 [debug] [MainThread]: On master: Close
[0m05:47:25.058120 [info ] [MainThread]: 
[0m05:47:25.058120 [info ] [MainThread]: Finished running 6 table models in 0 hours 1 minutes and 5.54 seconds (65.54s).
[0m05:47:25.062908 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:47:25.062908 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m05:47:25.062908 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m05:47:25.064326 [debug] [MainThread]: Connection 'model.finance.dim_check_month' was properly closed.
[0m05:47:25.073459 [info ] [MainThread]: 
[0m05:47:25.073729 [info ] [MainThread]: [32mCompleted successfully[0m
[0m05:47:25.073729 [info ] [MainThread]: 
[0m05:47:25.082190 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m05:47:25.082190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BB75A5480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BB75475B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BB85ACB20>]}
[0m05:47:25.085179 [debug] [MainThread]: Flushing usage events


============================== 2022-11-28 05:47:32.251506 | 167c9a03-3791-471b-83ec-e2ad0c02eafb ==============================
[0m05:47:32.251506 [info ] [MainThread]: Running with dbt=1.3.1
[0m05:47:32.251506 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'which': 'test', 'rpc_method': 'test'}
[0m05:47:32.253603 [debug] [MainThread]: Tracking: tracking
[0m05:47:32.261661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025AA27E9C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025AA27E9900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025AA27EB340>]}
[0m05:47:32.321535 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m05:47:32.321535 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m05:47:32.321535 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.finance.example

[0m05:47:32.331500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '167c9a03-3791-471b-83ec-e2ad0c02eafb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025AA2A100D0>]}
[0m05:47:32.345556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '167c9a03-3791-471b-83ec-e2ad0c02eafb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025AA29E4040>]}
[0m05:47:32.346329 [info ] [MainThread]: Found 6 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m05:47:32.346654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '167c9a03-3791-471b-83ec-e2ad0c02eafb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025AA29E40D0>]}
[0m05:47:32.346654 [info ] [MainThread]: 
[0m05:47:32.346654 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:47:32.351618 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m05:47:32.361650 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:32.364659 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m05:47:32.364894 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m05:47:32.364894 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:47:35.101998 [debug] [ThreadPool]: SQL status: OK in 2.74 seconds
[0m05:47:35.111970 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m05:47:35.111970 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m05:47:35.111970 [debug] [ThreadPool]: On list_None_finance: Close
[0m05:47:36.032098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '167c9a03-3791-471b-83ec-e2ad0c02eafb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025AA29BFD90>]}
[0m05:47:36.032098 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:36.032098 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:47:36.038857 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m05:47:36.038857 [info ] [MainThread]: 
[0m05:47:36.044037 [debug] [Thread-1 (]: Began running node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:47:36.044037 [info ] [Thread-1 (]: 1 of 25 START test accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business  [RUN]
[0m05:47:36.044037 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m05:47:36.044037 [debug] [Thread-1 (]: Began compiling node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:47:36.044037 [debug] [Thread-1 (]: Compiling test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:47:36.062297 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m05:47:36.063371 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:36.063371 [debug] [Thread-1 (]: Began executing node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:47:36.076801 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m05:47:36.076801 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:36.076801 [debug] [Thread-1 (]: Using databricks connection "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m05:47:36.076801 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        Segment as value_field,
        count(*) as n_records

    from finance.dim_info
    group by Segment

)

select *
from all_values
where value_field not in (
    'Government','Channel Partners','Midmarket','Enterprise','Small Business'
)



      
    ) dbt_internal_test
[0m05:47:36.076801 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m05:47:39.101791 [debug] [Thread-1 (]: SQL status: OK in 3.02 seconds
[0m05:47:39.112335 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:39.112335 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: ROLLBACK
[0m05:47:39.112335 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:39.112335 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: Close
[0m05:47:40.021848 [info ] [Thread-1 (]: 1 of 25 PASS accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business  [[32mPASS[0m in 3.98s]
[0m05:47:40.031988 [debug] [Thread-1 (]: Finished running node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:47:40.036658 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:47:40.036658 [info ] [Thread-1 (]: 2 of 25 START test not_null_dim_exp_Actual ..................................... [RUN]
[0m05:47:40.042275 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Actual.1d45aa27dd"
[0m05:47:40.042275 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:47:40.042275 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:47:40.081985 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Actual.1d45aa27dd"
[0m05:47:40.089455 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:40.090509 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:47:40.091799 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Actual.1d45aa27dd"
[0m05:47:40.093928 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:40.094544 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Actual.1d45aa27dd"
[0m05:47:40.094743 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Actual.1d45aa27dd: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Actual.1d45aa27dd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_exp
where Actual is null



      
    ) dbt_internal_test
[0m05:47:40.094969 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:47:42.402058 [debug] [Thread-1 (]: SQL status: OK in 2.31 seconds
[0m05:47:42.402058 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:42.411731 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Actual.1d45aa27dd: ROLLBACK
[0m05:47:42.412441 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:42.412782 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Actual.1d45aa27dd: Close
[0m05:47:43.311996 [info ] [Thread-1 (]: 2 of 25 PASS not_null_dim_exp_Actual ........................................... [[32mPASS[0m in 3.27s]
[0m05:47:43.321735 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:47:43.321735 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:47:43.323673 [info ] [Thread-1 (]: 3 of 25 START test not_null_dim_exp_Expenses ................................... [RUN]
[0m05:47:43.324539 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m05:47:43.324539 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:47:43.324539 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:47:43.326673 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m05:47:43.331504 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:43.331504 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:47:43.333049 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m05:47:43.333049 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:43.335753 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m05:47:43.336046 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Expenses.c94546fc2a: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Expenses.c94546fc2a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Expenses
from finance.dim_exp
where Expenses is null



      
    ) dbt_internal_test
[0m05:47:43.336563 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:47:45.404277 [debug] [Thread-1 (]: SQL status: OK in 2.07 seconds
[0m05:47:45.404277 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:45.404277 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Expenses.c94546fc2a: ROLLBACK
[0m05:47:45.408403 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:45.408637 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Expenses.c94546fc2a: Close
[0m05:47:46.372154 [info ] [Thread-1 (]: 3 of 25 PASS not_null_dim_exp_Expenses ......................................... [[32mPASS[0m in 3.05s]
[0m05:47:46.372154 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:47:46.380734 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:47:46.380836 [info ] [Thread-1 (]: 4 of 25 START test not_null_dim_exp_Gross_sales ................................ [RUN]
[0m05:47:46.381694 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m05:47:46.381694 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:47:46.381694 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:47:46.383755 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m05:47:46.389001 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:46.390514 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:47:46.392178 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m05:47:46.392178 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:46.394579 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m05:47:46.394579 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Gross_sales.07439d5e88: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Gross_sales
from finance.dim_exp
where Gross_sales is null



      
    ) dbt_internal_test
[0m05:47:46.395315 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:47:48.462602 [debug] [Thread-1 (]: SQL status: OK in 2.07 seconds
[0m05:47:48.471975 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:48.471975 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Gross_sales.07439d5e88: ROLLBACK
[0m05:47:48.471975 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:48.471975 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Gross_sales.07439d5e88: Close
[0m05:47:49.362236 [info ] [Thread-1 (]: 4 of 25 PASS not_null_dim_exp_Gross_sales ...................................... [[32mPASS[0m in 2.98s]
[0m05:47:49.372028 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:47:49.373637 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:47:49.374144 [info ] [Thread-1 (]: 5 of 25 START test not_null_dim_exp_Manufacturing_price ........................ [RUN]
[0m05:47:49.375381 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m05:47:49.375381 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:47:49.377309 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:47:49.382118 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m05:47:49.382305 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:49.382305 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:47:49.383280 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m05:47:49.385438 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:49.385750 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m05:47:49.386018 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Manufacturing_price
from finance.dim_exp
where Manufacturing_price is null



      
    ) dbt_internal_test
[0m05:47:49.386123 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:47:51.462096 [debug] [Thread-1 (]: SQL status: OK in 2.08 seconds
[0m05:47:51.462096 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:51.471609 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f: ROLLBACK
[0m05:47:51.471609 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:51.471609 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f: Close
[0m05:47:52.381988 [info ] [Thread-1 (]: 5 of 25 PASS not_null_dim_exp_Manufacturing_price .............................. [[32mPASS[0m in 3.01s]
[0m05:47:52.381988 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:47:52.391772 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:47:52.392541 [info ] [Thread-1 (]: 6 of 25 START test not_null_dim_info_Actual .................................... [RUN]
[0m05:47:52.393035 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m05:47:52.393035 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:47:52.393035 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:47:52.397394 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m05:47:52.397394 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:52.397394 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:47:52.402122 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m05:47:52.402122 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:52.404833 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m05:47:52.404833 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Actual.5cb2ffbb95"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_info
where Actual is null



      
    ) dbt_internal_test
[0m05:47:52.406096 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:47:54.436844 [debug] [Thread-1 (]: SQL status: OK in 2.03 seconds
[0m05:47:54.436844 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:54.436844 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: ROLLBACK
[0m05:47:54.440783 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:54.440783 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: Close
[0m05:47:55.332221 [info ] [Thread-1 (]: 6 of 25 PASS not_null_dim_info_Actual .......................................... [[32mPASS[0m in 2.94s]
[0m05:47:55.342014 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:47:55.342014 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:47:55.345094 [info ] [Thread-1 (]: 7 of 25 START test not_null_dim_info_COGS ...................................... [RUN]
[0m05:47:55.346155 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m05:47:55.346155 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:47:55.348009 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:47:55.352198 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m05:47:55.352198 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:55.354727 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:47:55.355571 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m05:47:55.358459 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:55.358684 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m05:47:55.358684 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_COGS.efb9b66052"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select COGS
from finance.dim_info
where COGS is null



      
    ) dbt_internal_test
[0m05:47:55.359229 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:47:57.463476 [debug] [Thread-1 (]: SQL status: OK in 2.1 seconds
[0m05:47:57.463476 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:57.463476 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: ROLLBACK
[0m05:47:57.469937 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:57.470212 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: Close
[0m05:47:58.382056 [info ] [Thread-1 (]: 7 of 25 PASS not_null_dim_info_COGS ............................................ [[32mPASS[0m in 3.04s]
[0m05:47:58.392228 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:47:58.394340 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Inventory.45016db171
[0m05:47:58.394840 [info ] [Thread-1 (]: 8 of 25 START test not_null_dim_info_Inventory ................................. [RUN]
[0m05:47:58.394840 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Inventory.45016db171"
[0m05:47:58.394840 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Inventory.45016db171
[0m05:47:58.394840 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Inventory.45016db171
[0m05:47:58.401997 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Inventory.45016db171"
[0m05:47:58.405595 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:58.405595 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Inventory.45016db171
[0m05:47:58.407109 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Inventory.45016db171"
[0m05:47:58.407109 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:58.407109 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Inventory.45016db171"
[0m05:47:58.407109 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Inventory.45016db171"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Inventory
from finance.dim_info
where Inventory is null



      
    ) dbt_internal_test
[0m05:47:58.407109 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:00.444260 [debug] [Thread-1 (]: SQL status: OK in 2.04 seconds
[0m05:48:00.451680 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:00.451680 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: ROLLBACK
[0m05:48:00.451680 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:00.451680 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: Close
[0m05:48:01.374266 [info ] [Thread-1 (]: 8 of 25 PASS not_null_dim_info_Inventory ....................................... [[32mPASS[0m in 2.98s]
[0m05:48:01.383260 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Inventory.45016db171
[0m05:48:01.386302 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:48:01.386357 [info ] [Thread-1 (]: 9 of 25 START test not_null_dim_info_Segment ................................... [RUN]
[0m05:48:01.387672 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m05:48:01.387672 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:48:01.387672 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:48:01.392139 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m05:48:01.395509 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:01.395772 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:48:01.395772 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m05:48:01.395772 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:01.395772 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m05:48:01.395772 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Segment.57cc6a2d98"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Segment
from finance.dim_info
where Segment is null



      
    ) dbt_internal_test
[0m05:48:01.398960 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:03.452314 [debug] [Thread-1 (]: SQL status: OK in 2.05 seconds
[0m05:48:03.462264 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:03.462264 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: ROLLBACK
[0m05:48:03.462264 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:03.468300 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: Close
[0m05:48:04.382321 [info ] [Thread-1 (]: 9 of 25 PASS not_null_dim_info_Segment ......................................... [[32mPASS[0m in 2.99s]
[0m05:48:04.382321 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:48:04.382321 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:48:04.390682 [info ] [Thread-1 (]: 10 of 25 START test not_null_dim_month_Actual .................................. [RUN]
[0m05:48:04.391676 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Actual.9fdc4e57c7"
[0m05:48:04.391676 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:48:04.391676 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:48:04.396870 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Actual.9fdc4e57c7"
[0m05:48:04.400815 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:04.401048 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:48:04.401640 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Actual.9fdc4e57c7"
[0m05:48:04.401640 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:04.403946 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Actual.9fdc4e57c7"
[0m05:48:04.404121 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Actual.9fdc4e57c7: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Actual.9fdc4e57c7"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_month
where Actual is null



      
    ) dbt_internal_test
[0m05:48:04.404474 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:06.505581 [debug] [Thread-1 (]: SQL status: OK in 2.1 seconds
[0m05:48:06.505581 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:06.511774 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Actual.9fdc4e57c7: ROLLBACK
[0m05:48:06.512104 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:06.512104 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Actual.9fdc4e57c7: Close
[0m05:48:07.532099 [info ] [Thread-1 (]: 10 of 25 PASS not_null_dim_month_Actual ........................................ [[32mPASS[0m in 3.14s]
[0m05:48:07.532099 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:48:07.535881 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:48:07.536506 [info ] [Thread-1 (]: 11 of 25 START test not_null_dim_month_Date .................................... [RUN]
[0m05:48:07.537325 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m05:48:07.537325 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:48:07.537325 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:48:07.541838 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m05:48:07.544679 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:07.544836 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:48:07.545336 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m05:48:07.545336 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:07.545336 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m05:48:07.547814 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Date.ce09cea79f: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Date.ce09cea79f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Date
from finance.dim_month
where Date is null



      
    ) dbt_internal_test
[0m05:48:07.547814 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:09.644812 [debug] [Thread-1 (]: SQL status: OK in 2.1 seconds
[0m05:48:09.648276 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:09.648276 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Date.ce09cea79f: ROLLBACK
[0m05:48:09.648276 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:09.650467 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Date.ce09cea79f: Close
[0m05:48:10.548106 [info ] [Thread-1 (]: 11 of 25 PASS not_null_dim_month_Date .......................................... [[32mPASS[0m in 3.01s]
[0m05:48:10.548106 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:48:10.550176 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:48:10.550231 [info ] [Thread-1 (]: 12 of 25 START test not_null_dim_month_MonthName ............................... [RUN]
[0m05:48:10.550727 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m05:48:10.550727 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:48:10.551871 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:48:10.557229 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m05:48:10.558421 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:10.558918 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:48:10.562656 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m05:48:10.565674 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:10.565774 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m05:48:10.566215 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_MonthName.c1d116d50b: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_MonthName.c1d116d50b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select MonthName
from finance.dim_month
where MonthName is null



      
    ) dbt_internal_test
[0m05:48:10.566215 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:12.601880 [debug] [Thread-1 (]: SQL status: OK in 2.04 seconds
[0m05:48:12.601880 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:12.601880 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_MonthName.c1d116d50b: ROLLBACK
[0m05:48:12.610387 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:12.610387 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_MonthName.c1d116d50b: Close
[0m05:48:13.502645 [info ] [Thread-1 (]: 12 of 25 PASS not_null_dim_month_MonthName ..................................... [[32mPASS[0m in 2.95s]
[0m05:48:13.510047 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:48:13.511580 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:48:13.511580 [info ] [Thread-1 (]: 13 of 25 START test not_null_dim_month_Month_Number ............................ [RUN]
[0m05:48:13.513372 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m05:48:13.514369 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:48:13.514587 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:48:13.515051 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m05:48:13.515051 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:13.515051 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:48:13.522180 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m05:48:13.523797 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:13.523797 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m05:48:13.523797 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Month_Number.be337b93c3: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Month_Number.be337b93c3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Month_Number
from finance.dim_month
where Month_Number is null



      
    ) dbt_internal_test
[0m05:48:13.524401 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:15.581985 [debug] [Thread-1 (]: SQL status: OK in 2.06 seconds
[0m05:48:15.581985 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:15.581985 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Month_Number.be337b93c3: ROLLBACK
[0m05:48:15.581985 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:15.581985 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Month_Number.be337b93c3: Close
[0m05:48:16.502032 [info ] [Thread-1 (]: 13 of 25 PASS not_null_dim_month_Month_Number .................................. [[32mPASS[0m in 2.99s]
[0m05:48:16.502032 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:48:16.502032 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:48:16.502032 [info ] [Thread-1 (]: 14 of 25 START test not_null_dim_month_Year .................................... [RUN]
[0m05:48:16.510482 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m05:48:16.510482 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:48:16.511947 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:48:16.525779 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m05:48:16.528004 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:16.528527 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:48:16.531758 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m05:48:16.531758 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:16.531758 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m05:48:16.531758 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Year.5d11bba8bb: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Year.5d11bba8bb"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Year
from finance.dim_month
where Year is null



      
    ) dbt_internal_test
[0m05:48:16.535049 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:18.681611 [debug] [Thread-1 (]: SQL status: OK in 2.15 seconds
[0m05:48:18.681611 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:18.681611 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Year.5d11bba8bb: ROLLBACK
[0m05:48:18.690287 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:18.690287 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Year.5d11bba8bb: Close
[0m05:48:19.701932 [info ] [Thread-1 (]: 14 of 25 PASS not_null_dim_month_Year .......................................... [[32mPASS[0m in 3.20s]
[0m05:48:19.701932 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:48:19.701932 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Actual.6279364421
[0m05:48:19.709824 [info ] [Thread-1 (]: 15 of 25 START test not_null_dim_profit_Actual ................................. [RUN]
[0m05:48:19.711425 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Actual.6279364421"
[0m05:48:19.712044 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Actual.6279364421
[0m05:48:19.712540 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Actual.6279364421
[0m05:48:19.721442 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Actual.6279364421"
[0m05:48:19.726058 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:19.726058 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Actual.6279364421
[0m05:48:19.728060 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Actual.6279364421"
[0m05:48:19.731775 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:19.731775 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Actual.6279364421"
[0m05:48:19.731775 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Actual.6279364421: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Actual.6279364421"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_profit
where Actual is null



      
    ) dbt_internal_test
[0m05:48:19.731775 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:21.772021 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Actual.6279364421"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_profit
where Actual is null



      
    ) dbt_internal_test
[0m05:48:21.772021 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Actual` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`finance`.`dim_profit`.`Sales`, `spark_catalog`.`finance`.`dim_profit`.`Profit`, `spark_catalog`.`finance`.`dim_profit`.`Country`, `spark_catalog`.`finance`.`dim_profit`.`Product`, `spark_catalog`.`finance`.`dim_profit`.`Discounts`]; line 15 pos 6
[0m05:48:21.772021 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Actual` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`finance`.`dim_profit`.`Sales`, `spark_catalog`.`finance`.`dim_profit`.`Profit`, `spark_catalog`.`finance`.`dim_profit`.`Country`, `spark_catalog`.`finance`.`dim_profit`.`Product`, `spark_catalog`.`finance`.`dim_profit`.`Discounts`]; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Actual` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`finance`.`dim_profit`.`Sales`, `spark_catalog`.`finance`.`dim_profit`.`Profit`, `spark_catalog`.`finance`.`dim_profit`.`Country`, `spark_catalog`.`finance`.`dim_profit`.`Product`, `spark_catalog`.`finance`.`dim_profit`.`Discounts`]; line 15 pos 6
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m05:48:21.772021 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\xb3\x1f\x8f\x8a\x0b?@\x1a\xba\xd0\xb8\x95y\xa3.M'
[0m05:48:21.781685 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:21.781685 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Actual.6279364421: ROLLBACK
[0m05:48:21.781685 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:21.781685 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Actual.6279364421: Close
[0m05:48:22.695604 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_profit_Actual (models\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Actual` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`finance`.`dim_profit`.`Sales`, `spark_catalog`.`finance`.`dim_profit`.`Profit`, `spark_catalog`.`finance`.`dim_profit`.`Country`, `spark_catalog`.`finance`.`dim_profit`.`Product`, `spark_catalog`.`finance`.`dim_profit`.`Discounts`]; line 15 pos 6
[0m05:48:22.696602 [error] [Thread-1 (]: 15 of 25 ERROR not_null_dim_profit_Actual ...................................... [[31mERROR[0m in 2.99s]
[0m05:48:22.699585 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Actual.6279364421
[0m05:48:22.700632 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:48:22.702580 [info ] [Thread-1 (]: 16 of 25 START test not_null_dim_profit_Country ................................ [RUN]
[0m05:48:22.704567 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m05:48:22.705640 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:48:22.706560 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:48:22.718739 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m05:48:22.721677 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:22.722649 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:48:22.727190 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m05:48:22.728601 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:22.728601 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m05:48:22.729773 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Country.363de9fbd5: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Country.363de9fbd5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Country
from finance.dim_profit
where Country is null



      
    ) dbt_internal_test
[0m05:48:22.729951 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:25.014126 [debug] [Thread-1 (]: SQL status: OK in 2.28 seconds
[0m05:48:25.020097 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:25.021091 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Country.363de9fbd5: ROLLBACK
[0m05:48:25.022020 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:25.023013 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Country.363de9fbd5: Close
[0m05:48:25.930000 [info ] [Thread-1 (]: 16 of 25 PASS not_null_dim_profit_Country ...................................... [[32mPASS[0m in 3.23s]
[0m05:48:25.932990 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:48:25.934456 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:48:25.934456 [info ] [Thread-1 (]: 17 of 25 START test not_null_dim_profit_Discount_Band .......................... [RUN]
[0m05:48:25.937049 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m05:48:25.938053 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:48:25.939049 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:48:25.947088 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m05:48:25.949100 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:25.949100 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:48:25.953427 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m05:48:25.954432 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:25.954977 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m05:48:25.955125 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Discount_Band
from finance.dim_profit
where Discount_Band is null



      
    ) dbt_internal_test
[0m05:48:25.955427 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:28.317907 [debug] [Thread-1 (]: SQL status: OK in 2.36 seconds
[0m05:48:28.320906 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:28.322015 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa: ROLLBACK
[0m05:48:28.322975 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:28.322975 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa: Close
[0m05:48:29.206866 [info ] [Thread-1 (]: 17 of 25 PASS not_null_dim_profit_Discount_Band ................................ [[32mPASS[0m in 3.27s]
[0m05:48:29.208905 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:48:29.209890 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:48:29.210368 [info ] [Thread-1 (]: 18 of 25 START test not_null_dim_profit_Discounts .............................. [RUN]
[0m05:48:29.213081 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m05:48:29.213421 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:48:29.213893 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:48:29.224529 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m05:48:29.227477 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:29.228444 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:48:29.234527 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m05:48:29.236337 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:29.237339 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m05:48:29.238336 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discounts.03e349480e: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Discounts.03e349480e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Discounts
from finance.dim_profit
where Discounts is null



      
    ) dbt_internal_test
[0m05:48:29.239340 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:31.448127 [debug] [Thread-1 (]: SQL status: OK in 2.21 seconds
[0m05:48:31.453108 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:31.454180 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discounts.03e349480e: ROLLBACK
[0m05:48:31.454180 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:31.455104 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discounts.03e349480e: Close
[0m05:48:32.506237 [info ] [Thread-1 (]: 18 of 25 PASS not_null_dim_profit_Discounts .................................... [[32mPASS[0m in 3.29s]
[0m05:48:32.508828 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:48:32.509829 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Product.1422e74993
[0m05:48:32.510486 [info ] [Thread-1 (]: 19 of 25 START test not_null_dim_profit_Product ................................ [RUN]
[0m05:48:32.512492 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Product.1422e74993"
[0m05:48:32.512492 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Product.1422e74993
[0m05:48:32.513486 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Product.1422e74993
[0m05:48:32.524293 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Product.1422e74993"
[0m05:48:32.526294 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:32.526294 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Product.1422e74993
[0m05:48:32.534975 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Product.1422e74993"
[0m05:48:32.537040 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:32.537963 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Product.1422e74993"
[0m05:48:32.537963 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Product.1422e74993: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Product.1422e74993"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Product
from finance.dim_profit
where Product is null



      
    ) dbt_internal_test
[0m05:48:32.539052 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:34.558140 [debug] [Thread-1 (]: SQL status: OK in 2.02 seconds
[0m05:48:34.564114 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:34.565112 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Product.1422e74993: ROLLBACK
[0m05:48:34.566109 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:34.567107 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Product.1422e74993: Close
[0m05:48:35.448686 [info ] [Thread-1 (]: 19 of 25 PASS not_null_dim_profit_Product ...................................... [[32mPASS[0m in 2.94s]
[0m05:48:35.451567 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Product.1422e74993
[0m05:48:35.451567 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:48:35.452769 [info ] [Thread-1 (]: 20 of 25 START test not_null_dim_profit_Profit ................................. [RUN]
[0m05:48:35.454541 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m05:48:35.455534 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:48:35.455534 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:48:35.463879 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m05:48:35.465196 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:35.465196 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:48:35.471195 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m05:48:35.473194 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:35.474184 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m05:48:35.475181 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Profit.3c5f35db12: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Profit.3c5f35db12"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Profit
from finance.dim_profit
where Profit is null



      
    ) dbt_internal_test
[0m05:48:35.476048 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:37.479235 [debug] [Thread-1 (]: SQL status: OK in 2.0 seconds
[0m05:48:37.484826 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:37.484826 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Profit.3c5f35db12: ROLLBACK
[0m05:48:37.485936 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:37.486193 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Profit.3c5f35db12: Close
[0m05:48:38.384445 [info ] [Thread-1 (]: 20 of 25 PASS not_null_dim_profit_Profit ....................................... [[32mPASS[0m in 2.93s]
[0m05:48:38.387389 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:48:38.388390 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:48:38.389387 [info ] [Thread-1 (]: 21 of 25 START test not_null_dim_profit_Sales .................................. [RUN]
[0m05:48:38.390384 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m05:48:38.391380 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:48:38.391766 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:48:38.395949 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m05:48:38.397401 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:38.397898 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:48:38.400067 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m05:48:38.400921 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:38.401014 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m05:48:38.401014 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Sales.05c16c548a: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Sales.05c16c548a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Sales
from finance.dim_profit
where Sales is null



      
    ) dbt_internal_test
[0m05:48:38.401468 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:40.401075 [debug] [Thread-1 (]: SQL status: OK in 2.0 seconds
[0m05:48:40.406055 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:40.407050 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Sales.05c16c548a: ROLLBACK
[0m05:48:40.407050 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:40.407775 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Sales.05c16c548a: Close
[0m05:48:41.291358 [info ] [Thread-1 (]: 21 of 25 PASS not_null_dim_profit_Sales ........................................ [[32mPASS[0m in 2.90s]
[0m05:48:41.293352 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:48:41.293840 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:48:41.294185 [info ] [Thread-1 (]: 22 of 25 START test not_null_dim_unit_Actual ................................... [RUN]
[0m05:48:41.295042 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Actual.7f6f830652"
[0m05:48:41.296171 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:48:41.296171 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:48:41.301094 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Actual.7f6f830652"
[0m05:48:41.302071 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:41.302570 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:48:41.306215 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Actual.7f6f830652"
[0m05:48:41.307211 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:41.307595 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Actual.7f6f830652"
[0m05:48:41.307799 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Actual.7f6f830652: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Actual.7f6f830652"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_unit
where Actual is null



      
    ) dbt_internal_test
[0m05:48:41.308001 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:43.485163 [debug] [Thread-1 (]: SQL status: OK in 2.18 seconds
[0m05:48:43.489756 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:43.490753 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Actual.7f6f830652: ROLLBACK
[0m05:48:43.490753 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:43.491596 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Actual.7f6f830652: Close
[0m05:48:44.370842 [info ] [Thread-1 (]: 22 of 25 PASS not_null_dim_unit_Actual ......................................... [[32mPASS[0m in 3.08s]
[0m05:48:44.372761 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:48:44.373761 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:48:44.373761 [info ] [Thread-1 (]: 23 of 25 START test not_null_dim_unit_Target ................................... [RUN]
[0m05:48:44.376452 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m05:48:44.376452 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:48:44.376452 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:48:44.380508 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m05:48:44.382187 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:44.382658 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:48:44.385204 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m05:48:44.386469 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:44.386469 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m05:48:44.387268 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Target.33e9c88d57: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Target.33e9c88d57"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Target
from finance.dim_unit
where Target is null



      
    ) dbt_internal_test
[0m05:48:44.387697 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:46.382098 [debug] [Thread-1 (]: SQL status: OK in 1.99 seconds
[0m05:48:46.386006 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:46.387004 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Target.33e9c88d57: ROLLBACK
[0m05:48:46.387755 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:46.387755 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Target.33e9c88d57: Close
[0m05:48:47.301266 [info ] [Thread-1 (]: 23 of 25 PASS not_null_dim_unit_Target ......................................... [[32mPASS[0m in 2.93s]
[0m05:48:47.304234 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:48:47.305237 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:48:47.305598 [info ] [Thread-1 (]: 24 of 25 START test not_null_dim_unit_Unit_Price ............................... [RUN]
[0m05:48:47.306635 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m05:48:47.307678 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:48:47.307678 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:48:47.312670 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m05:48:47.313807 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:47.314014 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:48:47.315719 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m05:48:47.316988 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:47.317214 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m05:48:47.317214 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Unit_Price.912d40d17b: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Unit_Price
from finance.dim_unit
where Unit_Price is null



      
    ) dbt_internal_test
[0m05:48:47.317214 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:49.340842 [debug] [Thread-1 (]: SQL status: OK in 2.02 seconds
[0m05:48:49.345914 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:49.346823 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Unit_Price.912d40d17b: ROLLBACK
[0m05:48:49.346823 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:49.346823 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Unit_Price.912d40d17b: Close
[0m05:48:50.239273 [info ] [Thread-1 (]: 24 of 25 PASS not_null_dim_unit_Unit_Price ..................................... [[32mPASS[0m in 2.93s]
[0m05:48:50.241187 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:48:50.242182 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:48:50.242182 [info ] [Thread-1 (]: 25 of 25 START test not_null_dim_unit_Units_Sold ............................... [RUN]
[0m05:48:50.244038 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m05:48:50.244038 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:48:50.244507 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:48:50.248535 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m05:48:50.248535 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:50.249532 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:48:50.251362 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m05:48:50.252373 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:50.253359 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m05:48:50.253359 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Units_Sold.fd5939c596: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Units_Sold
from finance.dim_unit
where Units_Sold is null



      
    ) dbt_internal_test
[0m05:48:50.253853 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:48:52.234512 [debug] [Thread-1 (]: SQL status: OK in 1.98 seconds
[0m05:48:52.237599 [debug] [Thread-1 (]: finished collecting timing info
[0m05:48:52.237599 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Units_Sold.fd5939c596: ROLLBACK
[0m05:48:52.238516 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:48:52.238516 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Units_Sold.fd5939c596: Close
[0m05:48:53.121819 [info ] [Thread-1 (]: 25 of 25 PASS not_null_dim_unit_Units_Sold ..................................... [[32mPASS[0m in 2.88s]
[0m05:48:53.123543 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:48:53.124573 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:48:53.124573 [debug] [MainThread]: On master: ROLLBACK
[0m05:48:53.124573 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:48:54.041425 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:48:54.041425 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:54.041425 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:48:54.042346 [debug] [MainThread]: On master: ROLLBACK
[0m05:48:54.042346 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:48:54.043010 [debug] [MainThread]: On master: Close
[0m05:48:54.929934 [info ] [MainThread]: 
[0m05:48:54.930939 [info ] [MainThread]: Finished running 25 tests in 0 hours 1 minutes and 22.58 seconds (82.58s).
[0m05:48:54.932771 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:48:54.933940 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m05:48:54.934302 [debug] [MainThread]: Connection 'test.finance.not_null_dim_unit_Units_Sold.fd5939c596' was properly closed.
[0m05:48:54.946374 [info ] [MainThread]: 
[0m05:48:54.946374 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m05:48:54.948195 [info ] [MainThread]: 
[0m05:48:54.948508 [error] [MainThread]: [33mRuntime Error in test not_null_dim_profit_Actual (models\schema.yml)[0m
[0m05:48:54.949566 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Actual` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`finance`.`dim_profit`.`Sales`, `spark_catalog`.`finance`.`dim_profit`.`Profit`, `spark_catalog`.`finance`.`dim_profit`.`Country`, `spark_catalog`.`finance`.`dim_profit`.`Product`, `spark_catalog`.`finance`.`dim_profit`.`Discounts`]; line 15 pos 6
[0m05:48:54.951030 [info ] [MainThread]: 
[0m05:48:54.953771 [info ] [MainThread]: Done. PASS=24 WARN=0 ERROR=1 SKIP=0 TOTAL=25
[0m05:48:54.955508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025AA29871F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025AA2B745E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025AA2B74790>]}
[0m05:48:54.956518 [debug] [MainThread]: Flushing usage events


============================== 2022-11-28 05:51:23.776270 | 015ccdeb-8af9-46ce-9ede-c7cee8527bf1 ==============================
[0m05:51:23.776270 [info ] [MainThread]: Running with dbt=1.3.1
[0m05:51:23.776270 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'which': 'test', 'rpc_method': 'test'}
[0m05:51:23.776270 [debug] [MainThread]: Tracking: tracking
[0m05:51:23.786148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017019139C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017019139870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017019139AB0>]}
[0m05:51:23.855936 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:51:23.856711 [debug] [MainThread]: Partial parsing: updated file: finance://models\dim_profit.sql
[0m05:51:23.861314 [debug] [MainThread]: 1699: static parser successfully parsed dim_profit.sql
[0m05:51:23.942692 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.finance.example

[0m05:51:23.952476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '015ccdeb-8af9-46ce-9ede-c7cee8527bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001701A325C60>]}
[0m05:51:23.964293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '015ccdeb-8af9-46ce-9ede-c7cee8527bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000170192A7520>]}
[0m05:51:23.964293 [info ] [MainThread]: Found 6 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m05:51:23.964814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '015ccdeb-8af9-46ce-9ede-c7cee8527bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000170192A7640>]}
[0m05:51:23.966671 [info ] [MainThread]: 
[0m05:51:23.966671 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:51:23.971321 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m05:51:23.976326 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m05:51:23.976326 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m05:51:23.980674 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m05:51:23.980838 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:51:26.932078 [debug] [ThreadPool]: SQL status: OK in 2.95 seconds
[0m05:51:26.941327 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m05:51:26.941327 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m05:51:26.941327 [debug] [ThreadPool]: On list_None_finance: Close
[0m05:51:28.021317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '015ccdeb-8af9-46ce-9ede-c7cee8527bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000170192D42B0>]}
[0m05:51:28.021317 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:51:28.021317 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:51:28.031182 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m05:51:28.032799 [info ] [MainThread]: 
[0m05:51:28.041701 [debug] [Thread-1 (]: Began running node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:51:28.041701 [info ] [Thread-1 (]: 1 of 25 START test accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business  [RUN]
[0m05:51:28.041701 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m05:51:28.041701 [debug] [Thread-1 (]: Began compiling node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:51:28.047565 [debug] [Thread-1 (]: Compiling test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:51:28.056282 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m05:51:28.060349 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:28.060349 [debug] [Thread-1 (]: Began executing node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:51:28.071547 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m05:51:28.071547 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:51:28.071547 [debug] [Thread-1 (]: Using databricks connection "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m05:51:28.071547 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        Segment as value_field,
        count(*) as n_records

    from finance.dim_info
    group by Segment

)

select *
from all_values
where value_field not in (
    'Government','Channel Partners','Midmarket','Enterprise','Small Business'
)



      
    ) dbt_internal_test
[0m05:51:28.075621 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m05:51:30.501269 [debug] [Thread-1 (]: SQL status: OK in 2.43 seconds
[0m05:51:30.507322 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:30.507322 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: ROLLBACK
[0m05:51:30.507322 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:51:30.507322 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: Close
[0m05:51:31.391694 [info ] [Thread-1 (]: 1 of 25 PASS accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business  [[32mPASS[0m in 3.35s]
[0m05:51:31.401315 [debug] [Thread-1 (]: Finished running node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:51:31.401315 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:51:31.405732 [info ] [Thread-1 (]: 2 of 25 START test not_null_dim_exp_Actual ..................................... [RUN]
[0m05:51:31.406254 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Actual.1d45aa27dd"
[0m05:51:31.406254 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:51:31.408674 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:51:31.411480 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Actual.1d45aa27dd"
[0m05:51:31.415898 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:31.416582 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:51:31.417853 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Actual.1d45aa27dd"
[0m05:51:31.420215 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:51:31.420215 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Actual.1d45aa27dd"
[0m05:51:31.420732 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Actual.1d45aa27dd: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Actual.1d45aa27dd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_exp
where Actual is null



      
    ) dbt_internal_test
[0m05:51:31.420873 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:51:33.413555 [debug] [Thread-1 (]: SQL status: OK in 1.99 seconds
[0m05:51:33.413555 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:33.413555 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Actual.1d45aa27dd: ROLLBACK
[0m05:51:33.419134 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:51:33.419425 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Actual.1d45aa27dd: Close
[0m05:51:34.333905 [info ] [Thread-1 (]: 2 of 25 PASS not_null_dim_exp_Actual ........................................... [[32mPASS[0m in 2.93s]
[0m05:51:34.333905 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:51:34.333905 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:51:34.333905 [info ] [Thread-1 (]: 3 of 25 START test not_null_dim_exp_Expenses ................................... [RUN]
[0m05:51:34.341324 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m05:51:34.341324 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:51:34.341324 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:51:34.341324 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m05:51:34.348360 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:34.348456 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:51:34.349505 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m05:51:34.352152 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:51:34.352152 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m05:51:34.352733 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Expenses.c94546fc2a: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Expenses.c94546fc2a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Expenses
from finance.dim_exp
where Expenses is null



      
    ) dbt_internal_test
[0m05:51:34.352917 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:51:36.332559 [debug] [Thread-1 (]: SQL status: OK in 1.98 seconds
[0m05:51:36.346267 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:36.346267 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Expenses.c94546fc2a: ROLLBACK
[0m05:51:36.347951 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:51:36.347951 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Expenses.c94546fc2a: Close
[0m05:51:37.251448 [info ] [Thread-1 (]: 3 of 25 PASS not_null_dim_exp_Expenses ......................................... [[32mPASS[0m in 2.91s]
[0m05:51:37.251448 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:51:37.251448 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:51:37.260019 [info ] [Thread-1 (]: 4 of 25 START test not_null_dim_exp_Gross_sales ................................ [RUN]
[0m05:51:37.261477 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m05:51:37.261477 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:51:37.261477 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:51:37.263712 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m05:51:37.263712 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:37.263712 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:51:37.271371 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m05:51:37.271371 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:51:37.272390 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m05:51:37.272795 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Gross_sales.07439d5e88: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Gross_sales
from finance.dim_exp
where Gross_sales is null



      
    ) dbt_internal_test
[0m05:51:37.272795 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:51:39.248896 [debug] [Thread-1 (]: SQL status: OK in 1.98 seconds
[0m05:51:39.251402 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:39.251402 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Gross_sales.07439d5e88: ROLLBACK
[0m05:51:39.253255 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:51:39.253589 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Gross_sales.07439d5e88: Close
[0m05:51:40.131434 [info ] [Thread-1 (]: 4 of 25 PASS not_null_dim_exp_Gross_sales ...................................... [[32mPASS[0m in 2.87s]
[0m05:51:40.131434 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:51:40.141182 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:51:40.141764 [info ] [Thread-1 (]: 5 of 25 START test not_null_dim_exp_Manufacturing_price ........................ [RUN]
[0m05:51:40.141764 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m05:51:40.141764 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:51:40.141764 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:51:40.141764 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m05:51:40.149461 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:40.149671 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:51:40.151271 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m05:51:40.151271 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:51:40.152451 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m05:51:40.152822 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Manufacturing_price
from finance.dim_exp
where Manufacturing_price is null



      
    ) dbt_internal_test
[0m05:51:40.153065 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:51:42.168430 [debug] [Thread-1 (]: SQL status: OK in 2.02 seconds
[0m05:51:42.171272 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:42.171272 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f: ROLLBACK
[0m05:51:42.171272 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:51:42.171272 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f: Close
[0m05:51:43.061694 [info ] [Thread-1 (]: 5 of 25 PASS not_null_dim_exp_Manufacturing_price .............................. [[32mPASS[0m in 2.92s]
[0m05:51:43.071196 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:51:43.071196 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:51:43.071196 [info ] [Thread-1 (]: 6 of 25 START test not_null_dim_info_Actual .................................... [RUN]
[0m05:51:43.074288 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m05:51:43.074288 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:51:43.076124 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:51:43.081558 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m05:51:43.082613 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:43.082879 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:51:43.084586 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m05:51:43.085445 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:51:43.085445 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m05:51:43.085941 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Actual.5cb2ffbb95"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_info
where Actual is null



      
    ) dbt_internal_test
[0m05:51:43.086151 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:51:45.181175 [debug] [Thread-1 (]: SQL status: OK in 2.1 seconds
[0m05:51:45.181175 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:45.181175 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: ROLLBACK
[0m05:51:45.181175 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:51:45.189579 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: Close
[0m05:51:46.172601 [info ] [Thread-1 (]: 6 of 25 PASS not_null_dim_info_Actual .......................................... [[32mPASS[0m in 3.10s]
[0m05:51:46.172601 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:51:46.172601 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:51:46.172601 [info ] [Thread-1 (]: 7 of 25 START test not_null_dim_info_COGS ...................................... [RUN]
[0m05:51:46.172601 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m05:51:46.172601 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:51:46.172601 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:51:46.172601 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m05:51:46.181179 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:46.181357 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:51:46.181715 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m05:51:46.181715 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:51:46.181715 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m05:51:46.184465 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_COGS.efb9b66052"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select COGS
from finance.dim_info
where COGS is null



      
    ) dbt_internal_test
[0m05:51:46.184556 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:51:48.211844 [debug] [Thread-1 (]: SQL status: OK in 2.03 seconds
[0m05:51:48.211844 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:48.221357 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: ROLLBACK
[0m05:51:48.221357 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:51:48.221357 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: Close
[0m05:51:49.121421 [info ] [Thread-1 (]: 7 of 25 PASS not_null_dim_info_COGS ............................................ [[32mPASS[0m in 2.95s]
[0m05:51:49.121421 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:51:49.131253 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Inventory.45016db171
[0m05:51:49.131253 [info ] [Thread-1 (]: 8 of 25 START test not_null_dim_info_Inventory ................................. [RUN]
[0m05:51:49.131253 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Inventory.45016db171"
[0m05:51:49.131253 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Inventory.45016db171
[0m05:51:49.131253 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Inventory.45016db171
[0m05:51:49.141440 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Inventory.45016db171"
[0m05:51:49.145078 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:49.145078 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Inventory.45016db171
[0m05:51:49.145767 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Inventory.45016db171"
[0m05:51:49.145767 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:51:49.145767 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Inventory.45016db171"
[0m05:51:49.145767 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Inventory.45016db171"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Inventory
from finance.dim_info
where Inventory is null



      
    ) dbt_internal_test
[0m05:51:49.145767 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:51:51.151327 [debug] [Thread-1 (]: SQL status: OK in 2.01 seconds
[0m05:51:51.151327 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:51.151327 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: ROLLBACK
[0m05:51:51.151327 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:51:51.151327 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: Close
[0m05:51:52.061258 [info ] [Thread-1 (]: 8 of 25 PASS not_null_dim_info_Inventory ....................................... [[32mPASS[0m in 2.93s]
[0m05:51:52.061258 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Inventory.45016db171
[0m05:51:52.061258 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:51:52.068188 [info ] [Thread-1 (]: 9 of 25 START test not_null_dim_info_Segment ................................... [RUN]
[0m05:51:52.068838 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m05:51:52.069890 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:51:52.069890 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:51:52.071320 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m05:51:52.071320 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:52.071320 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:51:52.071320 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m05:51:52.071320 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:51:52.071320 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m05:51:52.071320 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Segment.57cc6a2d98"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Segment
from finance.dim_info
where Segment is null



      
    ) dbt_internal_test
[0m05:51:52.078442 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:51:54.061248 [debug] [Thread-1 (]: SQL status: OK in 1.98 seconds
[0m05:51:54.068219 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:54.068219 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: ROLLBACK
[0m05:51:54.068219 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:51:54.071095 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: Close
[0m05:51:54.951589 [info ] [Thread-1 (]: 9 of 25 PASS not_null_dim_info_Segment ......................................... [[32mPASS[0m in 2.88s]
[0m05:51:54.951589 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:51:54.951589 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:51:54.951589 [info ] [Thread-1 (]: 10 of 25 START test not_null_dim_month_Actual .................................. [RUN]
[0m05:51:54.961363 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Actual.9fdc4e57c7"
[0m05:51:54.961363 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:51:54.961363 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:51:54.961363 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Actual.9fdc4e57c7"
[0m05:51:54.961363 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:54.961363 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:51:54.961363 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Actual.9fdc4e57c7"
[0m05:51:54.961363 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:51:54.961363 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Actual.9fdc4e57c7"
[0m05:51:54.970555 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Actual.9fdc4e57c7: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Actual.9fdc4e57c7"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_month
where Actual is null



      
    ) dbt_internal_test
[0m05:51:54.970675 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:51:56.981992 [debug] [Thread-1 (]: SQL status: OK in 2.01 seconds
[0m05:51:56.981992 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:56.981992 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Actual.9fdc4e57c7: ROLLBACK
[0m05:51:56.991457 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:51:56.991457 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Actual.9fdc4e57c7: Close
[0m05:51:57.882140 [info ] [Thread-1 (]: 10 of 25 PASS not_null_dim_month_Actual ........................................ [[32mPASS[0m in 2.92s]
[0m05:51:57.882140 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:51:57.882140 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:51:57.890969 [info ] [Thread-1 (]: 11 of 25 START test not_null_dim_month_Date .................................... [RUN]
[0m05:51:57.891349 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m05:51:57.891349 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:51:57.892466 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:51:57.896334 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m05:51:57.896334 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:57.896334 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:51:57.898920 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m05:51:57.898920 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:51:57.901434 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m05:51:57.901434 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Date.ce09cea79f: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Date.ce09cea79f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Date
from finance.dim_month
where Date is null



      
    ) dbt_internal_test
[0m05:51:57.901434 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:51:59.901395 [debug] [Thread-1 (]: SQL status: OK in 2.0 seconds
[0m05:51:59.901395 [debug] [Thread-1 (]: finished collecting timing info
[0m05:51:59.901395 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Date.ce09cea79f: ROLLBACK
[0m05:51:59.901395 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:51:59.910559 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Date.ce09cea79f: Close
[0m05:52:00.791137 [info ] [Thread-1 (]: 11 of 25 PASS not_null_dim_month_Date .......................................... [[32mPASS[0m in 2.90s]
[0m05:52:00.791137 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:52:00.798879 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:52:00.799609 [info ] [Thread-1 (]: 12 of 25 START test not_null_dim_month_MonthName ............................... [RUN]
[0m05:52:00.801165 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m05:52:00.801165 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:52:00.801165 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:52:00.802848 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m05:52:00.802848 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:00.802848 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:52:00.811475 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m05:52:00.811475 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:52:00.811475 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m05:52:00.811475 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_MonthName.c1d116d50b: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_MonthName.c1d116d50b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select MonthName
from finance.dim_month
where MonthName is null



      
    ) dbt_internal_test
[0m05:52:00.812907 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:52:02.855446 [debug] [Thread-1 (]: SQL status: OK in 2.04 seconds
[0m05:52:02.861557 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:02.861557 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_MonthName.c1d116d50b: ROLLBACK
[0m05:52:02.861557 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:52:02.861557 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_MonthName.c1d116d50b: Close
[0m05:52:03.771377 [info ] [Thread-1 (]: 12 of 25 PASS not_null_dim_month_MonthName ..................................... [[32mPASS[0m in 2.97s]
[0m05:52:03.773053 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:52:03.773053 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:52:03.775257 [info ] [Thread-1 (]: 13 of 25 START test not_null_dim_month_Month_Number ............................ [RUN]
[0m05:52:03.776080 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m05:52:03.776080 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:52:03.776080 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:52:03.781215 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m05:52:03.781215 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:03.781215 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:52:03.784294 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m05:52:03.784294 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:52:03.784294 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m05:52:03.784294 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Month_Number.be337b93c3: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Month_Number.be337b93c3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Month_Number
from finance.dim_month
where Month_Number is null



      
    ) dbt_internal_test
[0m05:52:03.784294 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:52:05.841618 [debug] [Thread-1 (]: SQL status: OK in 2.06 seconds
[0m05:52:05.850989 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:05.850989 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Month_Number.be337b93c3: ROLLBACK
[0m05:52:05.850989 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:52:05.850989 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Month_Number.be337b93c3: Close
[0m05:52:06.751581 [info ] [Thread-1 (]: 13 of 25 PASS not_null_dim_month_Month_Number .................................. [[32mPASS[0m in 2.98s]
[0m05:52:06.751581 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:52:06.756362 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:52:06.756362 [info ] [Thread-1 (]: 14 of 25 START test not_null_dim_month_Year .................................... [RUN]
[0m05:52:06.760125 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m05:52:06.760256 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:52:06.761260 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:52:06.766289 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m05:52:06.766289 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:06.766289 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:52:06.766289 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m05:52:06.771313 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:52:06.771313 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m05:52:06.771313 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Year.5d11bba8bb: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Year.5d11bba8bb"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Year
from finance.dim_month
where Year is null



      
    ) dbt_internal_test
[0m05:52:06.772606 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:52:08.761749 [debug] [Thread-1 (]: SQL status: OK in 1.99 seconds
[0m05:52:08.761749 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:08.761749 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Year.5d11bba8bb: ROLLBACK
[0m05:52:08.761749 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:52:08.761749 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Year.5d11bba8bb: Close
[0m05:52:09.671234 [info ] [Thread-1 (]: 14 of 25 PASS not_null_dim_month_Year .......................................... [[32mPASS[0m in 2.91s]
[0m05:52:09.671234 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:52:09.671234 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Actual.6279364421
[0m05:52:09.679388 [info ] [Thread-1 (]: 15 of 25 START test not_null_dim_profit_Actual ................................. [RUN]
[0m05:52:09.680218 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Actual.6279364421"
[0m05:52:09.681381 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Actual.6279364421
[0m05:52:09.681381 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Actual.6279364421
[0m05:52:09.682421 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Actual.6279364421"
[0m05:52:09.686808 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:09.686808 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Actual.6279364421
[0m05:52:09.687983 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Actual.6279364421"
[0m05:52:09.690082 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:52:09.690252 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Actual.6279364421"
[0m05:52:09.690469 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Actual.6279364421: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Actual.6279364421"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_profit
where Actual is null



      
    ) dbt_internal_test
[0m05:52:09.690469 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:52:11.562297 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Actual.6279364421"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_profit
where Actual is null



      
    ) dbt_internal_test
[0m05:52:11.563462 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Actual` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`finance`.`dim_profit`.`Sales`, `spark_catalog`.`finance`.`dim_profit`.`Profit`, `spark_catalog`.`finance`.`dim_profit`.`Country`, `spark_catalog`.`finance`.`dim_profit`.`Product`, `spark_catalog`.`finance`.`dim_profit`.`Discounts`]; line 15 pos 6
[0m05:52:11.564559 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Actual` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`finance`.`dim_profit`.`Sales`, `spark_catalog`.`finance`.`dim_profit`.`Profit`, `spark_catalog`.`finance`.`dim_profit`.`Country`, `spark_catalog`.`finance`.`dim_profit`.`Product`, `spark_catalog`.`finance`.`dim_profit`.`Discounts`]; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Actual` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`finance`.`dim_profit`.`Sales`, `spark_catalog`.`finance`.`dim_profit`.`Profit`, `spark_catalog`.`finance`.`dim_profit`.`Country`, `spark_catalog`.`finance`.`dim_profit`.`Product`, `spark_catalog`.`finance`.`dim_profit`.`Discounts`]; line 15 pos 6
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m05:52:11.564559 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\x9flx\n\xca\x04M\x8d\xa9GF\xb7\xda\x87\xb9\xa9'
[0m05:52:11.564559 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:11.566837 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Actual.6279364421: ROLLBACK
[0m05:52:11.566837 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:52:11.566837 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Actual.6279364421: Close
[0m05:52:12.471115 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_profit_Actual (models\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Actual` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`finance`.`dim_profit`.`Sales`, `spark_catalog`.`finance`.`dim_profit`.`Profit`, `spark_catalog`.`finance`.`dim_profit`.`Country`, `spark_catalog`.`finance`.`dim_profit`.`Product`, `spark_catalog`.`finance`.`dim_profit`.`Discounts`]; line 15 pos 6
[0m05:52:12.471115 [error] [Thread-1 (]: 15 of 25 ERROR not_null_dim_profit_Actual ...................................... [[31mERROR[0m in 2.79s]
[0m05:52:12.471115 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Actual.6279364421
[0m05:52:12.481214 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:52:12.481214 [info ] [Thread-1 (]: 16 of 25 START test not_null_dim_profit_Country ................................ [RUN]
[0m05:52:12.482541 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m05:52:12.482541 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:52:12.482541 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:52:12.482541 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m05:52:12.482541 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:12.482541 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:52:12.491080 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m05:52:12.491080 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:52:12.491080 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m05:52:12.491080 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Country.363de9fbd5: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Country.363de9fbd5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Country
from finance.dim_profit
where Country is null



      
    ) dbt_internal_test
[0m05:52:12.491080 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:52:14.491513 [debug] [Thread-1 (]: SQL status: OK in 2.0 seconds
[0m05:52:14.491513 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:14.501295 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Country.363de9fbd5: ROLLBACK
[0m05:52:14.501668 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:52:14.502358 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Country.363de9fbd5: Close
[0m05:52:15.396404 [info ] [Thread-1 (]: 16 of 25 PASS not_null_dim_profit_Country ...................................... [[32mPASS[0m in 2.91s]
[0m05:52:15.401513 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:52:15.401513 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:52:15.401513 [info ] [Thread-1 (]: 17 of 25 START test not_null_dim_profit_Discount_Band .......................... [RUN]
[0m05:52:15.401513 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m05:52:15.401513 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:52:15.401513 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:52:15.414163 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m05:52:15.414769 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:15.414769 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:52:15.416223 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m05:52:15.416223 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:52:15.416223 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m05:52:15.416223 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Discount_Band
from finance.dim_profit
where Discount_Band is null



      
    ) dbt_internal_test
[0m05:52:15.416223 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:52:17.411641 [debug] [Thread-1 (]: SQL status: OK in 2.0 seconds
[0m05:52:17.416532 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:17.417583 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa: ROLLBACK
[0m05:52:17.417583 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:52:17.417583 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa: Close
[0m05:52:18.332423 [info ] [Thread-1 (]: 17 of 25 PASS not_null_dim_profit_Discount_Band ................................ [[32mPASS[0m in 2.93s]
[0m05:52:18.332423 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:52:18.332423 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:52:18.332423 [info ] [Thread-1 (]: 18 of 25 START test not_null_dim_profit_Discounts .............................. [RUN]
[0m05:52:18.332423 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m05:52:18.332423 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:52:18.332423 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:52:18.340928 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m05:52:18.340928 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:18.341984 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:52:18.342529 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m05:52:18.345716 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:52:18.345716 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m05:52:18.345716 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discounts.03e349480e: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Discounts.03e349480e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Discounts
from finance.dim_profit
where Discounts is null



      
    ) dbt_internal_test
[0m05:52:18.345716 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:52:20.441101 [debug] [Thread-1 (]: SQL status: OK in 2.1 seconds
[0m05:52:20.441101 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:20.441101 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discounts.03e349480e: ROLLBACK
[0m05:52:20.441101 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:52:20.441101 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discounts.03e349480e: Close
[0m05:52:21.401601 [info ] [Thread-1 (]: 18 of 25 PASS not_null_dim_profit_Discounts .................................... [[32mPASS[0m in 3.07s]
[0m05:52:21.401601 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:52:21.401601 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Product.1422e74993
[0m05:52:21.408639 [info ] [Thread-1 (]: 19 of 25 START test not_null_dim_profit_Product ................................ [RUN]
[0m05:52:21.408839 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Product.1422e74993"
[0m05:52:21.408839 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Product.1422e74993
[0m05:52:21.411113 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Product.1422e74993
[0m05:52:21.411113 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Product.1422e74993"
[0m05:52:21.416439 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:21.416439 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Product.1422e74993
[0m05:52:21.417647 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Product.1422e74993"
[0m05:52:21.417647 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:52:21.417647 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Product.1422e74993"
[0m05:52:21.417647 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Product.1422e74993: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Product.1422e74993"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Product
from finance.dim_profit
where Product is null



      
    ) dbt_internal_test
[0m05:52:21.420208 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:52:23.391387 [debug] [Thread-1 (]: SQL status: OK in 1.97 seconds
[0m05:52:23.401270 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:23.401270 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Product.1422e74993: ROLLBACK
[0m05:52:23.401270 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:52:23.401270 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Product.1422e74993: Close
[0m05:52:24.379998 [info ] [Thread-1 (]: 19 of 25 PASS not_null_dim_profit_Product ...................................... [[32mPASS[0m in 2.97s]
[0m05:52:24.382026 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Product.1422e74993
[0m05:52:24.383427 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:52:24.383427 [info ] [Thread-1 (]: 20 of 25 START test not_null_dim_profit_Profit ................................. [RUN]
[0m05:52:24.385494 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m05:52:24.385494 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:52:24.385494 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:52:24.389791 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m05:52:24.391396 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:24.391396 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:52:24.396019 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m05:52:24.397121 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:52:24.397121 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m05:52:24.397121 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Profit.3c5f35db12: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Profit.3c5f35db12"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Profit
from finance.dim_profit
where Profit is null



      
    ) dbt_internal_test
[0m05:52:24.398187 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:52:26.411163 [debug] [Thread-1 (]: SQL status: OK in 2.01 seconds
[0m05:52:26.422250 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:26.422250 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Profit.3c5f35db12: ROLLBACK
[0m05:52:26.422250 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:52:26.422250 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Profit.3c5f35db12: Close
[0m05:52:27.351609 [info ] [Thread-1 (]: 20 of 25 PASS not_null_dim_profit_Profit ....................................... [[32mPASS[0m in 2.97s]
[0m05:52:27.351609 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:52:27.361350 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:52:27.361350 [info ] [Thread-1 (]: 21 of 25 START test not_null_dim_profit_Sales .................................. [RUN]
[0m05:52:27.363737 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m05:52:27.363737 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:52:27.363737 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:52:27.363737 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m05:52:27.370312 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:27.370312 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:52:27.371858 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m05:52:27.371858 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:52:27.371858 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m05:52:27.371858 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Sales.05c16c548a: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Sales.05c16c548a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Sales
from finance.dim_profit
where Sales is null



      
    ) dbt_internal_test
[0m05:52:27.371858 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:52:29.496075 [debug] [Thread-1 (]: SQL status: OK in 2.12 seconds
[0m05:52:29.496280 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:29.496280 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Sales.05c16c548a: ROLLBACK
[0m05:52:29.496280 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:52:29.496280 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Sales.05c16c548a: Close
[0m05:52:30.401581 [info ] [Thread-1 (]: 21 of 25 PASS not_null_dim_profit_Sales ........................................ [[32mPASS[0m in 3.04s]
[0m05:52:30.401581 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:52:30.401581 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:52:30.406648 [info ] [Thread-1 (]: 22 of 25 START test not_null_dim_unit_Actual ................................... [RUN]
[0m05:52:30.407493 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Actual.7f6f830652"
[0m05:52:30.407493 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:52:30.408960 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:52:30.411039 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Actual.7f6f830652"
[0m05:52:30.411039 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:30.411039 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:52:30.411039 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Actual.7f6f830652"
[0m05:52:30.411039 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:52:30.411039 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Actual.7f6f830652"
[0m05:52:30.411039 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Actual.7f6f830652: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Actual.7f6f830652"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_unit
where Actual is null



      
    ) dbt_internal_test
[0m05:52:30.411039 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:52:32.441433 [debug] [Thread-1 (]: SQL status: OK in 2.03 seconds
[0m05:52:32.451169 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:32.451169 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Actual.7f6f830652: ROLLBACK
[0m05:52:32.452992 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:52:32.452992 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Actual.7f6f830652: Close
[0m05:52:33.601580 [info ] [Thread-1 (]: 22 of 25 PASS not_null_dim_unit_Actual ......................................... [[32mPASS[0m in 3.19s]
[0m05:52:33.601580 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:52:33.601580 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:52:33.607067 [info ] [Thread-1 (]: 23 of 25 START test not_null_dim_unit_Target ................................... [RUN]
[0m05:52:33.607546 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m05:52:33.607546 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:52:33.607546 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:52:33.611153 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m05:52:33.614076 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:33.614076 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:52:33.614735 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m05:52:33.614735 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:52:33.614735 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m05:52:33.617626 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Target.33e9c88d57: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Target.33e9c88d57"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Target
from finance.dim_unit
where Target is null



      
    ) dbt_internal_test
[0m05:52:33.617626 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:52:35.721385 [debug] [Thread-1 (]: SQL status: OK in 2.1 seconds
[0m05:52:35.721385 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:35.721385 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Target.33e9c88d57: ROLLBACK
[0m05:52:35.730350 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:52:35.730701 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Target.33e9c88d57: Close
[0m05:52:36.631357 [info ] [Thread-1 (]: 23 of 25 PASS not_null_dim_unit_Target ......................................... [[32mPASS[0m in 3.02s]
[0m05:52:36.631357 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:52:36.631357 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:52:36.637844 [info ] [Thread-1 (]: 24 of 25 START test not_null_dim_unit_Unit_Price ............................... [RUN]
[0m05:52:36.638943 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m05:52:36.638943 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:52:36.640116 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:52:36.641366 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m05:52:36.641366 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:36.645545 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:52:36.646227 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m05:52:36.649645 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:52:36.649645 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m05:52:36.650110 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Unit_Price.912d40d17b: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Unit_Price
from finance.dim_unit
where Unit_Price is null



      
    ) dbt_internal_test
[0m05:52:36.650110 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:52:38.651221 [debug] [Thread-1 (]: SQL status: OK in 2.0 seconds
[0m05:52:38.661200 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:38.661200 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Unit_Price.912d40d17b: ROLLBACK
[0m05:52:38.664427 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:52:38.664427 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Unit_Price.912d40d17b: Close
[0m05:52:39.551224 [info ] [Thread-1 (]: 24 of 25 PASS not_null_dim_unit_Unit_Price ..................................... [[32mPASS[0m in 2.91s]
[0m05:52:39.551224 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:52:39.551224 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:52:39.561991 [info ] [Thread-1 (]: 25 of 25 START test not_null_dim_unit_Units_Sold ............................... [RUN]
[0m05:52:39.562957 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m05:52:39.562957 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:52:39.562957 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:52:39.565352 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m05:52:39.571077 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:39.571077 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:52:39.571758 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m05:52:39.571758 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:52:39.571758 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m05:52:39.571758 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Units_Sold.fd5939c596: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Units_Sold
from finance.dim_unit
where Units_Sold is null



      
    ) dbt_internal_test
[0m05:52:39.575023 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:52:41.561319 [debug] [Thread-1 (]: SQL status: OK in 1.99 seconds
[0m05:52:41.561319 [debug] [Thread-1 (]: finished collecting timing info
[0m05:52:41.571101 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Units_Sold.fd5939c596: ROLLBACK
[0m05:52:41.571101 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:52:41.571101 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Units_Sold.fd5939c596: Close
[0m05:52:42.452257 [info ] [Thread-1 (]: 25 of 25 PASS not_null_dim_unit_Units_Sold ..................................... [[32mPASS[0m in 2.89s]
[0m05:52:42.452257 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:52:42.457580 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:52:42.458636 [error] [MainThread]: CANCEL query list_None_finance ................................................. [[31mCANCEL[0m]
[0m05:52:42.459393 [error] [MainThread]: CANCEL query test.finance.not_null_dim_unit_Units_Sold.fd5939c596 .............. [[31mCANCEL[0m]
[0m05:52:42.460405 [info ] [MainThread]: 
[0m05:52:42.460673 [info ] [MainThread]: [33mExited because of keyboard interrupt.[0m
[0m05:52:42.461639 [info ] [MainThread]: 
[0m05:52:42.462245 [error] [MainThread]: [33mRuntime Error in test not_null_dim_profit_Actual (models\schema.yml)[0m
[0m05:52:42.463052 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Actual` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`finance`.`dim_profit`.`Sales`, `spark_catalog`.`finance`.`dim_profit`.`Profit`, `spark_catalog`.`finance`.`dim_profit`.`Country`, `spark_catalog`.`finance`.`dim_profit`.`Product`, `spark_catalog`.`finance`.`dim_profit`.`Discounts`]; line 15 pos 6
[0m05:52:42.464293 [info ] [MainThread]: 
[0m05:52:42.464961 [info ] [MainThread]: Done. PASS=24 WARN=0 ERROR=1 SKIP=0 TOTAL=25
[0m05:52:42.466253 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:52:42.466674 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m05:52:42.466981 [debug] [MainThread]: Connection 'test.finance.not_null_dim_unit_Units_Sold.fd5939c596' was properly closed.
[0m05:52:42.467192 [debug] [MainThread]: Flushing usage events
[0m05:52:44.132398 [info ] [MainThread]: ctrl-c


============================== 2022-11-28 05:52:57.364359 | b0c2d3c8-f0de-460d-9545-5da15631b7be ==============================
[0m05:52:57.364359 [info ] [MainThread]: Running with dbt=1.3.1
[0m05:52:57.365175 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m05:52:57.365175 [debug] [MainThread]: Tracking: tracking
[0m05:52:57.375325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B086DCDC90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B086DCD870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B086DCDAB0>]}
[0m05:52:57.450297 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m05:52:57.450297 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m05:52:57.451283 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.finance.example

[0m05:52:57.459545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b0c2d3c8-f0de-460d-9545-5da15631b7be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B087FD00D0>]}
[0m05:52:57.469455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b0c2d3c8-f0de-460d-9545-5da15631b7be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B087FA4040>]}
[0m05:52:57.469455 [info ] [MainThread]: Found 6 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m05:52:57.470555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b0c2d3c8-f0de-460d-9545-5da15631b7be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B087FA4100>]}
[0m05:52:57.471805 [info ] [MainThread]: 
[0m05:52:57.472803 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:52:57.484751 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m05:52:57.494741 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m05:52:57.495819 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m05:52:57.496208 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:52:59.505373 [debug] [ThreadPool]: SQL status: OK in 2.01 seconds
[0m05:52:59.514111 [debug] [ThreadPool]: On list_schemas: Close
[0m05:53:00.411542 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m05:53:00.419729 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m05:53:00.419729 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m05:53:00.419729 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m05:53:00.420832 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:53:02.525590 [debug] [ThreadPool]: SQL status: OK in 2.1 seconds
[0m05:53:02.531578 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m05:53:02.531578 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m05:53:02.532502 [debug] [ThreadPool]: On list_None_finance: Close
[0m05:53:03.435949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b0c2d3c8-f0de-460d-9545-5da15631b7be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B086FABDF0>]}
[0m05:53:03.436942 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:53:03.436942 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:53:03.437994 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m05:53:03.438511 [info ] [MainThread]: 
[0m05:53:03.453287 [debug] [Thread-1 (]: Began running node model.finance.dim_exp
[0m05:53:03.453287 [info ] [Thread-1 (]: 1 of 6 START sql table model finance.dim_exp ................................... [RUN]
[0m05:53:03.454473 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_exp"
[0m05:53:03.454473 [debug] [Thread-1 (]: Began compiling node model.finance.dim_exp
[0m05:53:03.455650 [debug] [Thread-1 (]: Compiling model.finance.dim_exp
[0m05:53:03.458957 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_exp"
[0m05:53:03.458957 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:03.458957 [debug] [Thread-1 (]: Began executing node model.finance.dim_exp
[0m05:53:03.536387 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_exp"
[0m05:53:03.537305 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:53:03.537305 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_exp"
[0m05:53:03.537305 [debug] [Thread-1 (]: On model.finance.dim_exp: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_exp"} */

  
    
      create or replace table finance.dim_exp
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Expenses, Gross_sales, Manufacturing_price from data.table_finance;
  
[0m05:53:03.537305 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m05:53:08.451776 [debug] [Thread-1 (]: SQL status: OK in 4.91 seconds
[0m05:53:08.463649 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:08.463649 [debug] [Thread-1 (]: On model.finance.dim_exp: ROLLBACK
[0m05:53:08.463649 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:53:08.464576 [debug] [Thread-1 (]: On model.finance.dim_exp: Close
[0m05:53:09.406003 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0c2d3c8-f0de-460d-9545-5da15631b7be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B086DD0FA0>]}
[0m05:53:09.407000 [info ] [Thread-1 (]: 1 of 6 OK created sql table model finance.dim_exp .............................. [[32mOK[0m in 5.95s]
[0m05:53:09.409123 [debug] [Thread-1 (]: Finished running node model.finance.dim_exp
[0m05:53:09.409123 [debug] [Thread-1 (]: Began running node model.finance.dim_info
[0m05:53:09.410124 [info ] [Thread-1 (]: 2 of 6 START sql table model finance.dim_info .................................. [RUN]
[0m05:53:09.411120 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_info"
[0m05:53:09.411120 [debug] [Thread-1 (]: Began compiling node model.finance.dim_info
[0m05:53:09.412116 [debug] [Thread-1 (]: Compiling model.finance.dim_info
[0m05:53:09.415183 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_info"
[0m05:53:09.415394 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:09.415394 [debug] [Thread-1 (]: Began executing node model.finance.dim_info
[0m05:53:09.420379 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_info"
[0m05:53:09.421375 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:53:09.421375 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_info"
[0m05:53:09.421375 [debug] [Thread-1 (]: On model.finance.dim_info: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_info"} */

  
    
      create or replace table finance.dim_info
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, COGS, Inventory, Segment from data.table_finance;
  
[0m05:53:09.422007 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:53:13.818164 [debug] [Thread-1 (]: SQL status: OK in 4.4 seconds
[0m05:53:13.820157 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:13.821152 [debug] [Thread-1 (]: On model.finance.dim_info: ROLLBACK
[0m05:53:13.821399 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:53:13.821645 [debug] [Thread-1 (]: On model.finance.dim_info: Close
[0m05:53:14.725941 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0c2d3c8-f0de-460d-9545-5da15631b7be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B086DD3D30>]}
[0m05:53:14.727221 [info ] [Thread-1 (]: 2 of 6 OK created sql table model finance.dim_info ............................. [[32mOK[0m in 5.31s]
[0m05:53:14.729227 [debug] [Thread-1 (]: Finished running node model.finance.dim_info
[0m05:53:14.729227 [debug] [Thread-1 (]: Began running node model.finance.dim_month
[0m05:53:14.730229 [info ] [Thread-1 (]: 3 of 6 START sql table model finance.dim_month ................................. [RUN]
[0m05:53:14.731133 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_month"
[0m05:53:14.731133 [debug] [Thread-1 (]: Began compiling node model.finance.dim_month
[0m05:53:14.731133 [debug] [Thread-1 (]: Compiling model.finance.dim_month
[0m05:53:14.735005 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_month"
[0m05:53:14.735834 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:14.735834 [debug] [Thread-1 (]: Began executing node model.finance.dim_month
[0m05:53:14.739721 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_month"
[0m05:53:14.740712 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:53:14.740827 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_month"
[0m05:53:14.740827 [debug] [Thread-1 (]: On model.finance.dim_month: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_month"} */

  
    
      create or replace table finance.dim_month
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Month_Number, Year, MonthName, Date from data.table_finance;
  
[0m05:53:14.740827 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:53:18.697576 [debug] [Thread-1 (]: SQL status: OK in 3.96 seconds
[0m05:53:18.700575 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:18.700575 [debug] [Thread-1 (]: On model.finance.dim_month: ROLLBACK
[0m05:53:18.700575 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:53:18.701494 [debug] [Thread-1 (]: On model.finance.dim_month: Close
[0m05:53:19.605372 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0c2d3c8-f0de-460d-9545-5da15631b7be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B088066980>]}
[0m05:53:19.606416 [info ] [Thread-1 (]: 3 of 6 OK created sql table model finance.dim_month ............................ [[32mOK[0m in 4.87s]
[0m05:53:19.608674 [debug] [Thread-1 (]: Finished running node model.finance.dim_month
[0m05:53:19.608674 [debug] [Thread-1 (]: Began running node model.finance.dim_profit
[0m05:53:19.609675 [info ] [Thread-1 (]: 4 of 6 START sql table model finance.dim_profit ................................ [RUN]
[0m05:53:19.611203 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_profit"
[0m05:53:19.611203 [debug] [Thread-1 (]: Began compiling node model.finance.dim_profit
[0m05:53:19.611203 [debug] [Thread-1 (]: Compiling model.finance.dim_profit
[0m05:53:19.614731 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_profit"
[0m05:53:19.615548 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:19.615788 [debug] [Thread-1 (]: Began executing node model.finance.dim_profit
[0m05:53:19.618413 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_profit"
[0m05:53:19.619422 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:53:19.619422 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_profit"
[0m05:53:19.620333 [debug] [Thread-1 (]: On model.finance.dim_profit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_profit"} */

  
    
      create or replace table finance.dim_profit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Discounts, Profit, Sales, Country, Discount_Band, Product from data.table_finance;
  
[0m05:53:19.620676 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:53:24.153998 [debug] [Thread-1 (]: SQL status: OK in 4.53 seconds
[0m05:53:24.157913 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:24.158910 [debug] [Thread-1 (]: On model.finance.dim_profit: ROLLBACK
[0m05:53:24.158910 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:53:24.158910 [debug] [Thread-1 (]: On model.finance.dim_profit: Close
[0m05:53:25.060147 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0c2d3c8-f0de-460d-9545-5da15631b7be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B0880F0D30>]}
[0m05:53:25.061220 [info ] [Thread-1 (]: 4 of 6 OK created sql table model finance.dim_profit ........................... [[32mOK[0m in 5.45s]
[0m05:53:25.063132 [debug] [Thread-1 (]: Finished running node model.finance.dim_profit
[0m05:53:25.063407 [debug] [Thread-1 (]: Began running node model.finance.dim_unit
[0m05:53:25.063407 [info ] [Thread-1 (]: 5 of 6 START sql table model finance.dim_unit .................................. [RUN]
[0m05:53:25.064608 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_unit"
[0m05:53:25.064608 [debug] [Thread-1 (]: Began compiling node model.finance.dim_unit
[0m05:53:25.066022 [debug] [Thread-1 (]: Compiling model.finance.dim_unit
[0m05:53:25.069648 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_unit"
[0m05:53:25.070697 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:25.070795 [debug] [Thread-1 (]: Began executing node model.finance.dim_unit
[0m05:53:25.074555 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_unit"
[0m05:53:25.075980 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:53:25.075980 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_unit"
[0m05:53:25.075980 [debug] [Thread-1 (]: On model.finance.dim_unit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_unit"} */

  
    
      create or replace table finance.dim_unit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Target, Unit_Price, Units_Sold from data.table_finance;
  
[0m05:53:25.076673 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:53:28.976489 [debug] [Thread-1 (]: SQL status: OK in 3.9 seconds
[0m05:53:28.980508 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:28.980508 [debug] [Thread-1 (]: On model.finance.dim_unit: ROLLBACK
[0m05:53:28.981435 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:53:28.982100 [debug] [Thread-1 (]: On model.finance.dim_unit: Close
[0m05:53:30.040112 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0c2d3c8-f0de-460d-9545-5da15631b7be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B0880A3100>]}
[0m05:53:30.041108 [info ] [Thread-1 (]: 5 of 6 OK created sql table model finance.dim_unit ............................. [[32mOK[0m in 4.98s]
[0m05:53:30.044191 [debug] [Thread-1 (]: Finished running node model.finance.dim_unit
[0m05:53:30.045266 [debug] [Thread-1 (]: Began running node model.finance.dim_check_month
[0m05:53:30.045266 [info ] [Thread-1 (]: 6 of 6 START sql table model finance.dim_check_month ........................... [RUN]
[0m05:53:30.047157 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_check_month"
[0m05:53:30.048169 [debug] [Thread-1 (]: Began compiling node model.finance.dim_check_month
[0m05:53:30.048169 [debug] [Thread-1 (]: Compiling model.finance.dim_check_month
[0m05:53:30.052146 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_check_month"
[0m05:53:30.053103 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:30.053103 [debug] [Thread-1 (]: Began executing node model.finance.dim_check_month
[0m05:53:30.056442 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_check_month"
[0m05:53:30.057446 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:53:30.057446 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_check_month"
[0m05:53:30.057446 [debug] [Thread-1 (]: On model.finance.dim_check_month: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_check_month"} */

  
    
      create or replace table finance.dim_check_month
    
    
    using delta
    
    
    
    
    
    
    as
      

select date from finance.dim_month where date > '2018-02-01' and date < '2018-03-01';
  
[0m05:53:30.058465 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:53:35.044020 [debug] [Thread-1 (]: SQL status: OK in 4.99 seconds
[0m05:53:35.047136 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:35.048766 [debug] [Thread-1 (]: On model.finance.dim_check_month: ROLLBACK
[0m05:53:35.049246 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:53:35.049586 [debug] [Thread-1 (]: On model.finance.dim_check_month: Close
[0m05:53:36.101345 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0c2d3c8-f0de-460d-9545-5da15631b7be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B0F77A9AB0>]}
[0m05:53:36.102319 [info ] [Thread-1 (]: 6 of 6 OK created sql table model finance.dim_check_month ...................... [[32mOK[0m in 6.05s]
[0m05:53:36.103375 [debug] [Thread-1 (]: Finished running node model.finance.dim_check_month
[0m05:53:36.104330 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:53:36.104330 [debug] [MainThread]: On master: ROLLBACK
[0m05:53:36.104330 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:53:37.023227 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:53:37.023227 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:53:37.024230 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:53:37.024230 [debug] [MainThread]: On master: ROLLBACK
[0m05:53:37.024765 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:53:37.024765 [debug] [MainThread]: On master: Close
[0m05:53:37.939969 [info ] [MainThread]: 
[0m05:53:37.941196 [info ] [MainThread]: Finished running 6 table models in 0 hours 0 minutes and 40.47 seconds (40.47s).
[0m05:53:37.943379 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:53:37.944394 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m05:53:37.944394 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m05:53:37.944394 [debug] [MainThread]: Connection 'model.finance.dim_check_month' was properly closed.
[0m05:53:37.954350 [info ] [MainThread]: 
[0m05:53:37.955515 [info ] [MainThread]: [32mCompleted successfully[0m
[0m05:53:37.956602 [info ] [MainThread]: 
[0m05:53:37.956602 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m05:53:37.960224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B086F77220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B087FA4160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B087FEF250>]}
[0m05:53:37.961179 [debug] [MainThread]: Flushing usage events


============================== 2022-11-28 05:53:44.973295 | d05de448-1bb7-4c64-b88e-19e3ed14ccf7 ==============================
[0m05:53:44.973295 [info ] [MainThread]: Running with dbt=1.3.1
[0m05:53:44.974511 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'which': 'test', 'rpc_method': 'test'}
[0m05:53:44.975508 [debug] [MainThread]: Tracking: tracking
[0m05:53:44.985089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE35CDDC90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE35CDD900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE35CDF340>]}
[0m05:53:45.050688 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m05:53:45.051702 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m05:53:45.051702 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.finance.example

[0m05:53:45.058274 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd05de448-1bb7-4c64-b88e-19e3ed14ccf7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE36EE00D0>]}
[0m05:53:45.068584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd05de448-1bb7-4c64-b88e-19e3ed14ccf7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE36EB4040>]}
[0m05:53:45.069583 [info ] [MainThread]: Found 6 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m05:53:45.069583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd05de448-1bb7-4c64-b88e-19e3ed14ccf7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE36EB40D0>]}
[0m05:53:45.071705 [info ] [MainThread]: 
[0m05:53:45.072700 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:53:45.084873 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m05:53:45.093234 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m05:53:45.094350 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m05:53:45.094350 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m05:53:45.094350 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:53:47.243472 [debug] [ThreadPool]: SQL status: OK in 2.15 seconds
[0m05:53:47.253725 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m05:53:47.254856 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m05:53:47.254856 [debug] [ThreadPool]: On list_None_finance: Close
[0m05:53:48.194509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd05de448-1bb7-4c64-b88e-19e3ed14ccf7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE35EBBD90>]}
[0m05:53:48.195319 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:53:48.195779 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:53:48.196433 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m05:53:48.196927 [info ] [MainThread]: 
[0m05:53:48.201528 [debug] [Thread-1 (]: Began running node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:53:48.201788 [info ] [Thread-1 (]: 1 of 25 START test accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business  [RUN]
[0m05:53:48.202771 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m05:53:48.203115 [debug] [Thread-1 (]: Began compiling node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:53:48.203348 [debug] [Thread-1 (]: Compiling test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:53:48.215943 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m05:53:48.217686 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:48.217971 [debug] [Thread-1 (]: Began executing node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:53:48.235979 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m05:53:48.236968 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:53:48.236968 [debug] [Thread-1 (]: Using databricks connection "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m05:53:48.237964 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        Segment as value_field,
        count(*) as n_records

    from finance.dim_info
    group by Segment

)

select *
from all_values
where value_field not in (
    'Government','Channel Partners','Midmarket','Enterprise','Small Business'
)



      
    ) dbt_internal_test
[0m05:53:48.237964 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m05:53:50.637348 [debug] [Thread-1 (]: SQL status: OK in 2.4 seconds
[0m05:53:50.643249 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:50.644245 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: ROLLBACK
[0m05:53:50.644245 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:53:50.645242 [debug] [Thread-1 (]: On test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885: Close
[0m05:53:51.531246 [info ] [Thread-1 (]: 1 of 25 PASS accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business  [[32mPASS[0m in 3.33s]
[0m05:53:51.532426 [debug] [Thread-1 (]: Finished running node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:53:51.532488 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:53:51.533292 [info ] [Thread-1 (]: 2 of 25 START test not_null_dim_exp_Actual ..................................... [RUN]
[0m05:53:51.533820 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Actual.1d45aa27dd"
[0m05:53:51.534330 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:53:51.534659 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:53:51.578387 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Actual.1d45aa27dd"
[0m05:53:51.579392 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:51.579392 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:53:51.582373 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Actual.1d45aa27dd"
[0m05:53:51.582373 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:53:51.583421 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Actual.1d45aa27dd"
[0m05:53:51.583548 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Actual.1d45aa27dd: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Actual.1d45aa27dd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_exp
where Actual is null



      
    ) dbt_internal_test
[0m05:53:51.583548 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:53:53.753153 [debug] [Thread-1 (]: SQL status: OK in 2.17 seconds
[0m05:53:53.757151 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:53.757151 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Actual.1d45aa27dd: ROLLBACK
[0m05:53:53.758213 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:53:53.758489 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Actual.1d45aa27dd: Close
[0m05:53:54.654234 [info ] [Thread-1 (]: 2 of 25 PASS not_null_dim_exp_Actual ........................................... [[32mPASS[0m in 3.12s]
[0m05:53:54.656836 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:53:54.657752 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:53:54.658020 [info ] [Thread-1 (]: 3 of 25 START test not_null_dim_exp_Expenses ................................... [RUN]
[0m05:53:54.658462 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m05:53:54.659518 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:53:54.659518 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:53:54.664584 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m05:53:54.665580 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:54.666509 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:53:54.668388 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m05:53:54.668388 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:53:54.669310 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m05:53:54.669310 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Expenses.c94546fc2a: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Expenses.c94546fc2a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Expenses
from finance.dim_exp
where Expenses is null



      
    ) dbt_internal_test
[0m05:53:54.669813 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:53:56.666165 [debug] [Thread-1 (]: SQL status: OK in 2.0 seconds
[0m05:53:56.670975 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:56.670975 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Expenses.c94546fc2a: ROLLBACK
[0m05:53:56.671554 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:53:56.671554 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Expenses.c94546fc2a: Close
[0m05:53:57.557389 [info ] [Thread-1 (]: 3 of 25 PASS not_null_dim_exp_Expenses ......................................... [[32mPASS[0m in 2.90s]
[0m05:53:57.559833 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:53:57.561411 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:53:57.562058 [info ] [Thread-1 (]: 4 of 25 START test not_null_dim_exp_Gross_sales ................................ [RUN]
[0m05:53:57.563488 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m05:53:57.564417 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:53:57.564863 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:53:57.568451 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m05:53:57.570187 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:57.570187 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:53:57.572362 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m05:53:57.572362 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:53:57.573319 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m05:53:57.573319 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Gross_sales.07439d5e88: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Gross_sales
from finance.dim_exp
where Gross_sales is null



      
    ) dbt_internal_test
[0m05:53:57.573319 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:53:59.569207 [debug] [Thread-1 (]: SQL status: OK in 2.0 seconds
[0m05:53:59.574190 [debug] [Thread-1 (]: finished collecting timing info
[0m05:53:59.574190 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Gross_sales.07439d5e88: ROLLBACK
[0m05:53:59.575056 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:53:59.575056 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Gross_sales.07439d5e88: Close
[0m05:54:00.578448 [info ] [Thread-1 (]: 4 of 25 PASS not_null_dim_exp_Gross_sales ...................................... [[32mPASS[0m in 3.02s]
[0m05:54:00.580400 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:54:00.581472 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:54:00.581472 [info ] [Thread-1 (]: 5 of 25 START test not_null_dim_exp_Manufacturing_price ........................ [RUN]
[0m05:54:00.582541 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m05:54:00.582541 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:54:00.583658 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:54:00.587638 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m05:54:00.588198 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:00.588695 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:54:00.590942 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m05:54:00.590942 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:00.591674 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m05:54:00.592171 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Manufacturing_price
from finance.dim_exp
where Manufacturing_price is null



      
    ) dbt_internal_test
[0m05:54:00.592171 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:02.616380 [debug] [Thread-1 (]: SQL status: OK in 2.02 seconds
[0m05:54:02.620666 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:02.621239 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f: ROLLBACK
[0m05:54:02.621458 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:02.621458 [debug] [Thread-1 (]: On test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f: Close
[0m05:54:03.522206 [info ] [Thread-1 (]: 5 of 25 PASS not_null_dim_exp_Manufacturing_price .............................. [[32mPASS[0m in 2.94s]
[0m05:54:03.524661 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:54:03.524661 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:54:03.525665 [info ] [Thread-1 (]: 6 of 25 START test not_null_dim_info_Actual .................................... [RUN]
[0m05:54:03.526738 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m05:54:03.526738 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:54:03.528027 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:54:03.532013 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m05:54:03.533008 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:03.533815 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:54:03.536165 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m05:54:03.537163 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:03.537163 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m05:54:03.537163 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Actual.5cb2ffbb95"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_info
where Actual is null



      
    ) dbt_internal_test
[0m05:54:03.537701 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:05.573930 [debug] [Thread-1 (]: SQL status: OK in 2.04 seconds
[0m05:54:05.577123 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:05.578458 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: ROLLBACK
[0m05:54:05.578458 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:05.578953 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Actual.5cb2ffbb95: Close
[0m05:54:06.498761 [info ] [Thread-1 (]: 6 of 25 PASS not_null_dim_info_Actual .......................................... [[32mPASS[0m in 2.97s]
[0m05:54:06.500751 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:54:06.501805 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:54:06.502305 [info ] [Thread-1 (]: 7 of 25 START test not_null_dim_info_COGS ...................................... [RUN]
[0m05:54:06.502941 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m05:54:06.502941 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:54:06.504105 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:54:06.508538 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m05:54:06.509006 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:06.509006 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:54:06.510827 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m05:54:06.511454 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:06.512031 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m05:54:06.512500 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_COGS.efb9b66052"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select COGS
from finance.dim_info
where COGS is null



      
    ) dbt_internal_test
[0m05:54:06.512500 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:08.532875 [debug] [Thread-1 (]: SQL status: OK in 2.02 seconds
[0m05:54:08.537127 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:08.537127 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: ROLLBACK
[0m05:54:08.537127 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:08.538044 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_COGS.efb9b66052: Close
[0m05:54:09.424676 [info ] [Thread-1 (]: 7 of 25 PASS not_null_dim_info_COGS ............................................ [[32mPASS[0m in 2.92s]
[0m05:54:09.427595 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:54:09.428648 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Inventory.45016db171
[0m05:54:09.428786 [info ] [Thread-1 (]: 8 of 25 START test not_null_dim_info_Inventory ................................. [RUN]
[0m05:54:09.431082 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Inventory.45016db171"
[0m05:54:09.431208 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Inventory.45016db171
[0m05:54:09.431208 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Inventory.45016db171
[0m05:54:09.436321 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Inventory.45016db171"
[0m05:54:09.437693 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:09.437693 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Inventory.45016db171
[0m05:54:09.438795 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Inventory.45016db171"
[0m05:54:09.439790 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:09.439790 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Inventory.45016db171"
[0m05:54:09.440714 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Inventory.45016db171"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Inventory
from finance.dim_info
where Inventory is null



      
    ) dbt_internal_test
[0m05:54:09.440714 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:11.450656 [debug] [Thread-1 (]: SQL status: OK in 2.01 seconds
[0m05:54:11.454642 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:11.455638 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: ROLLBACK
[0m05:54:11.455761 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:11.455761 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Inventory.45016db171: Close
[0m05:54:12.351209 [info ] [Thread-1 (]: 8 of 25 PASS not_null_dim_info_Inventory ....................................... [[32mPASS[0m in 2.92s]
[0m05:54:12.354628 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Inventory.45016db171
[0m05:54:12.355622 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:54:12.356119 [info ] [Thread-1 (]: 9 of 25 START test not_null_dim_info_Segment ................................... [RUN]
[0m05:54:12.357655 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m05:54:12.357655 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:54:12.358094 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:54:12.361846 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m05:54:12.362841 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:12.362841 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:54:12.364702 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m05:54:12.365700 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:12.365700 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m05:54:12.366541 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_info_Segment.57cc6a2d98"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Segment
from finance.dim_info
where Segment is null



      
    ) dbt_internal_test
[0m05:54:12.366707 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:14.371258 [debug] [Thread-1 (]: SQL status: OK in 2.0 seconds
[0m05:54:14.375395 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:14.376434 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: ROLLBACK
[0m05:54:14.376786 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:14.376786 [debug] [Thread-1 (]: On test.finance.not_null_dim_info_Segment.57cc6a2d98: Close
[0m05:54:15.284940 [info ] [Thread-1 (]: 9 of 25 PASS not_null_dim_info_Segment ......................................... [[32mPASS[0m in 2.93s]
[0m05:54:15.286933 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:54:15.287778 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:54:15.288276 [info ] [Thread-1 (]: 10 of 25 START test not_null_dim_month_Actual .................................. [RUN]
[0m05:54:15.289310 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Actual.9fdc4e57c7"
[0m05:54:15.289310 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:54:15.290333 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:54:15.295648 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Actual.9fdc4e57c7"
[0m05:54:15.296600 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:15.296600 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:54:15.298094 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Actual.9fdc4e57c7"
[0m05:54:15.299181 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:15.300140 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Actual.9fdc4e57c7"
[0m05:54:15.300140 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Actual.9fdc4e57c7: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Actual.9fdc4e57c7"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_month
where Actual is null



      
    ) dbt_internal_test
[0m05:54:15.300140 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:17.369431 [debug] [Thread-1 (]: SQL status: OK in 2.07 seconds
[0m05:54:17.372513 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:17.372513 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Actual.9fdc4e57c7: ROLLBACK
[0m05:54:17.372513 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:17.372513 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Actual.9fdc4e57c7: Close
[0m05:54:18.272316 [info ] [Thread-1 (]: 10 of 25 PASS not_null_dim_month_Actual ........................................ [[32mPASS[0m in 2.98s]
[0m05:54:18.279939 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:54:18.280936 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:54:18.281106 [info ] [Thread-1 (]: 11 of 25 START test not_null_dim_month_Date .................................... [RUN]
[0m05:54:18.284620 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m05:54:18.284620 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:54:18.285621 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:54:18.290191 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m05:54:18.291162 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:18.291162 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:54:18.293238 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m05:54:18.294721 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:18.294721 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m05:54:18.294721 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Date.ce09cea79f: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Date.ce09cea79f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Date
from finance.dim_month
where Date is null



      
    ) dbt_internal_test
[0m05:54:18.295266 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:20.340439 [debug] [Thread-1 (]: SQL status: OK in 2.05 seconds
[0m05:54:20.343800 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:20.344815 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Date.ce09cea79f: ROLLBACK
[0m05:54:20.344815 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:20.345313 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Date.ce09cea79f: Close
[0m05:54:21.231983 [info ] [Thread-1 (]: 11 of 25 PASS not_null_dim_month_Date .......................................... [[32mPASS[0m in 2.95s]
[0m05:54:21.239810 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:54:21.239810 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:54:21.241196 [info ] [Thread-1 (]: 12 of 25 START test not_null_dim_month_MonthName ............................... [RUN]
[0m05:54:21.244082 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m05:54:21.245164 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:54:21.245164 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:54:21.248950 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m05:54:21.249703 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:21.249703 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:54:21.251914 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m05:54:21.252910 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:21.253749 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m05:54:21.253749 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_MonthName.c1d116d50b: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_MonthName.c1d116d50b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select MonthName
from finance.dim_month
where MonthName is null



      
    ) dbt_internal_test
[0m05:54:21.254336 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:23.283842 [debug] [Thread-1 (]: SQL status: OK in 2.03 seconds
[0m05:54:23.286243 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:23.287237 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_MonthName.c1d116d50b: ROLLBACK
[0m05:54:23.287467 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:23.287731 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_MonthName.c1d116d50b: Close
[0m05:54:24.182370 [info ] [Thread-1 (]: 12 of 25 PASS not_null_dim_month_MonthName ..................................... [[32mPASS[0m in 2.94s]
[0m05:54:24.190288 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:54:24.190288 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:54:24.191406 [info ] [Thread-1 (]: 13 of 25 START test not_null_dim_month_Month_Number ............................ [RUN]
[0m05:54:24.194902 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m05:54:24.194902 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:54:24.195836 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:54:24.199447 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m05:54:24.201316 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:24.201758 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:54:24.203959 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m05:54:24.204881 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:24.204881 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m05:54:24.204881 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Month_Number.be337b93c3: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Month_Number.be337b93c3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Month_Number
from finance.dim_month
where Month_Number is null



      
    ) dbt_internal_test
[0m05:54:24.204881 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:26.256254 [debug] [Thread-1 (]: SQL status: OK in 2.05 seconds
[0m05:54:26.258247 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:26.259326 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Month_Number.be337b93c3: ROLLBACK
[0m05:54:26.259326 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:26.259326 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Month_Number.be337b93c3: Close
[0m05:54:27.247458 [info ] [Thread-1 (]: 13 of 25 PASS not_null_dim_month_Month_Number .................................. [[32mPASS[0m in 3.05s]
[0m05:54:27.254656 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:54:27.255695 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:54:27.255695 [info ] [Thread-1 (]: 14 of 25 START test not_null_dim_month_Year .................................... [RUN]
[0m05:54:27.259775 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m05:54:27.259775 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:54:27.260766 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:54:27.265222 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m05:54:27.266729 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:27.266729 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:54:27.269734 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m05:54:27.270840 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:27.270840 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m05:54:27.270840 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Year.5d11bba8bb: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_month_Year.5d11bba8bb"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Year
from finance.dim_month
where Year is null



      
    ) dbt_internal_test
[0m05:54:27.271736 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:29.292294 [debug] [Thread-1 (]: SQL status: OK in 2.02 seconds
[0m05:54:29.295536 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:29.295536 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Year.5d11bba8bb: ROLLBACK
[0m05:54:29.295536 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:29.295536 [debug] [Thread-1 (]: On test.finance.not_null_dim_month_Year.5d11bba8bb: Close
[0m05:54:30.202951 [info ] [Thread-1 (]: 14 of 25 PASS not_null_dim_month_Year .......................................... [[32mPASS[0m in 2.94s]
[0m05:54:30.203948 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:54:30.204944 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Actual.6279364421
[0m05:54:30.204944 [info ] [Thread-1 (]: 15 of 25 START test not_null_dim_profit_Actual ................................. [RUN]
[0m05:54:30.205940 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Actual.6279364421"
[0m05:54:30.205940 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Actual.6279364421
[0m05:54:30.205940 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Actual.6279364421
[0m05:54:30.210199 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Actual.6279364421"
[0m05:54:30.211195 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:30.211195 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Actual.6279364421
[0m05:54:30.213189 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Actual.6279364421"
[0m05:54:30.213189 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:30.214184 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Actual.6279364421"
[0m05:54:30.214184 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Actual.6279364421: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Actual.6279364421"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_profit
where Actual is null



      
    ) dbt_internal_test
[0m05:54:30.214184 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:32.641001 [debug] [Thread-1 (]: SQL status: OK in 2.43 seconds
[0m05:54:32.644989 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:32.646057 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Actual.6279364421: ROLLBACK
[0m05:54:32.646057 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:32.646982 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Actual.6279364421: Close
[0m05:54:33.540891 [info ] [Thread-1 (]: 15 of 25 PASS not_null_dim_profit_Actual ....................................... [[32mPASS[0m in 3.34s]
[0m05:54:33.543125 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Actual.6279364421
[0m05:54:33.543125 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:54:33.544037 [info ] [Thread-1 (]: 16 of 25 START test not_null_dim_profit_Country ................................ [RUN]
[0m05:54:33.544535 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m05:54:33.544535 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:54:33.545553 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:54:33.550344 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m05:54:33.551018 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:33.551158 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:54:33.552733 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m05:54:33.553974 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:33.554468 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m05:54:33.554468 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Country.363de9fbd5: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Country.363de9fbd5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Country
from finance.dim_profit
where Country is null



      
    ) dbt_internal_test
[0m05:54:33.555256 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:35.585729 [debug] [Thread-1 (]: SQL status: OK in 2.03 seconds
[0m05:54:35.590881 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:35.591878 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Country.363de9fbd5: ROLLBACK
[0m05:54:35.591878 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:35.592526 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Country.363de9fbd5: Close
[0m05:54:36.499603 [info ] [Thread-1 (]: 16 of 25 PASS not_null_dim_profit_Country ...................................... [[32mPASS[0m in 2.96s]
[0m05:54:36.501597 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:54:36.502833 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:54:36.503829 [info ] [Thread-1 (]: 17 of 25 START test not_null_dim_profit_Discount_Band .......................... [RUN]
[0m05:54:36.504819 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m05:54:36.504819 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:54:36.505692 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:54:36.510203 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m05:54:36.511052 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:36.511052 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:54:36.513577 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m05:54:36.513577 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:36.514573 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m05:54:36.514872 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Discount_Band
from finance.dim_profit
where Discount_Band is null



      
    ) dbt_internal_test
[0m05:54:36.515006 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:38.514105 [debug] [Thread-1 (]: SQL status: OK in 2.0 seconds
[0m05:54:38.517993 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:38.517993 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa: ROLLBACK
[0m05:54:38.519262 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:38.519499 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa: Close
[0m05:54:39.402885 [info ] [Thread-1 (]: 17 of 25 PASS not_null_dim_profit_Discount_Band ................................ [[32mPASS[0m in 2.90s]
[0m05:54:39.405276 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:54:39.406273 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:54:39.406478 [info ] [Thread-1 (]: 18 of 25 START test not_null_dim_profit_Discounts .............................. [RUN]
[0m05:54:39.407632 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m05:54:39.407632 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:54:39.408631 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:54:39.414014 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m05:54:39.414014 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:39.415064 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:54:39.417598 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m05:54:39.418521 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:39.418521 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m05:54:39.418521 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discounts.03e349480e: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Discounts.03e349480e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Discounts
from finance.dim_profit
where Discounts is null



      
    ) dbt_internal_test
[0m05:54:39.419100 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:41.489087 [debug] [Thread-1 (]: SQL status: OK in 2.07 seconds
[0m05:54:41.493075 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:41.494230 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discounts.03e349480e: ROLLBACK
[0m05:54:41.494230 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:41.494834 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Discounts.03e349480e: Close
[0m05:54:42.391085 [info ] [Thread-1 (]: 18 of 25 PASS not_null_dim_profit_Discounts .................................... [[32mPASS[0m in 2.98s]
[0m05:54:42.393557 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:54:42.393557 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Product.1422e74993
[0m05:54:42.394558 [info ] [Thread-1 (]: 19 of 25 START test not_null_dim_profit_Product ................................ [RUN]
[0m05:54:42.395554 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Product.1422e74993"
[0m05:54:42.395554 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Product.1422e74993
[0m05:54:42.396443 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Product.1422e74993
[0m05:54:42.400938 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Product.1422e74993"
[0m05:54:42.401852 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:42.401852 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Product.1422e74993
[0m05:54:42.403860 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Product.1422e74993"
[0m05:54:42.404832 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:42.404832 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Product.1422e74993"
[0m05:54:42.405688 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Product.1422e74993: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Product.1422e74993"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Product
from finance.dim_profit
where Product is null



      
    ) dbt_internal_test
[0m05:54:42.405928 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:44.382625 [debug] [Thread-1 (]: SQL status: OK in 1.98 seconds
[0m05:54:44.386446 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:44.386446 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Product.1422e74993: ROLLBACK
[0m05:54:44.387174 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:44.387385 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Product.1422e74993: Close
[0m05:54:45.275860 [info ] [Thread-1 (]: 19 of 25 PASS not_null_dim_profit_Product ...................................... [[32mPASS[0m in 2.88s]
[0m05:54:45.278612 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Product.1422e74993
[0m05:54:45.278612 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:54:45.279950 [info ] [Thread-1 (]: 20 of 25 START test not_null_dim_profit_Profit ................................. [RUN]
[0m05:54:45.281132 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m05:54:45.281132 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:54:45.281132 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:54:45.286198 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m05:54:45.287562 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:45.287562 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:54:45.290083 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m05:54:45.290872 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:45.290872 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m05:54:45.290872 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Profit.3c5f35db12: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Profit.3c5f35db12"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Profit
from finance.dim_profit
where Profit is null



      
    ) dbt_internal_test
[0m05:54:45.291837 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:47.261558 [debug] [Thread-1 (]: SQL status: OK in 1.97 seconds
[0m05:54:47.265677 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:47.266682 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Profit.3c5f35db12: ROLLBACK
[0m05:54:47.266682 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:47.266682 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Profit.3c5f35db12: Close
[0m05:54:48.167452 [info ] [Thread-1 (]: 20 of 25 PASS not_null_dim_profit_Profit ....................................... [[32mPASS[0m in 2.89s]
[0m05:54:48.170567 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:54:48.170567 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:54:48.170567 [info ] [Thread-1 (]: 21 of 25 START test not_null_dim_profit_Sales .................................. [RUN]
[0m05:54:48.173585 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m05:54:48.174658 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:54:48.174779 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:54:48.178381 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m05:54:48.178381 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:48.178381 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:54:48.180644 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m05:54:48.181717 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:48.181717 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m05:54:48.181717 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Sales.05c16c548a: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_profit_Sales.05c16c548a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Sales
from finance.dim_profit
where Sales is null



      
    ) dbt_internal_test
[0m05:54:48.182471 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:50.163120 [debug] [Thread-1 (]: SQL status: OK in 1.98 seconds
[0m05:54:50.166188 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:50.166188 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Sales.05c16c548a: ROLLBACK
[0m05:54:50.166188 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:50.167124 [debug] [Thread-1 (]: On test.finance.not_null_dim_profit_Sales.05c16c548a: Close
[0m05:54:51.068103 [info ] [Thread-1 (]: 21 of 25 PASS not_null_dim_profit_Sales ........................................ [[32mPASS[0m in 2.90s]
[0m05:54:51.069099 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:54:51.070191 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:54:51.070729 [info ] [Thread-1 (]: 22 of 25 START test not_null_dim_unit_Actual ................................... [RUN]
[0m05:54:51.071617 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Actual.7f6f830652"
[0m05:54:51.071617 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:54:51.071617 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:54:51.076499 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Actual.7f6f830652"
[0m05:54:51.078490 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:51.078729 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:54:51.082582 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Actual.7f6f830652"
[0m05:54:51.083915 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:51.084147 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Actual.7f6f830652"
[0m05:54:51.084147 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Actual.7f6f830652: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Actual.7f6f830652"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Actual
from finance.dim_unit
where Actual is null



      
    ) dbt_internal_test
[0m05:54:51.084688 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:53.204878 [debug] [Thread-1 (]: SQL status: OK in 2.12 seconds
[0m05:54:53.209700 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:53.209700 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Actual.7f6f830652: ROLLBACK
[0m05:54:53.210577 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:53.211090 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Actual.7f6f830652: Close
[0m05:54:54.092954 [info ] [Thread-1 (]: 22 of 25 PASS not_null_dim_unit_Actual ......................................... [[32mPASS[0m in 3.02s]
[0m05:54:54.093919 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:54:54.094213 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:54:54.094537 [info ] [Thread-1 (]: 23 of 25 START test not_null_dim_unit_Target ................................... [RUN]
[0m05:54:54.095239 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m05:54:54.095623 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:54:54.095796 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:54:54.100128 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m05:54:54.101024 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:54.101310 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:54:54.103440 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m05:54:54.104127 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:54.104365 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m05:54:54.104481 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Target.33e9c88d57: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Target.33e9c88d57"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Target
from finance.dim_unit
where Target is null



      
    ) dbt_internal_test
[0m05:54:54.104688 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:56.131038 [debug] [Thread-1 (]: SQL status: OK in 2.03 seconds
[0m05:54:56.136247 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:56.136247 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Target.33e9c88d57: ROLLBACK
[0m05:54:56.137316 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:56.137636 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Target.33e9c88d57: Close
[0m05:54:57.017138 [info ] [Thread-1 (]: 23 of 25 PASS not_null_dim_unit_Target ......................................... [[32mPASS[0m in 2.92s]
[0m05:54:57.018556 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:54:57.019058 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:54:57.019491 [info ] [Thread-1 (]: 24 of 25 START test not_null_dim_unit_Unit_Price ............................... [RUN]
[0m05:54:57.019654 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m05:54:57.020477 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:54:57.020684 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:54:57.025630 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m05:54:57.026473 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:57.026473 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:54:57.028478 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m05:54:57.029692 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:57.029918 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m05:54:57.029987 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Unit_Price.912d40d17b: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Unit_Price
from finance.dim_unit
where Unit_Price is null



      
    ) dbt_internal_test
[0m05:54:57.029987 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:54:58.989755 [debug] [Thread-1 (]: SQL status: OK in 1.96 seconds
[0m05:54:58.996137 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:58.997065 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Unit_Price.912d40d17b: ROLLBACK
[0m05:54:58.997542 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:54:58.997542 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Unit_Price.912d40d17b: Close
[0m05:54:59.892765 [info ] [Thread-1 (]: 24 of 25 PASS not_null_dim_unit_Unit_Price ..................................... [[32mPASS[0m in 2.87s]
[0m05:54:59.895435 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:54:59.896462 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:54:59.897433 [info ] [Thread-1 (]: 25 of 25 START test not_null_dim_unit_Units_Sold ............................... [RUN]
[0m05:54:59.898433 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m05:54:59.898433 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:54:59.899767 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:54:59.904362 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m05:54:59.905481 [debug] [Thread-1 (]: finished collecting timing info
[0m05:54:59.905918 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:54:59.907189 [debug] [Thread-1 (]: Writing runtime sql for node "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m05:54:59.908186 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:54:59.908186 [debug] [Thread-1 (]: Using databricks connection "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m05:54:59.908186 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Units_Sold.fd5939c596: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select Units_Sold
from finance.dim_unit
where Units_Sold is null



      
    ) dbt_internal_test
[0m05:54:59.908186 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:55:03.078508 [debug] [Thread-1 (]: SQL status: OK in 3.17 seconds
[0m05:55:03.082476 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:03.083496 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Units_Sold.fd5939c596: ROLLBACK
[0m05:55:03.083496 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:55:03.084021 [debug] [Thread-1 (]: On test.finance.not_null_dim_unit_Units_Sold.fd5939c596: Close
[0m05:55:04.051916 [info ] [Thread-1 (]: 25 of 25 PASS not_null_dim_unit_Units_Sold ..................................... [[32mPASS[0m in 4.15s]
[0m05:55:04.053584 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:55:04.055508 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:55:04.055508 [debug] [MainThread]: On master: ROLLBACK
[0m05:55:04.055508 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:55:04.996108 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:55:04.997108 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:55:04.997108 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:55:04.998102 [debug] [MainThread]: On master: ROLLBACK
[0m05:55:04.998102 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:55:04.998102 [debug] [MainThread]: On master: Close
[0m05:55:05.927310 [info ] [MainThread]: 
[0m05:55:05.929303 [info ] [MainThread]: Finished running 25 tests in 0 hours 1 minutes and 20.85 seconds (80.85s).
[0m05:55:05.930304 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:55:05.930674 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m05:55:05.931578 [debug] [MainThread]: Connection 'test.finance.not_null_dim_unit_Units_Sold.fd5939c596' was properly closed.
[0m05:55:05.941621 [info ] [MainThread]: 
[0m05:55:05.942242 [info ] [MainThread]: [32mCompleted successfully[0m
[0m05:55:05.943643 [info ] [MainThread]: 
[0m05:55:05.944227 [info ] [MainThread]: Done. PASS=25 WARN=0 ERROR=0 SKIP=0 TOTAL=25
[0m05:55:05.944691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE37090D60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE36EB4130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE370399F0>]}
[0m05:55:05.945085 [debug] [MainThread]: Flushing usage events


============================== 2022-11-28 05:55:50.220685 | 2c8228d5-587e-4895-aa70-7cbb75168bbd ==============================
[0m05:55:50.220685 [info ] [MainThread]: Running with dbt=1.3.1
[0m05:55:50.220685 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m05:55:50.230419 [debug] [MainThread]: Tracking: tracking
[0m05:55:50.240481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000266107F9C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000266107F9870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000266107F9AB0>]}
[0m05:55:50.303510 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m05:55:50.303510 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m05:55:50.303510 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.finance.example

[0m05:55:50.310603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2c8228d5-587e-4895-aa70-7cbb75168bbd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026610A200D0>]}
[0m05:55:50.320447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2c8228d5-587e-4895-aa70-7cbb75168bbd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000266109F4040>]}
[0m05:55:50.320447 [info ] [MainThread]: Found 6 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m05:55:50.321957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c8228d5-587e-4895-aa70-7cbb75168bbd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000266109F4130>]}
[0m05:55:50.323248 [info ] [MainThread]: 
[0m05:55:50.323248 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:55:50.335152 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m05:55:50.340842 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m05:55:50.340842 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m05:55:50.340842 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m05:55:50.340842 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:55:52.773117 [debug] [ThreadPool]: SQL status: OK in 2.43 seconds
[0m05:55:52.780629 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m05:55:52.780629 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m05:55:52.782704 [debug] [ThreadPool]: On list_None_finance: Close
[0m05:55:53.883341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c8228d5-587e-4895-aa70-7cbb75168bbd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002661080B7C0>]}
[0m05:55:53.883341 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m05:55:53.883341 [info ] [MainThread]: 
[0m05:55:53.902207 [debug] [Thread-1 (]: Began running node model.finance.dim_exp
[0m05:55:53.902207 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_exp"
[0m05:55:53.902207 [debug] [Thread-1 (]: Began compiling node model.finance.dim_exp
[0m05:55:53.905682 [debug] [Thread-1 (]: Compiling model.finance.dim_exp
[0m05:55:53.910850 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_exp"
[0m05:55:53.912018 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:53.912018 [debug] [Thread-1 (]: Began executing node model.finance.dim_exp
[0m05:55:53.912018 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:53.912985 [debug] [Thread-1 (]: Finished running node model.finance.dim_exp
[0m05:55:53.912985 [debug] [Thread-1 (]: Began running node model.finance.dim_info
[0m05:55:53.912985 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_info"
[0m05:55:53.912985 [debug] [Thread-1 (]: Began compiling node model.finance.dim_info
[0m05:55:53.912985 [debug] [Thread-1 (]: Compiling model.finance.dim_info
[0m05:55:53.915089 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_info"
[0m05:55:53.918087 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:53.918374 [debug] [Thread-1 (]: Began executing node model.finance.dim_info
[0m05:55:53.918870 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:53.919817 [debug] [Thread-1 (]: Finished running node model.finance.dim_info
[0m05:55:53.919817 [debug] [Thread-1 (]: Began running node model.finance.dim_month
[0m05:55:53.920877 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_month"
[0m05:55:53.922114 [debug] [Thread-1 (]: Began compiling node model.finance.dim_month
[0m05:55:53.922608 [debug] [Thread-1 (]: Compiling model.finance.dim_month
[0m05:55:53.922608 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_month"
[0m05:55:53.922608 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:53.922608 [debug] [Thread-1 (]: Began executing node model.finance.dim_month
[0m05:55:53.922608 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:53.922608 [debug] [Thread-1 (]: Finished running node model.finance.dim_month
[0m05:55:53.922608 [debug] [Thread-1 (]: Began running node model.finance.dim_profit
[0m05:55:53.927640 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_profit"
[0m05:55:53.927640 [debug] [Thread-1 (]: Began compiling node model.finance.dim_profit
[0m05:55:53.927640 [debug] [Thread-1 (]: Compiling model.finance.dim_profit
[0m05:55:53.965881 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_profit"
[0m05:55:53.967185 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:53.967310 [debug] [Thread-1 (]: Began executing node model.finance.dim_profit
[0m05:55:53.967729 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:53.968366 [debug] [Thread-1 (]: Finished running node model.finance.dim_profit
[0m05:55:53.968366 [debug] [Thread-1 (]: Began running node model.finance.dim_unit
[0m05:55:53.969201 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_unit"
[0m05:55:53.969201 [debug] [Thread-1 (]: Began compiling node model.finance.dim_unit
[0m05:55:53.969201 [debug] [Thread-1 (]: Compiling model.finance.dim_unit
[0m05:55:53.970500 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_unit"
[0m05:55:53.970500 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:53.970500 [debug] [Thread-1 (]: Began executing node model.finance.dim_unit
[0m05:55:53.973685 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:53.973685 [debug] [Thread-1 (]: Finished running node model.finance.dim_unit
[0m05:55:53.975363 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:55:53.975363 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Actual.1d45aa27dd"
[0m05:55:53.975363 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:55:53.975363 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:55:53.981102 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Actual.1d45aa27dd"
[0m05:55:53.986818 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:53.986818 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:55:53.987239 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:53.987809 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Actual.1d45aa27dd
[0m05:55:53.987809 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:55:53.987809 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m05:55:53.987809 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:55:53.990222 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:55:53.990675 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Expenses.c94546fc2a"
[0m05:55:53.994535 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:53.994535 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:55:53.995145 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:53.995145 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Expenses.c94546fc2a
[0m05:55:53.995145 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:55:53.996207 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m05:55:53.996207 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:55:53.996804 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:55:53.996804 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Gross_sales.07439d5e88"
[0m05:55:54.001035 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.001205 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:55:54.001205 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.001651 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Gross_sales.07439d5e88
[0m05:55:54.002395 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:55:54.002754 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m05:55:54.002754 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:55:54.003523 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:55:54.003523 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f"
[0m05:55:54.007429 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.007923 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:55:54.008168 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.008168 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_exp_Manufacturing_price.5f8d1ca24f
[0m05:55:54.008168 [debug] [Thread-1 (]: Began running node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:55:54.009119 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m05:55:54.009119 [debug] [Thread-1 (]: Began compiling node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:55:54.010120 [debug] [Thread-1 (]: Compiling test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:55:54.015784 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885"
[0m05:55:54.018921 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.018921 [debug] [Thread-1 (]: Began executing node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:55:54.019418 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.019952 [debug] [Thread-1 (]: Finished running node test.finance.accepted_values_dim_info_Segment__Government__Channel_Partners__Midmarket__Enterprise__Small_Business.174da36885
[0m05:55:54.020454 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:55:54.021352 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m05:55:54.021352 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:55:54.021848 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:55:54.022185 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Actual.5cb2ffbb95"
[0m05:55:54.022185 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.027231 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:55:54.027573 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.027904 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Actual.5cb2ffbb95
[0m05:55:54.027904 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:55:54.028889 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m05:55:54.028889 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:55:54.028889 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:55:54.030800 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_COGS.efb9b66052"
[0m05:55:54.033598 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.033598 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:55:54.033598 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.034505 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_COGS.efb9b66052
[0m05:55:54.034505 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Inventory.45016db171
[0m05:55:54.035320 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Inventory.45016db171"
[0m05:55:54.035920 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Inventory.45016db171
[0m05:55:54.036002 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Inventory.45016db171
[0m05:55:54.036522 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Inventory.45016db171"
[0m05:55:54.036522 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.036522 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Inventory.45016db171
[0m05:55:54.040494 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.040494 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Inventory.45016db171
[0m05:55:54.040494 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:55:54.040494 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m05:55:54.040494 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:55:54.042573 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:55:54.042976 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_info_Segment.57cc6a2d98"
[0m05:55:54.042976 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.047043 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:55:54.047043 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.047378 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_info_Segment.57cc6a2d98
[0m05:55:54.047378 [debug] [Thread-1 (]: Began running node model.finance.dim_check_month
[0m05:55:54.047378 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_check_month"
[0m05:55:54.047378 [debug] [Thread-1 (]: Began compiling node model.finance.dim_check_month
[0m05:55:54.049083 [debug] [Thread-1 (]: Compiling model.finance.dim_check_month
[0m05:55:54.050490 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_check_month"
[0m05:55:54.052379 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.052469 [debug] [Thread-1 (]: Began executing node model.finance.dim_check_month
[0m05:55:54.052871 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.052871 [debug] [Thread-1 (]: Finished running node model.finance.dim_check_month
[0m05:55:54.052871 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:55:54.053911 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Actual.9fdc4e57c7"
[0m05:55:54.053911 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:55:54.053911 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:55:54.053911 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Actual.9fdc4e57c7"
[0m05:55:54.059087 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.059306 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:55:54.059306 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.060431 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Actual.9fdc4e57c7
[0m05:55:54.060431 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:55:54.060928 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m05:55:54.061771 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:55:54.061873 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:55:54.061873 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Date.ce09cea79f"
[0m05:55:54.061873 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.061873 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:55:54.061873 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.061873 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Date.ce09cea79f
[0m05:55:54.061873 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:55:54.061873 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m05:55:54.061873 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:55:54.061873 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:55:54.070540 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_MonthName.c1d116d50b"
[0m05:55:54.070540 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.073189 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:55:54.073189 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.073754 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_MonthName.c1d116d50b
[0m05:55:54.073754 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:55:54.073754 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m05:55:54.073754 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:55:54.075345 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:55:54.076988 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Month_Number.be337b93c3"
[0m05:55:54.080497 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.080497 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:55:54.080497 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.080497 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Month_Number.be337b93c3
[0m05:55:54.080497 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:55:54.080497 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m05:55:54.080497 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:55:54.080497 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:55:54.080497 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_month_Year.5d11bba8bb"
[0m05:55:54.080497 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.080497 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:55:54.080497 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.080497 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_month_Year.5d11bba8bb
[0m05:55:54.080497 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Actual.6279364421
[0m05:55:54.080497 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Actual.6279364421"
[0m05:55:54.080497 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Actual.6279364421
[0m05:55:54.090717 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Actual.6279364421
[0m05:55:54.091214 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Actual.6279364421"
[0m05:55:54.091214 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.091214 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Actual.6279364421
[0m05:55:54.091214 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.091214 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Actual.6279364421
[0m05:55:54.091214 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:55:54.091214 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m05:55:54.091214 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:55:54.091214 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:55:54.091214 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Country.363de9fbd5"
[0m05:55:54.100955 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.100955 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:55:54.100955 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.101835 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Country.363de9fbd5
[0m05:55:54.101835 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:55:54.101835 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m05:55:54.101835 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:55:54.101835 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:55:54.101835 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa"
[0m05:55:54.101835 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.101835 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:55:54.101835 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.108768 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Discount_Band.59dff5d2fa
[0m05:55:54.108768 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:55:54.108768 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m05:55:54.108768 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:55:54.108768 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:55:54.110863 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Discounts.03e349480e"
[0m05:55:54.110863 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.110863 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:55:54.110863 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.110863 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Discounts.03e349480e
[0m05:55:54.110863 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Product.1422e74993
[0m05:55:54.110863 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Product.1422e74993"
[0m05:55:54.110863 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Product.1422e74993
[0m05:55:54.110863 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Product.1422e74993
[0m05:55:54.110863 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Product.1422e74993"
[0m05:55:54.110863 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.110863 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Product.1422e74993
[0m05:55:54.120859 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.120859 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Product.1422e74993
[0m05:55:54.121790 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:55:54.122064 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m05:55:54.122064 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:55:54.122064 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:55:54.122064 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Profit.3c5f35db12"
[0m05:55:54.122064 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.122064 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:55:54.122064 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.122064 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Profit.3c5f35db12
[0m05:55:54.122064 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:55:54.128072 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m05:55:54.128072 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:55:54.128072 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:55:54.130934 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_profit_Sales.05c16c548a"
[0m05:55:54.130934 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.130934 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:55:54.130934 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.133454 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_profit_Sales.05c16c548a
[0m05:55:54.133454 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:55:54.133454 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Actual.7f6f830652"
[0m05:55:54.133454 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:55:54.134987 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:55:54.135585 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Actual.7f6f830652"
[0m05:55:54.135585 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.135585 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:55:54.140644 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.140644 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Actual.7f6f830652
[0m05:55:54.140644 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:55:54.141618 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m05:55:54.141618 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:55:54.141618 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:55:54.141618 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Target.33e9c88d57"
[0m05:55:54.141618 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.141618 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:55:54.141618 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.141618 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Target.33e9c88d57
[0m05:55:54.141618 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:55:54.148437 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m05:55:54.148761 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:55:54.149031 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:55:54.152772 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Unit_Price.912d40d17b"
[0m05:55:54.155691 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.155691 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:55:54.156220 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.156632 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Unit_Price.912d40d17b
[0m05:55:54.156632 [debug] [Thread-1 (]: Began running node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:55:54.157455 [debug] [Thread-1 (]: Acquiring new databricks connection "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m05:55:54.157455 [debug] [Thread-1 (]: Began compiling node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:55:54.157455 [debug] [Thread-1 (]: Compiling test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:55:54.160535 [debug] [Thread-1 (]: Writing injected SQL for node "test.finance.not_null_dim_unit_Units_Sold.fd5939c596"
[0m05:55:54.160535 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.160535 [debug] [Thread-1 (]: Began executing node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:55:54.160535 [debug] [Thread-1 (]: finished collecting timing info
[0m05:55:54.160535 [debug] [Thread-1 (]: Finished running node test.finance.not_null_dim_unit_Units_Sold.fd5939c596
[0m05:55:54.164051 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:55:54.164051 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m05:55:54.164583 [debug] [MainThread]: Connection 'test.finance.not_null_dim_unit_Units_Sold.fd5939c596' was properly closed.
[0m05:55:54.170458 [info ] [MainThread]: Done.
[0m05:55:54.170458 [debug] [MainThread]: Acquiring new databricks connection "generate_catalog"
[0m05:55:54.170458 [info ] [MainThread]: Building catalog
[0m05:55:54.180744 [debug] [ThreadPool]: Acquiring new databricks connection "finance"
[0m05:55:54.180744 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation finance.dim_check_month
[0m05:55:54.180744 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation finance.dim_exp
[0m05:55:54.180744 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation finance.dim_info
[0m05:55:54.180744 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation finance.dim_month
[0m05:55:54.190736 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation finance.dim_profit
[0m05:55:54.191154 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation finance.dim_unit
[0m05:55:54.201276 [info ] [MainThread]: Catalog written to C:\Users\XavierDonBosco\Documents\dbt_project\finance\target\catalog.json
[0m05:55:54.201276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026610A53760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026610A538B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026610A528F0>]}
[0m05:55:54.201276 [debug] [MainThread]: Flushing usage events
[0m05:55:55.811057 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m05:55:55.811057 [debug] [MainThread]: Connection 'finance' was properly closed.


============================== 2022-11-28 05:56:06.130838 | 2596d8cf-3076-40e3-a1ca-17fde044a2ee ==============================
[0m05:56:06.130838 [info ] [MainThread]: Running with dbt=1.3.1
[0m05:56:06.130838 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'port': 8080, 'open_browser': True, 'which': 'serve', 'indirect_selection': 'eager'}
[0m05:56:06.130838 [debug] [MainThread]: Tracking: tracking
[0m05:56:06.142802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012E0A00DCC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012E0A00D8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012E0A00F310>]}
[0m05:56:06.146691 [info ] [MainThread]: Serving docs at 0.0.0.0:8080
[0m05:56:06.146691 [info ] [MainThread]: To access from your browser, navigate to:  http://localhost:8080
[0m05:56:06.149369 [info ] [MainThread]: 
[0m05:56:06.150414 [info ] [MainThread]: 
[0m05:56:06.151010 [info ] [MainThread]: Press Ctrl+C to exit.
[0m05:37:18.521970 [debug] [MainThread]: Flushing usage events
[0m05:37:20.312782 [info ] [MainThread]: ctrl-c


============================== 2022-11-30 05:37:39.669709 | 5156074d-70d8-45d5-970d-4cceb625b780 ==============================
[0m05:37:39.669709 [info ] [MainThread]: Running with dbt=1.3.1
[0m05:37:39.670705 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m05:37:39.670705 [debug] [MainThread]: Tracking: tracking
[0m05:37:39.675085 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D5A09C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D5A09870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D5A09AB0>]}
[0m05:37:40.203511 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m05:37:40.203511 [debug] [MainThread]: Partial parsing: added file: finance://models\fact_finance.sql
[0m05:37:40.219262 [debug] [MainThread]: 1699: static parser successfully parsed fact_finance.sql
[0m05:37:40.251221 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.finance.example

[0m05:37:40.256168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5156074d-70d8-45d5-970d-4cceb625b780', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D6C54CA0>]}
[0m05:37:40.292102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5156074d-70d8-45d5-970d-4cceb625b780', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D6C04B20>]}
[0m05:37:40.292102 [info ] [MainThread]: Found 7 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m05:37:40.307749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5156074d-70d8-45d5-970d-4cceb625b780', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D6C04550>]}
[0m05:37:40.308692 [info ] [MainThread]: 
[0m05:37:40.308692 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:37:40.324339 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m05:37:40.335400 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m05:37:40.335400 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m05:37:40.335400 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:40:26.935105 [debug] [ThreadPool]: SQL status: OK in 166.6 seconds
[0m05:40:26.946910 [debug] [ThreadPool]: On list_schemas: Close
[0m05:40:28.906802 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m05:40:28.923629 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m05:40:28.923629 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m05:40:28.923629 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m05:40:28.923629 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:40:34.087139 [debug] [ThreadPool]: SQL status: OK in 5.16 seconds
[0m05:40:34.093191 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m05:40:34.093191 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m05:40:34.093191 [debug] [ThreadPool]: On list_None_finance: Close
[0m05:40:35.080466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5156074d-70d8-45d5-970d-4cceb625b780', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D5BA73D0>]}
[0m05:40:35.080466 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:40:35.080466 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:40:35.080466 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m05:40:35.080466 [info ] [MainThread]: 
[0m05:40:35.096657 [debug] [Thread-1 (]: Began running node model.finance.dim_exp
[0m05:40:35.096657 [info ] [Thread-1 (]: 1 of 7 START sql table model finance.dim_exp ................................... [RUN]
[0m05:40:35.112417 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_exp"
[0m05:40:35.112417 [debug] [Thread-1 (]: Began compiling node model.finance.dim_exp
[0m05:40:35.112417 [debug] [Thread-1 (]: Compiling model.finance.dim_exp
[0m05:40:35.115743 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_exp"
[0m05:40:35.125911 [debug] [Thread-1 (]: finished collecting timing info
[0m05:40:35.126402 [debug] [Thread-1 (]: Began executing node model.finance.dim_exp
[0m05:40:35.191167 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_exp"
[0m05:40:35.191167 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:40:35.206883 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_exp"
[0m05:40:35.207063 [debug] [Thread-1 (]: On model.finance.dim_exp: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_exp"} */

  
    
      create or replace table finance.dim_exp
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Expenses, Gross_sales, Manufacturing_price from data.table_finance;
  
[0m05:40:35.207557 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m05:40:54.958369 [debug] [Thread-1 (]: SQL status: OK in 19.75 seconds
[0m05:40:54.980744 [debug] [Thread-1 (]: finished collecting timing info
[0m05:40:54.981302 [debug] [Thread-1 (]: On model.finance.dim_exp: ROLLBACK
[0m05:40:54.981302 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:40:54.982427 [debug] [Thread-1 (]: On model.finance.dim_exp: Close
[0m05:40:56.821546 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5156074d-70d8-45d5-970d-4cceb625b780', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D59C63B0>]}
[0m05:40:56.823534 [info ] [Thread-1 (]: 1 of 7 OK created sql table model finance.dim_exp .............................. [[32mOK[0m in 21.71s]
[0m05:40:56.826524 [debug] [Thread-1 (]: Finished running node model.finance.dim_exp
[0m05:40:56.827520 [debug] [Thread-1 (]: Began running node model.finance.dim_info
[0m05:40:56.827520 [info ] [Thread-1 (]: 2 of 7 START sql table model finance.dim_info .................................. [RUN]
[0m05:40:56.830555 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_info"
[0m05:40:56.831505 [debug] [Thread-1 (]: Began compiling node model.finance.dim_info
[0m05:40:56.832503 [debug] [Thread-1 (]: Compiling model.finance.dim_info
[0m05:40:56.837160 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_info"
[0m05:40:56.837160 [debug] [Thread-1 (]: finished collecting timing info
[0m05:40:56.837160 [debug] [Thread-1 (]: Began executing node model.finance.dim_info
[0m05:40:56.845986 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_info"
[0m05:40:56.845986 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:40:56.845986 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_info"
[0m05:40:56.862085 [debug] [Thread-1 (]: On model.finance.dim_info: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_info"} */

  
    
      create or replace table finance.dim_info
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, COGS, Inventory, Segment from data.table_finance;
  
[0m05:40:56.862706 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:41:01.677910 [debug] [Thread-1 (]: SQL status: OK in 4.82 seconds
[0m05:41:01.680182 [debug] [Thread-1 (]: finished collecting timing info
[0m05:41:01.680182 [debug] [Thread-1 (]: On model.finance.dim_info: ROLLBACK
[0m05:41:01.680182 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:41:01.680182 [debug] [Thread-1 (]: On model.finance.dim_info: Close
[0m05:41:02.613734 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5156074d-70d8-45d5-970d-4cceb625b780', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D59C5D20>]}
[0m05:41:02.614729 [info ] [Thread-1 (]: 2 of 7 OK created sql table model finance.dim_info ............................. [[32mOK[0m in 5.78s]
[0m05:41:02.615917 [debug] [Thread-1 (]: Finished running node model.finance.dim_info
[0m05:41:02.615917 [debug] [Thread-1 (]: Began running node model.finance.dim_month
[0m05:41:02.615917 [info ] [Thread-1 (]: 3 of 7 START sql table model finance.dim_month ................................. [RUN]
[0m05:41:02.615917 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_month"
[0m05:41:02.615917 [debug] [Thread-1 (]: Began compiling node model.finance.dim_month
[0m05:41:02.615917 [debug] [Thread-1 (]: Compiling model.finance.dim_month
[0m05:41:02.615917 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_month"
[0m05:41:02.633934 [debug] [Thread-1 (]: finished collecting timing info
[0m05:41:02.634933 [debug] [Thread-1 (]: Began executing node model.finance.dim_month
[0m05:41:02.641977 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_month"
[0m05:41:02.643595 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:41:02.643595 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_month"
[0m05:41:02.643595 [debug] [Thread-1 (]: On model.finance.dim_month: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_month"} */

  
    
      create or replace table finance.dim_month
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Month_Number, Year, MonthName, Date from data.table_finance;
  
[0m05:41:02.644629 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:41:07.654463 [debug] [Thread-1 (]: SQL status: OK in 5.01 seconds
[0m05:41:07.658455 [debug] [Thread-1 (]: finished collecting timing info
[0m05:41:07.659448 [debug] [Thread-1 (]: On model.finance.dim_month: ROLLBACK
[0m05:41:07.660368 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:41:07.660368 [debug] [Thread-1 (]: On model.finance.dim_month: Close
[0m05:41:09.017862 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5156074d-70d8-45d5-970d-4cceb625b780', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D59C5F30>]}
[0m05:41:09.019930 [info ] [Thread-1 (]: 3 of 7 OK created sql table model finance.dim_month ............................ [[32mOK[0m in 6.40s]
[0m05:41:09.022839 [debug] [Thread-1 (]: Finished running node model.finance.dim_month
[0m05:41:09.022839 [debug] [Thread-1 (]: Began running node model.finance.dim_profit
[0m05:41:09.023979 [info ] [Thread-1 (]: 4 of 7 START sql table model finance.dim_profit ................................ [RUN]
[0m05:41:09.026031 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_profit"
[0m05:41:09.027098 [debug] [Thread-1 (]: Began compiling node model.finance.dim_profit
[0m05:41:09.028021 [debug] [Thread-1 (]: Compiling model.finance.dim_profit
[0m05:41:09.033160 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_profit"
[0m05:41:09.033160 [debug] [Thread-1 (]: finished collecting timing info
[0m05:41:09.033160 [debug] [Thread-1 (]: Began executing node model.finance.dim_profit
[0m05:41:09.039675 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_profit"
[0m05:41:09.039675 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:41:09.039675 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_profit"
[0m05:41:09.039675 [debug] [Thread-1 (]: On model.finance.dim_profit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_profit"} */

  
    
      create or replace table finance.dim_profit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Discounts, Profit, Sales, Country, Discount_Band, Product from data.table_finance;
  
[0m05:41:09.055724 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:41:13.863967 [debug] [Thread-1 (]: SQL status: OK in 4.81 seconds
[0m05:41:13.866261 [debug] [Thread-1 (]: finished collecting timing info
[0m05:41:13.867191 [debug] [Thread-1 (]: On model.finance.dim_profit: ROLLBACK
[0m05:41:13.867353 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:41:13.867606 [debug] [Thread-1 (]: On model.finance.dim_profit: Close
[0m05:41:14.794888 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5156074d-70d8-45d5-970d-4cceb625b780', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D6D623E0>]}
[0m05:41:14.796883 [info ] [Thread-1 (]: 4 of 7 OK created sql table model finance.dim_profit ........................... [[32mOK[0m in 5.77s]
[0m05:41:14.800871 [debug] [Thread-1 (]: Finished running node model.finance.dim_profit
[0m05:41:14.801865 [debug] [Thread-1 (]: Began running node model.finance.dim_unit
[0m05:41:14.803167 [info ] [Thread-1 (]: 5 of 7 START sql table model finance.dim_unit .................................. [RUN]
[0m05:41:14.805658 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_unit"
[0m05:41:14.806655 [debug] [Thread-1 (]: Began compiling node model.finance.dim_unit
[0m05:41:14.807652 [debug] [Thread-1 (]: Compiling model.finance.dim_unit
[0m05:41:14.808890 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_unit"
[0m05:41:14.821514 [debug] [Thread-1 (]: finished collecting timing info
[0m05:41:14.821514 [debug] [Thread-1 (]: Began executing node model.finance.dim_unit
[0m05:41:14.827444 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_unit"
[0m05:41:14.829437 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:41:14.829437 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_unit"
[0m05:41:14.830364 [debug] [Thread-1 (]: On model.finance.dim_unit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_unit"} */

  
    
      create or replace table finance.dim_unit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Target, Unit_Price, Units_Sold from data.table_finance;
  
[0m05:41:14.830364 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:41:19.014243 [debug] [Thread-1 (]: SQL status: OK in 4.18 seconds
[0m05:41:19.033698 [debug] [Thread-1 (]: finished collecting timing info
[0m05:41:19.034700 [debug] [Thread-1 (]: On model.finance.dim_unit: ROLLBACK
[0m05:41:19.035684 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:41:19.035684 [debug] [Thread-1 (]: On model.finance.dim_unit: Close
[0m05:41:19.947582 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5156074d-70d8-45d5-970d-4cceb625b780', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D59C5F30>]}
[0m05:41:19.947582 [info ] [Thread-1 (]: 5 of 7 OK created sql table model finance.dim_unit ............................. [[32mOK[0m in 5.14s]
[0m05:41:19.963786 [debug] [Thread-1 (]: Finished running node model.finance.dim_unit
[0m05:41:19.964280 [debug] [Thread-1 (]: Began running node model.finance.fact_finance
[0m05:41:19.965284 [info ] [Thread-1 (]: 6 of 7 START sql table model finance.fact_finance .............................. [RUN]
[0m05:41:19.968355 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.fact_finance"
[0m05:41:19.969349 [debug] [Thread-1 (]: Began compiling node model.finance.fact_finance
[0m05:41:19.969349 [debug] [Thread-1 (]: Compiling model.finance.fact_finance
[0m05:41:19.976864 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.fact_finance"
[0m05:41:19.979150 [debug] [Thread-1 (]: finished collecting timing info
[0m05:41:19.979688 [debug] [Thread-1 (]: Began executing node model.finance.fact_finance
[0m05:41:19.980845 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.fact_finance"
[0m05:41:19.980845 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:41:19.980845 [debug] [Thread-1 (]: Using databricks connection "model.finance.fact_finance"
[0m05:41:19.980845 [debug] [Thread-1 (]: On model.finance.fact_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.fact_finance"} */

  
    
      create or replace table finance.fact_finance
    
    
    using delta
    
    
    
    
    
    
    as
      

select dim_exp.Actual, dim_exp.Expenses, dim_exp.Gross_sales, dim_exp.Manufacturing_price,
 dim_unit.Target, dim_unit.Unit_Price, dim_unit.Units_Sold,
 dim_month.Month_Number, dim_month.Year, dim_month.MonthName, dim_month.Date,
 dim_info.COGS, dim_info.Inventory, dim_info.Segment,
 dim_profit.Discounts, dim_profit.Profit, dim_profit.Sales, dim_profit.Country, dim_profit.Discount_Band, dim_profit.Product
 from dim_exp 
 join dim_unit on dim_exp.Actual = dim_unit.Actual
 join dim_month on dim_month.Actual = dim_unit.Actual
 join dim_info on dim_info.Actual = dim_unit.Actual
 join dim_profit on dim_profit.Actual = dim_unit.Actual
  
[0m05:41:19.980845 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:41:22.047079 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.fact_finance"} */

  
    
      create or replace table finance.fact_finance
    
    
    using delta
    
    
    
    
    
    
    as
      

select dim_exp.Actual, dim_exp.Expenses, dim_exp.Gross_sales, dim_exp.Manufacturing_price,
 dim_unit.Target, dim_unit.Unit_Price, dim_unit.Units_Sold,
 dim_month.Month_Number, dim_month.Year, dim_month.MonthName, dim_month.Date,
 dim_info.COGS, dim_info.Inventory, dim_info.Segment,
 dim_profit.Discounts, dim_profit.Profit, dim_profit.Sales, dim_profit.Country, dim_profit.Discount_Band, dim_profit.Product
 from dim_exp 
 join dim_unit on dim_exp.Actual = dim_unit.Actual
 join dim_month on dim_month.Actual = dim_unit.Actual
 join dim_info on dim_info.Actual = dim_unit.Actual
 join dim_profit on dim_profit.Actual = dim_unit.Actual
  
[0m05:41:22.053482 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: dim_exp; line 23 pos 6
[0m05:41:22.053482 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: dim_exp; line 23 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: dim_exp; line 23 pos 6
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m05:41:22.053980 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\x92\x81\xd3%\xd4\x19F\xcf\xbf\xb3\x16\xd9\xbb\xfe>\x96'
[0m05:41:22.054345 [debug] [Thread-1 (]: finished collecting timing info
[0m05:41:22.054345 [debug] [Thread-1 (]: On model.finance.fact_finance: ROLLBACK
[0m05:41:22.054345 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:41:22.054345 [debug] [Thread-1 (]: On model.finance.fact_finance: Close
[0m05:41:22.979137 [debug] [Thread-1 (]: Runtime Error in model fact_finance (models\fact_finance.sql)
  Table or view not found: dim_exp; line 23 pos 6
[0m05:41:22.979137 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5156074d-70d8-45d5-970d-4cceb625b780', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D6D01270>]}
[0m05:41:22.979137 [error] [Thread-1 (]: 6 of 7 ERROR creating sql table model finance.fact_finance ..................... [[31mERROR[0m in 3.01s]
[0m05:41:22.979137 [debug] [Thread-1 (]: Finished running node model.finance.fact_finance
[0m05:41:22.979137 [debug] [Thread-1 (]: Began running node model.finance.dim_check_month
[0m05:41:22.989426 [info ] [Thread-1 (]: 7 of 7 START sql table model finance.dim_check_month ........................... [RUN]
[0m05:41:22.990724 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_check_month"
[0m05:41:22.991230 [debug] [Thread-1 (]: Began compiling node model.finance.dim_check_month
[0m05:41:22.991731 [debug] [Thread-1 (]: Compiling model.finance.dim_check_month
[0m05:41:22.995815 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_check_month"
[0m05:41:22.999664 [debug] [Thread-1 (]: finished collecting timing info
[0m05:41:22.999836 [debug] [Thread-1 (]: Began executing node model.finance.dim_check_month
[0m05:41:23.006349 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_check_month"
[0m05:41:23.007890 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:41:23.008197 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_check_month"
[0m05:41:23.008595 [debug] [Thread-1 (]: On model.finance.dim_check_month: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_check_month"} */

  
    
      create or replace table finance.dim_check_month
    
    
    using delta
    
    
    
    
    
    
    as
      

select date from finance.dim_month where date > '2018-02-01' and date < '2018-03-01';
  
[0m05:41:23.008595 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:41:28.268463 [debug] [Thread-1 (]: SQL status: OK in 5.26 seconds
[0m05:41:28.268463 [debug] [Thread-1 (]: finished collecting timing info
[0m05:41:28.268463 [debug] [Thread-1 (]: On model.finance.dim_check_month: ROLLBACK
[0m05:41:28.268463 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:41:28.268463 [debug] [Thread-1 (]: On model.finance.dim_check_month: Close
[0m05:41:29.194858 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5156074d-70d8-45d5-970d-4cceb625b780', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D59C4E50>]}
[0m05:41:29.195862 [info ] [Thread-1 (]: 7 of 7 OK created sql table model finance.dim_check_month ...................... [[32mOK[0m in 6.20s]
[0m05:41:29.197852 [debug] [Thread-1 (]: Finished running node model.finance.dim_check_month
[0m05:41:29.200019 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:41:29.200019 [debug] [MainThread]: On master: ROLLBACK
[0m05:41:29.200019 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:41:30.138810 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:41:30.138810 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:41:30.138810 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:41:30.138810 [debug] [MainThread]: On master: ROLLBACK
[0m05:41:30.138810 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:41:30.138810 [debug] [MainThread]: On master: Close
[0m05:41:31.075171 [info ] [MainThread]: 
[0m05:41:31.075171 [info ] [MainThread]: Finished running 7 table models in 0 hours 3 minutes and 50.77 seconds (230.77s).
[0m05:41:31.084973 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:41:31.084973 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m05:41:31.084973 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m05:41:31.084973 [debug] [MainThread]: Connection 'model.finance.dim_check_month' was properly closed.
[0m05:41:31.108094 [info ] [MainThread]: 
[0m05:41:31.109784 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m05:41:31.109784 [info ] [MainThread]: 
[0m05:41:31.118744 [error] [MainThread]: [33mRuntime Error in model fact_finance (models\fact_finance.sql)[0m
[0m05:41:31.118744 [error] [MainThread]:   Table or view not found: dim_exp; line 23 pos 6
[0m05:41:31.144290 [info ] [MainThread]: 
[0m05:41:31.145290 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=1 SKIP=0 TOTAL=7
[0m05:41:31.148768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D6D922C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D6D90B20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202D6D91150>]}
[0m05:41:31.149917 [debug] [MainThread]: Flushing usage events


============================== 2022-11-30 05:45:32.337887 | 0c547bd5-3c6e-42cd-bccb-e58d2b9872b4 ==============================
[0m05:45:32.337887 [info ] [MainThread]: Running with dbt=1.3.1
[0m05:45:32.337887 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m05:45:32.337887 [debug] [MainThread]: Tracking: tracking
[0m05:45:32.363178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B36CDC9C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B36CDC9900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B36CDCB340>]}
[0m05:45:32.432690 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:45:32.432690 [debug] [MainThread]: Partial parsing: updated file: finance://models\fact_finance.sql
[0m05:45:32.440297 [debug] [MainThread]: 1699: static parser successfully parsed fact_finance.sql
[0m05:45:32.469668 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.finance.example

[0m05:45:32.469668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0c547bd5-3c6e-42cd-bccb-e58d2b9872b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B36E017FA0>]}
[0m05:45:32.516618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0c547bd5-3c6e-42cd-bccb-e58d2b9872b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B3578BEE60>]}
[0m05:45:32.520268 [info ] [MainThread]: Found 7 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m05:45:32.520268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c547bd5-3c6e-42cd-bccb-e58d2b9872b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B35D753FD0>]}
[0m05:45:32.520268 [info ] [MainThread]: 
[0m05:45:32.536019 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:45:32.536019 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m05:45:32.536019 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m05:45:32.536019 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m05:45:32.536019 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:45:34.733703 [debug] [ThreadPool]: SQL status: OK in 2.2 seconds
[0m05:45:34.735726 [debug] [ThreadPool]: On list_schemas: Close
[0m05:45:35.908510 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m05:45:35.908510 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m05:45:35.908510 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m05:45:35.908510 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m05:45:35.908510 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:45:38.576997 [debug] [ThreadPool]: SQL status: OK in 2.67 seconds
[0m05:45:38.592628 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m05:45:38.592628 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m05:45:38.592628 [debug] [ThreadPool]: On list_None_finance: Close
[0m05:45:39.694670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c547bd5-3c6e-42cd-bccb-e58d2b9872b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B36CF673D0>]}
[0m05:45:39.694670 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:45:39.694670 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:45:39.694670 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m05:45:39.694670 [info ] [MainThread]: 
[0m05:45:39.702836 [debug] [Thread-1 (]: Began running node model.finance.dim_exp
[0m05:45:39.702836 [info ] [Thread-1 (]: 1 of 7 START sql table model finance.dim_exp ................................... [RUN]
[0m05:45:39.702836 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_exp"
[0m05:45:39.702836 [debug] [Thread-1 (]: Began compiling node model.finance.dim_exp
[0m05:45:39.702836 [debug] [Thread-1 (]: Compiling model.finance.dim_exp
[0m05:45:39.719128 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_exp"
[0m05:45:39.719128 [debug] [Thread-1 (]: finished collecting timing info
[0m05:45:39.721472 [debug] [Thread-1 (]: Began executing node model.finance.dim_exp
[0m05:45:39.753301 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_exp"
[0m05:45:39.753301 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:45:39.753301 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_exp"
[0m05:45:39.753301 [debug] [Thread-1 (]: On model.finance.dim_exp: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_exp"} */

  
    
      create or replace table finance.dim_exp
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Expenses, Gross_sales, Manufacturing_price from data.table_finance;
  
[0m05:45:39.766855 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m05:45:43.778791 [debug] [Thread-1 (]: SQL status: OK in 4.01 seconds
[0m05:45:43.778791 [debug] [Thread-1 (]: finished collecting timing info
[0m05:45:43.778791 [debug] [Thread-1 (]: On model.finance.dim_exp: ROLLBACK
[0m05:45:43.778791 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:45:43.778791 [debug] [Thread-1 (]: On model.finance.dim_exp: Close
[0m05:45:44.692325 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c547bd5-3c6e-42cd-bccb-e58d2b9872b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B36CD871F0>]}
[0m05:45:44.692325 [info ] [Thread-1 (]: 1 of 7 OK created sql table model finance.dim_exp .............................. [[32mOK[0m in 4.99s]
[0m05:45:44.692325 [debug] [Thread-1 (]: Finished running node model.finance.dim_exp
[0m05:45:44.692325 [debug] [Thread-1 (]: Began running node model.finance.dim_info
[0m05:45:44.692325 [info ] [Thread-1 (]: 2 of 7 START sql table model finance.dim_info .................................. [RUN]
[0m05:45:44.692325 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_info"
[0m05:45:44.692325 [debug] [Thread-1 (]: Began compiling node model.finance.dim_info
[0m05:45:44.692325 [debug] [Thread-1 (]: Compiling model.finance.dim_info
[0m05:45:44.707935 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_info"
[0m05:45:44.707935 [debug] [Thread-1 (]: finished collecting timing info
[0m05:45:44.707935 [debug] [Thread-1 (]: Began executing node model.finance.dim_info
[0m05:45:44.707935 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_info"
[0m05:45:44.707935 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:45:44.707935 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_info"
[0m05:45:44.707935 [debug] [Thread-1 (]: On model.finance.dim_info: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_info"} */

  
    
      create or replace table finance.dim_info
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, COGS, Inventory, Segment from data.table_finance;
  
[0m05:45:44.707935 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:45:49.133638 [debug] [Thread-1 (]: SQL status: OK in 4.43 seconds
[0m05:45:49.133638 [debug] [Thread-1 (]: finished collecting timing info
[0m05:45:49.133638 [debug] [Thread-1 (]: On model.finance.dim_info: ROLLBACK
[0m05:45:49.133638 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:45:49.133638 [debug] [Thread-1 (]: On model.finance.dim_info: Close
[0m05:45:50.069394 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c547bd5-3c6e-42cd-bccb-e58d2b9872b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B36CD87460>]}
[0m05:45:50.069394 [info ] [Thread-1 (]: 2 of 7 OK created sql table model finance.dim_info ............................. [[32mOK[0m in 5.38s]
[0m05:45:50.069394 [debug] [Thread-1 (]: Finished running node model.finance.dim_info
[0m05:45:50.069394 [debug] [Thread-1 (]: Began running node model.finance.dim_month
[0m05:45:50.069394 [info ] [Thread-1 (]: 3 of 7 START sql table model finance.dim_month ................................. [RUN]
[0m05:45:50.078358 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_month"
[0m05:45:50.078358 [debug] [Thread-1 (]: Began compiling node model.finance.dim_month
[0m05:45:50.078358 [debug] [Thread-1 (]: Compiling model.finance.dim_month
[0m05:45:50.079364 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_month"
[0m05:45:50.079364 [debug] [Thread-1 (]: finished collecting timing info
[0m05:45:50.083106 [debug] [Thread-1 (]: Began executing node model.finance.dim_month
[0m05:45:50.083354 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_month"
[0m05:45:50.087128 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:45:50.087371 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_month"
[0m05:45:50.087566 [debug] [Thread-1 (]: On model.finance.dim_month: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_month"} */

  
    
      create or replace table finance.dim_month
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Month_Number, Year, MonthName, Date from data.table_finance;
  
[0m05:45:50.087566 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:45:53.865262 [debug] [Thread-1 (]: SQL status: OK in 3.78 seconds
[0m05:45:53.865262 [debug] [Thread-1 (]: finished collecting timing info
[0m05:45:53.865262 [debug] [Thread-1 (]: On model.finance.dim_month: ROLLBACK
[0m05:45:53.865262 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:45:53.865262 [debug] [Thread-1 (]: On model.finance.dim_month: Close
[0m05:45:54.787249 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c547bd5-3c6e-42cd-bccb-e58d2b9872b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B36CD84F10>]}
[0m05:45:54.787249 [info ] [Thread-1 (]: 3 of 7 OK created sql table model finance.dim_month ............................ [[32mOK[0m in 4.72s]
[0m05:45:54.793758 [debug] [Thread-1 (]: Finished running node model.finance.dim_month
[0m05:45:54.794865 [debug] [Thread-1 (]: Began running node model.finance.dim_profit
[0m05:45:54.795123 [info ] [Thread-1 (]: 4 of 7 START sql table model finance.dim_profit ................................ [RUN]
[0m05:45:54.795123 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_profit"
[0m05:45:54.796651 [debug] [Thread-1 (]: Began compiling node model.finance.dim_profit
[0m05:45:54.796707 [debug] [Thread-1 (]: Compiling model.finance.dim_profit
[0m05:45:54.797077 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_profit"
[0m05:45:54.797077 [debug] [Thread-1 (]: finished collecting timing info
[0m05:45:54.797077 [debug] [Thread-1 (]: Began executing node model.finance.dim_profit
[0m05:45:54.797077 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_profit"
[0m05:45:54.797077 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:45:54.797077 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_profit"
[0m05:45:54.797077 [debug] [Thread-1 (]: On model.finance.dim_profit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_profit"} */

  
    
      create or replace table finance.dim_profit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Discounts, Profit, Sales, Country, Discount_Band, Product from data.table_finance;
  
[0m05:45:54.805145 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:45:58.554879 [debug] [Thread-1 (]: SQL status: OK in 3.75 seconds
[0m05:45:58.555872 [debug] [Thread-1 (]: finished collecting timing info
[0m05:45:58.555872 [debug] [Thread-1 (]: On model.finance.dim_profit: ROLLBACK
[0m05:45:58.557595 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:45:58.557756 [debug] [Thread-1 (]: On model.finance.dim_profit: Close
[0m05:45:59.481277 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c547bd5-3c6e-42cd-bccb-e58d2b9872b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B36E11A2F0>]}
[0m05:45:59.481277 [info ] [Thread-1 (]: 4 of 7 OK created sql table model finance.dim_profit ........................... [[32mOK[0m in 4.69s]
[0m05:45:59.481277 [debug] [Thread-1 (]: Finished running node model.finance.dim_profit
[0m05:45:59.481277 [debug] [Thread-1 (]: Began running node model.finance.dim_unit
[0m05:45:59.493670 [info ] [Thread-1 (]: 5 of 7 START sql table model finance.dim_unit .................................. [RUN]
[0m05:45:59.493670 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_unit"
[0m05:45:59.493670 [debug] [Thread-1 (]: Began compiling node model.finance.dim_unit
[0m05:45:59.493670 [debug] [Thread-1 (]: Compiling model.finance.dim_unit
[0m05:45:59.493670 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_unit"
[0m05:45:59.493670 [debug] [Thread-1 (]: finished collecting timing info
[0m05:45:59.493670 [debug] [Thread-1 (]: Began executing node model.finance.dim_unit
[0m05:45:59.503110 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_unit"
[0m05:45:59.503110 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:45:59.503110 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_unit"
[0m05:45:59.503110 [debug] [Thread-1 (]: On model.finance.dim_unit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_unit"} */

  
    
      create or replace table finance.dim_unit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Target, Unit_Price, Units_Sold from data.table_finance;
  
[0m05:45:59.503110 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:46:03.371461 [debug] [Thread-1 (]: SQL status: OK in 3.87 seconds
[0m05:46:03.371461 [debug] [Thread-1 (]: finished collecting timing info
[0m05:46:03.371461 [debug] [Thread-1 (]: On model.finance.dim_unit: ROLLBACK
[0m05:46:03.371461 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:46:03.371461 [debug] [Thread-1 (]: On model.finance.dim_unit: Close
[0m05:46:04.315525 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c547bd5-3c6e-42cd-bccb-e58d2b9872b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B36CD84F10>]}
[0m05:46:04.315525 [info ] [Thread-1 (]: 5 of 7 OK created sql table model finance.dim_unit ............................. [[32mOK[0m in 4.82s]
[0m05:46:04.315525 [debug] [Thread-1 (]: Finished running node model.finance.dim_unit
[0m05:46:04.322347 [debug] [Thread-1 (]: Began running node model.finance.fact_finance
[0m05:46:04.322746 [info ] [Thread-1 (]: 6 of 7 START sql table model finance.fact_finance .............................. [RUN]
[0m05:46:04.323054 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.fact_finance"
[0m05:46:04.323054 [debug] [Thread-1 (]: Began compiling node model.finance.fact_finance
[0m05:46:04.323054 [debug] [Thread-1 (]: Compiling model.finance.fact_finance
[0m05:46:04.324678 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.fact_finance"
[0m05:46:04.324678 [debug] [Thread-1 (]: finished collecting timing info
[0m05:46:04.324678 [debug] [Thread-1 (]: Began executing node model.finance.fact_finance
[0m05:46:04.324678 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.fact_finance"
[0m05:46:04.324678 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:46:04.324678 [debug] [Thread-1 (]: Using databricks connection "model.finance.fact_finance"
[0m05:46:04.324678 [debug] [Thread-1 (]: On model.finance.fact_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.fact_finance"} */

  
    
      create or replace table finance.fact_finance
    
    
    using delta
    
    
    
    
    
    
    as
      

select dim_exp.Actual, dim_exp.Expenses, dim_exp.Gross_sales, dim_exp.Manufacturing_price,
 dim_unit.Target, dim_unit.Unit_Price, dim_unit.Units_Sold
 from dim_exp 
 join dim_unit on dim_exp.Actual = dim_unit.Actual
  
[0m05:46:04.324678 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:46:06.317536 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.fact_finance"} */

  
    
      create or replace table finance.fact_finance
    
    
    using delta
    
    
    
    
    
    
    as
      

select dim_exp.Actual, dim_exp.Expenses, dim_exp.Gross_sales, dim_exp.Manufacturing_price,
 dim_unit.Target, dim_unit.Unit_Price, dim_unit.Units_Sold
 from dim_exp 
 join dim_unit on dim_exp.Actual = dim_unit.Actual
  
[0m05:46:06.317536 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: dim_exp; line 20 pos 6
[0m05:46:06.317536 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: dim_exp; line 20 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: dim_exp; line 20 pos 6
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m05:46:06.317536 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'C\x87\x1c%\x01QG\xb4\x8b\xaa\xecp\xf9\x83\xa2\n'
[0m05:46:06.317536 [debug] [Thread-1 (]: finished collecting timing info
[0m05:46:06.317536 [debug] [Thread-1 (]: On model.finance.fact_finance: ROLLBACK
[0m05:46:06.317536 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:46:06.317536 [debug] [Thread-1 (]: On model.finance.fact_finance: Close
[0m05:46:07.250439 [debug] [Thread-1 (]: Runtime Error in model fact_finance (models\fact_finance.sql)
  Table or view not found: dim_exp; line 20 pos 6
[0m05:46:07.266234 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c547bd5-3c6e-42cd-bccb-e58d2b9872b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B36CD859C0>]}
[0m05:46:07.267637 [error] [Thread-1 (]: 6 of 7 ERROR creating sql table model finance.fact_finance ..................... [[31mERROR[0m in 2.94s]
[0m05:46:07.268268 [debug] [Thread-1 (]: Finished running node model.finance.fact_finance
[0m05:46:07.270110 [debug] [Thread-1 (]: Began running node model.finance.dim_check_month
[0m05:46:07.271238 [info ] [Thread-1 (]: 7 of 7 START sql table model finance.dim_check_month ........................... [RUN]
[0m05:46:07.272654 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_check_month"
[0m05:46:07.273061 [debug] [Thread-1 (]: Began compiling node model.finance.dim_check_month
[0m05:46:07.273560 [debug] [Thread-1 (]: Compiling model.finance.dim_check_month
[0m05:46:07.273560 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_check_month"
[0m05:46:07.278294 [debug] [Thread-1 (]: finished collecting timing info
[0m05:46:07.278550 [debug] [Thread-1 (]: Began executing node model.finance.dim_check_month
[0m05:46:07.278726 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_check_month"
[0m05:46:07.278726 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:46:07.278726 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_check_month"
[0m05:46:07.278726 [debug] [Thread-1 (]: On model.finance.dim_check_month: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_check_month"} */

  
    
      create or replace table finance.dim_check_month
    
    
    using delta
    
    
    
    
    
    
    as
      

select date from finance.dim_month where date > '2018-02-01' and date < '2018-03-01';
  
[0m05:46:07.278726 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:46:11.807550 [debug] [Thread-1 (]: SQL status: OK in 4.53 seconds
[0m05:46:11.823058 [debug] [Thread-1 (]: finished collecting timing info
[0m05:46:11.823058 [debug] [Thread-1 (]: On model.finance.dim_check_month: ROLLBACK
[0m05:46:11.823058 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:46:11.827065 [debug] [Thread-1 (]: On model.finance.dim_check_month: Close
[0m05:46:12.728060 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c547bd5-3c6e-42cd-bccb-e58d2b9872b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B36E0DA9E0>]}
[0m05:46:12.728060 [info ] [Thread-1 (]: 7 of 7 OK created sql table model finance.dim_check_month ...................... [[32mOK[0m in 5.46s]
[0m05:46:12.728060 [debug] [Thread-1 (]: Finished running node model.finance.dim_check_month
[0m05:46:12.728060 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:46:12.728060 [debug] [MainThread]: On master: ROLLBACK
[0m05:46:12.728060 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:46:13.674969 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:46:13.674969 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:46:13.674969 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:46:13.674969 [debug] [MainThread]: On master: ROLLBACK
[0m05:46:13.681890 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:46:13.682463 [debug] [MainThread]: On master: Close
[0m05:46:14.603106 [info ] [MainThread]: 
[0m05:46:14.603106 [info ] [MainThread]: Finished running 7 table models in 0 hours 0 minutes and 42.07 seconds (42.07s).
[0m05:46:14.603106 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:46:14.603106 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m05:46:14.618165 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m05:46:14.618165 [debug] [MainThread]: Connection 'model.finance.dim_check_month' was properly closed.
[0m05:46:14.618165 [info ] [MainThread]: 
[0m05:46:14.618165 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m05:46:14.631657 [info ] [MainThread]: 
[0m05:46:14.631657 [error] [MainThread]: [33mRuntime Error in model fact_finance (models\fact_finance.sql)[0m
[0m05:46:14.647306 [error] [MainThread]:   Table or view not found: dim_exp; line 20 pos 6
[0m05:46:14.647306 [info ] [MainThread]: 
[0m05:46:14.669092 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=1 SKIP=0 TOTAL=7
[0m05:46:14.669092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B36E14C130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B36E14DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B36E14F130>]}
[0m05:46:14.669092 [debug] [MainThread]: Flushing usage events


============================== 2022-11-30 05:46:57.001540 | 65c9bcba-050e-4e54-a28f-65c550ddc2a7 ==============================
[0m05:46:57.001540 [info ] [MainThread]: Running with dbt=1.3.1
[0m05:46:57.001540 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m05:46:57.001540 [debug] [MainThread]: Tracking: tracking
[0m05:46:57.018533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0183A9C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0183A9870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0183A9AB0>]}
[0m05:46:57.088108 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:46:57.088108 [debug] [MainThread]: Partial parsing: updated file: finance://models\fact_finance.sql
[0m05:46:57.103938 [debug] [MainThread]: 1699: static parser successfully parsed fact_finance.sql
[0m05:46:57.119805 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.finance.example

[0m05:46:57.119805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '65c9bcba-050e-4e54-a28f-65c550ddc2a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B018627FA0>]}
[0m05:46:57.168645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '65c9bcba-050e-4e54-a28f-65c550ddc2a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B002ECEE60>]}
[0m05:46:57.168645 [info ] [MainThread]: Found 7 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m05:46:57.168645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '65c9bcba-050e-4e54-a28f-65c550ddc2a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B008D43940>]}
[0m05:46:57.185542 [info ] [MainThread]: 
[0m05:46:57.192124 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:46:57.192124 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m05:46:57.192124 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m05:46:57.208338 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m05:46:57.208338 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:46:59.221060 [debug] [ThreadPool]: SQL status: OK in 2.01 seconds
[0m05:46:59.221060 [debug] [ThreadPool]: On list_schemas: Close
[0m05:47:00.257213 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m05:47:00.257213 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:00.257213 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m05:47:00.257213 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m05:47:00.257213 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:47:02.811823 [debug] [ThreadPool]: SQL status: OK in 2.55 seconds
[0m05:47:02.816070 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m05:47:02.816070 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m05:47:02.818592 [debug] [ThreadPool]: On list_None_finance: Close
[0m05:47:03.718240 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '65c9bcba-050e-4e54-a28f-65c550ddc2a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B018547460>]}
[0m05:47:03.718240 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:03.718240 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:47:03.718240 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m05:47:03.718240 [info ] [MainThread]: 
[0m05:47:03.729059 [debug] [Thread-1 (]: Began running node model.finance.dim_exp
[0m05:47:03.729059 [info ] [Thread-1 (]: 1 of 7 START sql table model finance.dim_exp ................................... [RUN]
[0m05:47:03.729059 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_exp"
[0m05:47:03.729059 [debug] [Thread-1 (]: Began compiling node model.finance.dim_exp
[0m05:47:03.729059 [debug] [Thread-1 (]: Compiling model.finance.dim_exp
[0m05:47:03.734180 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_exp"
[0m05:47:03.734180 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:03.734180 [debug] [Thread-1 (]: Began executing node model.finance.dim_exp
[0m05:47:03.771768 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_exp"
[0m05:47:03.771768 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:03.779372 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_exp"
[0m05:47:03.779372 [debug] [Thread-1 (]: On model.finance.dim_exp: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_exp"} */

  
    
      create or replace table finance.dim_exp
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Expenses, Gross_sales, Manufacturing_price from data.table_finance;
  
[0m05:47:03.779372 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m05:47:07.663739 [debug] [Thread-1 (]: SQL status: OK in 3.88 seconds
[0m05:47:07.679325 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:07.679325 [debug] [Thread-1 (]: On model.finance.dim_exp: ROLLBACK
[0m05:47:07.679325 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:07.679325 [debug] [Thread-1 (]: On model.finance.dim_exp: Close
[0m05:47:08.599488 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '65c9bcba-050e-4e54-a28f-65c550ddc2a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B008D88A30>]}
[0m05:47:08.599488 [info ] [Thread-1 (]: 1 of 7 OK created sql table model finance.dim_exp .............................. [[32mOK[0m in 4.87s]
[0m05:47:08.599488 [debug] [Thread-1 (]: Finished running node model.finance.dim_exp
[0m05:47:08.604659 [debug] [Thread-1 (]: Began running node model.finance.dim_info
[0m05:47:08.604979 [info ] [Thread-1 (]: 2 of 7 START sql table model finance.dim_info .................................. [RUN]
[0m05:47:08.605660 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_info"
[0m05:47:08.605660 [debug] [Thread-1 (]: Began compiling node model.finance.dim_info
[0m05:47:08.605660 [debug] [Thread-1 (]: Compiling model.finance.dim_info
[0m05:47:08.606899 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_info"
[0m05:47:08.606899 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:08.606899 [debug] [Thread-1 (]: Began executing node model.finance.dim_info
[0m05:47:08.611210 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_info"
[0m05:47:08.615268 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:08.615268 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_info"
[0m05:47:08.616075 [debug] [Thread-1 (]: On model.finance.dim_info: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_info"} */

  
    
      create or replace table finance.dim_info
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, COGS, Inventory, Segment from data.table_finance;
  
[0m05:47:08.616075 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:47:12.321146 [debug] [Thread-1 (]: SQL status: OK in 3.71 seconds
[0m05:47:12.321146 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:12.321146 [debug] [Thread-1 (]: On model.finance.dim_info: ROLLBACK
[0m05:47:12.321146 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:12.321146 [debug] [Thread-1 (]: On model.finance.dim_info: Close
[0m05:47:13.248422 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '65c9bcba-050e-4e54-a28f-65c550ddc2a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0185B26B0>]}
[0m05:47:13.248422 [info ] [Thread-1 (]: 2 of 7 OK created sql table model finance.dim_info ............................. [[32mOK[0m in 4.64s]
[0m05:47:13.248422 [debug] [Thread-1 (]: Finished running node model.finance.dim_info
[0m05:47:13.248422 [debug] [Thread-1 (]: Began running node model.finance.dim_month
[0m05:47:13.253433 [info ] [Thread-1 (]: 3 of 7 START sql table model finance.dim_month ................................. [RUN]
[0m05:47:13.253433 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_month"
[0m05:47:13.253433 [debug] [Thread-1 (]: Began compiling node model.finance.dim_month
[0m05:47:13.253433 [debug] [Thread-1 (]: Compiling model.finance.dim_month
[0m05:47:13.253433 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_month"
[0m05:47:13.253433 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:13.253433 [debug] [Thread-1 (]: Began executing node model.finance.dim_month
[0m05:47:13.269071 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_month"
[0m05:47:13.269071 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:13.269071 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_month"
[0m05:47:13.269071 [debug] [Thread-1 (]: On model.finance.dim_month: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_month"} */

  
    
      create or replace table finance.dim_month
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Month_Number, Year, MonthName, Date from data.table_finance;
  
[0m05:47:13.269071 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:47:17.673082 [debug] [Thread-1 (]: SQL status: OK in 4.4 seconds
[0m05:47:17.691311 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:17.692303 [debug] [Thread-1 (]: On model.finance.dim_month: ROLLBACK
[0m05:47:17.692303 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:17.693378 [debug] [Thread-1 (]: On model.finance.dim_month: Close
[0m05:47:18.640285 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '65c9bcba-050e-4e54-a28f-65c550ddc2a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B018365090>]}
[0m05:47:18.640285 [info ] [Thread-1 (]: 3 of 7 OK created sql table model finance.dim_month ............................ [[32mOK[0m in 5.39s]
[0m05:47:18.640285 [debug] [Thread-1 (]: Finished running node model.finance.dim_month
[0m05:47:18.640285 [debug] [Thread-1 (]: Began running node model.finance.dim_profit
[0m05:47:18.640285 [info ] [Thread-1 (]: 4 of 7 START sql table model finance.dim_profit ................................ [RUN]
[0m05:47:18.657047 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_profit"
[0m05:47:18.657804 [debug] [Thread-1 (]: Began compiling node model.finance.dim_profit
[0m05:47:18.657804 [debug] [Thread-1 (]: Compiling model.finance.dim_profit
[0m05:47:18.665821 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_profit"
[0m05:47:18.667216 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:18.668371 [debug] [Thread-1 (]: Began executing node model.finance.dim_profit
[0m05:47:18.673442 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_profit"
[0m05:47:18.673442 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:18.673442 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_profit"
[0m05:47:18.673442 [debug] [Thread-1 (]: On model.finance.dim_profit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_profit"} */

  
    
      create or replace table finance.dim_profit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Discounts, Profit, Sales, Country, Discount_Band, Product from data.table_finance;
  
[0m05:47:18.673442 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:47:22.763333 [debug] [Thread-1 (]: SQL status: OK in 4.09 seconds
[0m05:47:22.767320 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:22.768318 [debug] [Thread-1 (]: On model.finance.dim_profit: ROLLBACK
[0m05:47:22.769313 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:22.769313 [debug] [Thread-1 (]: On model.finance.dim_profit: Close
[0m05:47:23.684796 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '65c9bcba-050e-4e54-a28f-65c550ddc2a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B01872E380>]}
[0m05:47:23.684796 [info ] [Thread-1 (]: 4 of 7 OK created sql table model finance.dim_profit ........................... [[32mOK[0m in 5.03s]
[0m05:47:23.684796 [debug] [Thread-1 (]: Finished running node model.finance.dim_profit
[0m05:47:23.684796 [debug] [Thread-1 (]: Began running node model.finance.dim_unit
[0m05:47:23.684796 [info ] [Thread-1 (]: 5 of 7 START sql table model finance.dim_unit .................................. [RUN]
[0m05:47:23.700891 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_unit"
[0m05:47:23.700891 [debug] [Thread-1 (]: Began compiling node model.finance.dim_unit
[0m05:47:23.702636 [debug] [Thread-1 (]: Compiling model.finance.dim_unit
[0m05:47:23.702636 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_unit"
[0m05:47:23.702636 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:23.702636 [debug] [Thread-1 (]: Began executing node model.finance.dim_unit
[0m05:47:23.718321 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_unit"
[0m05:47:23.718321 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:23.718321 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_unit"
[0m05:47:23.718321 [debug] [Thread-1 (]: On model.finance.dim_unit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_unit"} */

  
    
      create or replace table finance.dim_unit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Target, Unit_Price, Units_Sold from data.table_finance;
  
[0m05:47:23.727631 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:47:27.970729 [debug] [Thread-1 (]: SQL status: OK in 4.24 seconds
[0m05:47:27.975125 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:27.975125 [debug] [Thread-1 (]: On model.finance.dim_unit: ROLLBACK
[0m05:47:27.975125 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:27.975125 [debug] [Thread-1 (]: On model.finance.dim_unit: Close
[0m05:47:28.904905 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '65c9bcba-050e-4e54-a28f-65c550ddc2a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0186D3580>]}
[0m05:47:28.904905 [info ] [Thread-1 (]: 5 of 7 OK created sql table model finance.dim_unit ............................. [[32mOK[0m in 5.20s]
[0m05:47:28.904905 [debug] [Thread-1 (]: Finished running node model.finance.dim_unit
[0m05:47:28.904905 [debug] [Thread-1 (]: Began running node model.finance.fact_finance
[0m05:47:28.904905 [info ] [Thread-1 (]: 6 of 7 START sql table model finance.fact_finance .............................. [RUN]
[0m05:47:28.904905 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.fact_finance"
[0m05:47:28.904905 [debug] [Thread-1 (]: Began compiling node model.finance.fact_finance
[0m05:47:28.904905 [debug] [Thread-1 (]: Compiling model.finance.fact_finance
[0m05:47:28.920428 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.fact_finance"
[0m05:47:28.920428 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:28.920428 [debug] [Thread-1 (]: Began executing node model.finance.fact_finance
[0m05:47:28.936133 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.fact_finance"
[0m05:47:28.940686 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:28.941009 [debug] [Thread-1 (]: Using databricks connection "model.finance.fact_finance"
[0m05:47:28.941009 [debug] [Thread-1 (]: On model.finance.fact_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.fact_finance"} */

  
    
      create or replace table finance.fact_finance
    
    
    using delta
    
    
    
    
    
    
    as
      

select dim_exp.Actual, dim_exp.Expenses, dim_exp.Gross_sales, dim_exp.Manufacturing_price,
 dim_unit.Target, dim_unit.Unit_Price, dim_unit.Units_Sold
 from dim_exp 
 join dim_unit on dim_exp.Actual = dim_unit.Actual;
  
[0m05:47:28.941009 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:47:30.909159 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.fact_finance"} */

  
    
      create or replace table finance.fact_finance
    
    
    using delta
    
    
    
    
    
    
    as
      

select dim_exp.Actual, dim_exp.Expenses, dim_exp.Gross_sales, dim_exp.Manufacturing_price,
 dim_unit.Target, dim_unit.Unit_Price, dim_unit.Units_Sold
 from dim_exp 
 join dim_unit on dim_exp.Actual = dim_unit.Actual;
  
[0m05:47:30.909159 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: dim_exp; line 20 pos 6
[0m05:47:30.909159 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: dim_exp; line 20 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: dim_exp; line 20 pos 6
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:128)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:437)
	... 19 more

[0m05:47:30.909159 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\xab\x12\xb4\xdaV\x17G\x16\x93#d*\xb4T\xfae'
[0m05:47:30.924774 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:30.924774 [debug] [Thread-1 (]: On model.finance.fact_finance: ROLLBACK
[0m05:47:30.924774 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:30.924774 [debug] [Thread-1 (]: On model.finance.fact_finance: Close
[0m05:47:31.833255 [debug] [Thread-1 (]: Runtime Error in model fact_finance (models\fact_finance.sql)
  Table or view not found: dim_exp; line 20 pos 6
[0m05:47:31.848905 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '65c9bcba-050e-4e54-a28f-65c550ddc2a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B018366EF0>]}
[0m05:47:31.848905 [error] [Thread-1 (]: 6 of 7 ERROR creating sql table model finance.fact_finance ..................... [[31mERROR[0m in 2.94s]
[0m05:47:31.848905 [debug] [Thread-1 (]: Finished running node model.finance.fact_finance
[0m05:47:31.848905 [debug] [Thread-1 (]: Began running node model.finance.dim_check_month
[0m05:47:31.848905 [info ] [Thread-1 (]: 7 of 7 START sql table model finance.dim_check_month ........................... [RUN]
[0m05:47:31.848905 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_check_month"
[0m05:47:31.848905 [debug] [Thread-1 (]: Began compiling node model.finance.dim_check_month
[0m05:47:31.857277 [debug] [Thread-1 (]: Compiling model.finance.dim_check_month
[0m05:47:31.857277 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_check_month"
[0m05:47:31.857277 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:31.857277 [debug] [Thread-1 (]: Began executing node model.finance.dim_check_month
[0m05:47:31.869610 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_check_month"
[0m05:47:31.881200 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:31.882175 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_check_month"
[0m05:47:31.882175 [debug] [Thread-1 (]: On model.finance.dim_check_month: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_check_month"} */

  
    
      create or replace table finance.dim_check_month
    
    
    using delta
    
    
    
    
    
    
    as
      

select date from finance.dim_month where date > '2018-02-01' and date < '2018-03-01';
  
[0m05:47:31.882175 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:47:36.267008 [debug] [Thread-1 (]: SQL status: OK in 4.38 seconds
[0m05:47:36.268245 [debug] [Thread-1 (]: finished collecting timing info
[0m05:47:36.268245 [debug] [Thread-1 (]: On model.finance.dim_check_month: ROLLBACK
[0m05:47:36.268245 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m05:47:36.273852 [debug] [Thread-1 (]: On model.finance.dim_check_month: Close
[0m05:47:37.279353 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '65c9bcba-050e-4e54-a28f-65c550ddc2a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B018547D90>]}
[0m05:47:37.280371 [info ] [Thread-1 (]: 7 of 7 OK created sql table model finance.dim_check_month ...................... [[32mOK[0m in 5.43s]
[0m05:47:37.283049 [debug] [Thread-1 (]: Finished running node model.finance.dim_check_month
[0m05:47:37.285132 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m05:47:37.286117 [debug] [MainThread]: On master: ROLLBACK
[0m05:47:37.286117 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:47:38.430818 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:47:38.430818 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:38.430818 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:47:38.430818 [debug] [MainThread]: On master: ROLLBACK
[0m05:47:38.430818 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m05:47:38.430818 [debug] [MainThread]: On master: Close
[0m05:47:39.518743 [info ] [MainThread]: 
[0m05:47:39.518743 [info ] [MainThread]: Finished running 7 table models in 0 hours 0 minutes and 42.33 seconds (42.33s).
[0m05:47:39.518743 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:47:39.518743 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m05:47:39.518743 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m05:47:39.518743 [debug] [MainThread]: Connection 'model.finance.dim_check_month' was properly closed.
[0m05:47:39.549948 [info ] [MainThread]: 
[0m05:47:39.549948 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m05:47:39.549948 [info ] [MainThread]: 
[0m05:47:39.565927 [error] [MainThread]: [33mRuntime Error in model fact_finance (models\fact_finance.sql)[0m
[0m05:47:39.566327 [error] [MainThread]:   Table or view not found: dim_exp; line 20 pos 6
[0m05:47:39.566327 [info ] [MainThread]: 
[0m05:47:39.566327 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=1 SKIP=0 TOTAL=7
[0m05:47:39.566327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0185142B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B018763DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0187634C0>]}
[0m05:47:39.566327 [debug] [MainThread]: Flushing usage events


============================== 2022-11-30 06:18:31.690916 | ab3309a8-1a0a-4626-b896-d62867db280f ==============================
[0m06:18:31.690916 [info ] [MainThread]: Running with dbt=1.3.1
[0m06:18:31.690916 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m06:18:31.697876 [debug] [MainThread]: Tracking: tracking
[0m06:18:31.707910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE1691DC90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE1691D900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE1691F340>]}
[0m06:18:31.778333 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:18:31.778333 [debug] [MainThread]: Partial parsing: updated file: finance://models\fact_finance.sql
[0m06:18:31.789869 [debug] [MainThread]: 1699: static parser successfully parsed fact_finance.sql
[0m06:18:31.805713 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.finance.example

[0m06:18:31.821517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ab3309a8-1a0a-4626-b896-d62867db280f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE17B7BFA0>]}
[0m06:18:31.871532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ab3309a8-1a0a-4626-b896-d62867db280f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE0136EE60>]}
[0m06:18:31.871532 [info ] [MainThread]: Found 7 models, 25 tests, 0 snapshots, 0 analyses, 334 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m06:18:31.871532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab3309a8-1a0a-4626-b896-d62867db280f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE03E6C250>]}
[0m06:18:31.871532 [info ] [MainThread]: 
[0m06:18:31.871532 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m06:18:31.886132 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m06:18:31.886132 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m06:18:31.886132 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m06:18:31.886132 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:18:34.718293 [debug] [ThreadPool]: SQL status: OK in 2.83 seconds
[0m06:18:34.733849 [debug] [ThreadPool]: On list_schemas: Close
[0m06:18:35.865870 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m06:18:35.882062 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m06:18:35.882062 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m06:18:35.882062 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m06:18:35.882062 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:18:38.020025 [debug] [ThreadPool]: SQL status: OK in 2.14 seconds
[0m06:18:38.020025 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m06:18:38.020025 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m06:18:38.020025 [debug] [ThreadPool]: On list_None_finance: Close
[0m06:18:38.933101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab3309a8-1a0a-4626-b896-d62867db280f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE16AC7430>]}
[0m06:18:38.933101 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m06:18:38.933101 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m06:18:38.933101 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m06:18:38.933101 [info ] [MainThread]: 
[0m06:18:38.964211 [debug] [Thread-1 (]: Began running node model.finance.dim_exp
[0m06:18:38.964211 [info ] [Thread-1 (]: 1 of 7 START sql table model finance.dim_exp ................................... [RUN]
[0m06:18:38.964211 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_exp"
[0m06:18:38.964211 [debug] [Thread-1 (]: Began compiling node model.finance.dim_exp
[0m06:18:38.964211 [debug] [Thread-1 (]: Compiling model.finance.dim_exp
[0m06:18:38.964211 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_exp"
[0m06:18:38.979830 [debug] [Thread-1 (]: finished collecting timing info
[0m06:18:38.982652 [debug] [Thread-1 (]: Began executing node model.finance.dim_exp
[0m06:18:39.094174 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_exp"
[0m06:18:39.094174 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m06:18:39.094174 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_exp"
[0m06:18:39.110803 [debug] [Thread-1 (]: On model.finance.dim_exp: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_exp"} */

  
    
      create or replace table finance.dim_exp
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Expenses, Gross_sales, Manufacturing_price from data.table_finance;
  
[0m06:18:39.111792 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m06:18:43.347286 [debug] [Thread-1 (]: SQL status: OK in 4.24 seconds
[0m06:18:43.362875 [debug] [Thread-1 (]: finished collecting timing info
[0m06:18:43.378498 [debug] [Thread-1 (]: On model.finance.dim_exp: ROLLBACK
[0m06:18:43.378498 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m06:18:43.378498 [debug] [Thread-1 (]: On model.finance.dim_exp: Close
[0m06:18:44.309450 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab3309a8-1a0a-4626-b896-d62867db280f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE072F8A60>]}
[0m06:18:44.309450 [info ] [Thread-1 (]: 1 of 7 OK created sql table model finance.dim_exp .............................. [[32mOK[0m in 5.35s]
[0m06:18:44.318548 [debug] [Thread-1 (]: Finished running node model.finance.dim_exp
[0m06:18:44.318548 [debug] [Thread-1 (]: Began running node model.finance.dim_info
[0m06:18:44.318548 [info ] [Thread-1 (]: 2 of 7 START sql table model finance.dim_info .................................. [RUN]
[0m06:18:44.318548 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_info"
[0m06:18:44.318548 [debug] [Thread-1 (]: Began compiling node model.finance.dim_info
[0m06:18:44.318548 [debug] [Thread-1 (]: Compiling model.finance.dim_info
[0m06:18:44.334387 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_info"
[0m06:18:44.334387 [debug] [Thread-1 (]: finished collecting timing info
[0m06:18:44.334387 [debug] [Thread-1 (]: Began executing node model.finance.dim_info
[0m06:18:44.350170 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_info"
[0m06:18:44.350170 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m06:18:44.350170 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_info"
[0m06:18:44.350170 [debug] [Thread-1 (]: On model.finance.dim_info: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_info"} */

  
    
      create or replace table finance.dim_info
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, COGS, Inventory, Segment from data.table_finance;
  
[0m06:18:44.350170 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:18:48.471577 [debug] [Thread-1 (]: SQL status: OK in 4.12 seconds
[0m06:18:48.471577 [debug] [Thread-1 (]: finished collecting timing info
[0m06:18:48.487144 [debug] [Thread-1 (]: On model.finance.dim_info: ROLLBACK
[0m06:18:48.487144 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m06:18:48.487144 [debug] [Thread-1 (]: On model.finance.dim_info: Close
[0m06:18:49.401609 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab3309a8-1a0a-4626-b896-d62867db280f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE17B00130>]}
[0m06:18:49.408118 [info ] [Thread-1 (]: 2 of 7 OK created sql table model finance.dim_info ............................. [[32mOK[0m in 5.08s]
[0m06:18:49.408118 [debug] [Thread-1 (]: Finished running node model.finance.dim_info
[0m06:18:49.408118 [debug] [Thread-1 (]: Began running node model.finance.dim_month
[0m06:18:49.408118 [info ] [Thread-1 (]: 3 of 7 START sql table model finance.dim_month ................................. [RUN]
[0m06:18:49.408118 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_month"
[0m06:18:49.408118 [debug] [Thread-1 (]: Began compiling node model.finance.dim_month
[0m06:18:49.408118 [debug] [Thread-1 (]: Compiling model.finance.dim_month
[0m06:18:49.417707 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_month"
[0m06:18:49.417707 [debug] [Thread-1 (]: finished collecting timing info
[0m06:18:49.417707 [debug] [Thread-1 (]: Began executing node model.finance.dim_month
[0m06:18:49.417707 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_month"
[0m06:18:49.417707 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m06:18:49.417707 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_month"
[0m06:18:49.417707 [debug] [Thread-1 (]: On model.finance.dim_month: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_month"} */

  
    
      create or replace table finance.dim_month
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Month_Number, Year, MonthName, Date from data.table_finance;
  
[0m06:18:49.417707 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:18:53.103050 [debug] [Thread-1 (]: SQL status: OK in 3.69 seconds
[0m06:18:53.103050 [debug] [Thread-1 (]: finished collecting timing info
[0m06:18:53.103050 [debug] [Thread-1 (]: On model.finance.dim_month: ROLLBACK
[0m06:18:53.103050 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m06:18:53.103050 [debug] [Thread-1 (]: On model.finance.dim_month: Close
[0m06:18:54.035555 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab3309a8-1a0a-4626-b896-d62867db280f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE168F68C0>]}
[0m06:18:54.035555 [info ] [Thread-1 (]: 3 of 7 OK created sql table model finance.dim_month ............................ [[32mOK[0m in 4.63s]
[0m06:18:54.035555 [debug] [Thread-1 (]: Finished running node model.finance.dim_month
[0m06:18:54.035555 [debug] [Thread-1 (]: Began running node model.finance.dim_profit
[0m06:18:54.035555 [info ] [Thread-1 (]: 4 of 7 START sql table model finance.dim_profit ................................ [RUN]
[0m06:18:54.035555 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_profit"
[0m06:18:54.035555 [debug] [Thread-1 (]: Began compiling node model.finance.dim_profit
[0m06:18:54.035555 [debug] [Thread-1 (]: Compiling model.finance.dim_profit
[0m06:18:54.051269 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_profit"
[0m06:18:54.051269 [debug] [Thread-1 (]: finished collecting timing info
[0m06:18:54.051269 [debug] [Thread-1 (]: Began executing node model.finance.dim_profit
[0m06:18:54.051269 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_profit"
[0m06:18:54.051269 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m06:18:54.051269 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_profit"
[0m06:18:54.051269 [debug] [Thread-1 (]: On model.finance.dim_profit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_profit"} */

  
    
      create or replace table finance.dim_profit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Discounts, Profit, Sales, Country, Discount_Band, Product from data.table_finance;
  
[0m06:18:54.051269 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:18:58.426605 [debug] [Thread-1 (]: SQL status: OK in 4.38 seconds
[0m06:18:58.426605 [debug] [Thread-1 (]: finished collecting timing info
[0m06:18:58.426605 [debug] [Thread-1 (]: On model.finance.dim_profit: ROLLBACK
[0m06:18:58.426605 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m06:18:58.426605 [debug] [Thread-1 (]: On model.finance.dim_profit: Close
[0m06:18:59.371815 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab3309a8-1a0a-4626-b896-d62867db280f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE17C7E350>]}
[0m06:18:59.371815 [info ] [Thread-1 (]: 4 of 7 OK created sql table model finance.dim_profit ........................... [[32mOK[0m in 5.34s]
[0m06:18:59.371815 [debug] [Thread-1 (]: Finished running node model.finance.dim_profit
[0m06:18:59.371815 [debug] [Thread-1 (]: Began running node model.finance.dim_unit
[0m06:18:59.371815 [info ] [Thread-1 (]: 5 of 7 START sql table model finance.dim_unit .................................. [RUN]
[0m06:18:59.371815 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_unit"
[0m06:18:59.371815 [debug] [Thread-1 (]: Began compiling node model.finance.dim_unit
[0m06:18:59.371815 [debug] [Thread-1 (]: Compiling model.finance.dim_unit
[0m06:18:59.387805 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_unit"
[0m06:18:59.387805 [debug] [Thread-1 (]: finished collecting timing info
[0m06:18:59.387805 [debug] [Thread-1 (]: Began executing node model.finance.dim_unit
[0m06:18:59.387805 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_unit"
[0m06:18:59.387805 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m06:18:59.387805 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_unit"
[0m06:18:59.387805 [debug] [Thread-1 (]: On model.finance.dim_unit: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_unit"} */

  
    
      create or replace table finance.dim_unit
    
    
    using delta
    
    
    
    
    
    
    as
      

select Actual, Target, Unit_Price, Units_Sold from data.table_finance;
  
[0m06:18:59.387805 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:19:03.148625 [debug] [Thread-1 (]: SQL status: OK in 3.76 seconds
[0m06:19:03.148625 [debug] [Thread-1 (]: finished collecting timing info
[0m06:19:03.148625 [debug] [Thread-1 (]: On model.finance.dim_unit: ROLLBACK
[0m06:19:03.148625 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m06:19:03.148625 [debug] [Thread-1 (]: On model.finance.dim_unit: Close
[0m06:19:04.061408 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab3309a8-1a0a-4626-b896-d62867db280f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE17C23670>]}
[0m06:19:04.061408 [info ] [Thread-1 (]: 5 of 7 OK created sql table model finance.dim_unit ............................. [[32mOK[0m in 4.69s]
[0m06:19:04.061408 [debug] [Thread-1 (]: Finished running node model.finance.dim_unit
[0m06:19:04.061408 [debug] [Thread-1 (]: Began running node model.finance.fact_finance
[0m06:19:04.061408 [info ] [Thread-1 (]: 6 of 7 START sql table model finance.fact_finance .............................. [RUN]
[0m06:19:04.061408 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.fact_finance"
[0m06:19:04.061408 [debug] [Thread-1 (]: Began compiling node model.finance.fact_finance
[0m06:19:04.061408 [debug] [Thread-1 (]: Compiling model.finance.fact_finance
[0m06:19:04.077009 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.fact_finance"
[0m06:19:04.077009 [debug] [Thread-1 (]: finished collecting timing info
[0m06:19:04.077009 [debug] [Thread-1 (]: Began executing node model.finance.fact_finance
[0m06:19:04.077009 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.fact_finance"
[0m06:19:04.077009 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m06:19:04.077009 [debug] [Thread-1 (]: Using databricks connection "model.finance.fact_finance"
[0m06:19:04.077009 [debug] [Thread-1 (]: On model.finance.fact_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.fact_finance"} */

  
    
      create or replace table finance.fact_finance
    
    
    using delta
    
    
    
    
    
    
    as
      

select dim_exp.Actual, dim_exp.Expenses, dim_exp.Gross_sales, dim_exp.Manufacturing_price,
 dim_unit.Target, dim_unit.Unit_Price, dim_unit.Units_Sold,
 dim_month.Month_Number, dim_month.Year, dim_month.MonthName, dim_month.Date,
 dim_info.COGS, dim_info.Inventory, dim_info.Segment,
 dim_profit.Discounts, dim_profit.Profit, dim_profit.Sales, dim_profit.Country, dim_profit.Discount_Band, dim_profit.Product
 from finance.dim_exp 
 join finance.dim_unit on dim_exp.Actual = dim_unit.Actual
 join finance.dim_month on dim_month.Actual = dim_unit.Actual
 join finance.dim_info on dim_info.Actual = dim_unit.Actual
 join finance.dim_profit on dim_profit.Actual = dim_unit.Actual;
  
[0m06:19:04.077009 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:28:41.224303 [debug] [Thread-1 (]: SQL status: OK in 577.15 seconds
[0m06:28:41.227293 [debug] [Thread-1 (]: finished collecting timing info
[0m06:28:41.228295 [debug] [Thread-1 (]: On model.finance.fact_finance: ROLLBACK
[0m06:28:41.229291 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m06:28:41.230283 [debug] [Thread-1 (]: On model.finance.fact_finance: Close
[0m06:28:43.233836 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab3309a8-1a0a-4626-b896-d62867db280f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE17C38B20>]}
[0m06:28:43.233836 [info ] [Thread-1 (]: 6 of 7 OK created sql table model finance.fact_finance ......................... [[32mOK[0m in 579.17s]
[0m06:28:43.236475 [debug] [Thread-1 (]: Finished running node model.finance.fact_finance
[0m06:28:43.237545 [debug] [Thread-1 (]: Began running node model.finance.dim_check_month
[0m06:28:43.238357 [info ] [Thread-1 (]: 7 of 7 START sql table model finance.dim_check_month ........................... [RUN]
[0m06:28:43.240296 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_check_month"
[0m06:28:43.240296 [debug] [Thread-1 (]: Began compiling node model.finance.dim_check_month
[0m06:28:43.241210 [debug] [Thread-1 (]: Compiling model.finance.dim_check_month
[0m06:28:43.246199 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_check_month"
[0m06:28:43.247196 [debug] [Thread-1 (]: finished collecting timing info
[0m06:28:43.248187 [debug] [Thread-1 (]: Began executing node model.finance.dim_check_month
[0m06:28:43.259153 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_check_month"
[0m06:28:43.261143 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m06:28:43.261143 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_check_month"
[0m06:28:43.262139 [debug] [Thread-1 (]: On model.finance.dim_check_month: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_check_month"} */

  
    
      create or replace table finance.dim_check_month
    
    
    using delta
    
    
    
    
    
    
    as
      

select date from finance.dim_month where date > '2018-02-01' and date < '2018-03-01';
  
[0m06:28:43.262139 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:28:47.717485 [debug] [Thread-1 (]: SQL status: OK in 4.46 seconds
[0m06:28:47.720484 [debug] [Thread-1 (]: finished collecting timing info
[0m06:28:47.721477 [debug] [Thread-1 (]: On model.finance.dim_check_month: ROLLBACK
[0m06:28:47.721477 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m06:28:47.722711 [debug] [Thread-1 (]: On model.finance.dim_check_month: Close
[0m06:28:48.642586 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab3309a8-1a0a-4626-b896-d62867db280f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE168F4A00>]}
[0m06:28:48.642586 [info ] [Thread-1 (]: 7 of 7 OK created sql table model finance.dim_check_month ...................... [[32mOK[0m in 5.40s]
[0m06:28:48.642586 [debug] [Thread-1 (]: Finished running node model.finance.dim_check_month
[0m06:28:48.656508 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m06:28:48.656508 [debug] [MainThread]: On master: ROLLBACK
[0m06:28:48.656508 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:28:49.606395 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m06:28:49.607405 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m06:28:49.608398 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m06:28:49.608398 [debug] [MainThread]: On master: ROLLBACK
[0m06:28:49.609401 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m06:28:49.610483 [debug] [MainThread]: On master: Close
[0m06:28:50.507602 [info ] [MainThread]: 
[0m06:28:50.509609 [info ] [MainThread]: Finished running 7 table models in 0 hours 10 minutes and 18.64 seconds (618.64s).
[0m06:28:50.510588 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:28:50.511595 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m06:28:50.512588 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m06:28:50.513579 [debug] [MainThread]: Connection 'model.finance.dim_check_month' was properly closed.
[0m06:28:50.539664 [info ] [MainThread]: 
[0m06:28:50.540639 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:28:50.543496 [info ] [MainThread]: 
[0m06:28:50.543496 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m06:28:50.543496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE17CB11B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE17CB1AE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE17CB2980>]}
[0m06:28:50.544505 [debug] [MainThread]: Flushing usage events


============================== 2022-11-30 13:03:07.220360 | 202ce94f-eb55-45e5-bd08-05ba4b00989d ==============================
[0m13:03:07.220360 [info ] [MainThread]: Running with dbt=1.3.1
[0m13:03:07.220360 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\Documents\\dbt_project\\finance', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:03:07.220360 [debug] [MainThread]: Tracking: tracking
[0m13:03:07.247508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AE56FCDCF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AE56FCD810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AE56FCDA50>]}
[0m13:03:07.248502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AE56FCD810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AE56FCDCF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AE56FCDA80>]}
[0m13:03:07.248502 [debug] [MainThread]: Flushing usage events
[0m13:03:09.169423 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 1 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.


============================== 2022-11-30 13:43:39.446170 | 244fdc18-78dd-4722-b42c-2dd37c2a5577 ==============================
[0m13:43:39.446170 [info ] [MainThread]: Running with dbt=1.3.1
[0m13:43:39.446170 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\Documents\\dbt_project\\finance', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:43:39.446170 [debug] [MainThread]: Tracking: tracking
[0m13:43:39.460834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8A2749CF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8A27498A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8A2749FC0>]}
[0m13:43:39.466505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8A27498A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8A2749CF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8A274B310>]}
[0m13:43:39.466505 [debug] [MainThread]: Flushing usage events
[0m13:43:40.676767 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 1 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.


============================== 2022-11-30 13:59:49.616595 | 12ce9c0e-a10f-4cf9-873e-2a2818592358 ==============================
[0m13:59:49.616595 [info ] [MainThread]: Running with dbt=1.3.1
[0m13:59:49.616595 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\Documents\\dbt_project\\finance', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'deps', 'rpc_method': 'deps', 'indirect_selection': 'eager'}
[0m13:59:49.616595 [debug] [MainThread]: Tracking: tracking
[0m13:59:49.673605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222466F35B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222466F3850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222466F3640>]}
[0m13:59:49.675750 [debug] [MainThread]: Set downloads directory='C:\Users\XAVIER~1\AppData\Local\Temp\dbt-downloads-6f09ofbz'
[0m13:59:49.676243 [debug] [MainThread]: Executing "git clone --depth 1 https://github.com/xavier-donbosco/dbt_core_finance.git 36eb6ee112939fa16d73756d0ac618b0"
[0m13:59:51.855559 [debug] [MainThread]: STDOUT: "b''"
[0m13:59:51.855559 [debug] [MainThread]: STDERR: "b"Cloning into '36eb6ee112939fa16d73756d0ac618b0'...\nremote: Repository not found.\nfatal: repository 'https://github.com/xavier-donbosco/dbt_core_finance.git/' not found\n""
[0m13:59:51.855559 [debug] [MainThread]: command return code=128
[0m13:59:51.855559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222466F3910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222467680A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022246768100>]}
[0m13:59:51.855559 [debug] [MainThread]: Flushing usage events
[0m13:59:53.436602 [error] [MainThread]: Encountered an error:
Error checking out spec='None' for repo https://github.com/xavier-donbosco/dbt_core_finance.git
Cloning into '36eb6ee112939fa16d73756d0ac618b0'...
remote: Repository not found.
fatal: repository 'https://github.com/xavier-donbosco/dbt_core_finance.git/' not found


============================== 2022-11-30 14:05:57.391369 | 852c8d69-a9f4-4b4a-9dd7-d22d8df07c97 ==============================
[0m14:05:57.391369 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:05:57.391369 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\Documents\\dbt_project\\finance', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'deps', 'rpc_method': 'deps', 'indirect_selection': 'eager'}
[0m14:05:57.391369 [debug] [MainThread]: Tracking: tracking
[0m14:05:57.427616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021DD50E35B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021DD50E3850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021DD50E3640>]}
[0m14:05:57.430444 [debug] [MainThread]: Set downloads directory='C:\Users\XAVIER~1\AppData\Local\Temp\dbt-downloads-3rp6_kcf'
[0m14:05:57.432060 [debug] [MainThread]: Executing "git clone --depth 1 https://github.com/xavier-donbosco/dbt_core_finance_.git c32959047db82a0902db112006f112f5"
[0m14:05:59.322935 [debug] [MainThread]: STDOUT: "b''"
[0m14:05:59.322935 [debug] [MainThread]: STDERR: "b"Cloning into 'c32959047db82a0902db112006f112f5'...\n""
[0m14:05:59.322935 [debug] [MainThread]: Pulling new dependency c32959047db82a0902db112006f112f5.
[0m14:05:59.322935 [debug] [MainThread]: Executing "git rev-parse HEAD"
[0m14:05:59.360368 [debug] [MainThread]: STDOUT: "b'8351768e5b90e8e8d837ec3488054030d8ed1bd8\n'"
[0m14:05:59.364077 [debug] [MainThread]: STDERR: "b''"
[0m14:05:59.364077 [debug] [MainThread]:   Checking out revision HEAD.
[0m14:05:59.364649 [debug] [MainThread]: Executing "git remote set-branches origin HEAD"
[0m14:05:59.398433 [debug] [MainThread]: STDOUT: "b''"
[0m14:05:59.398433 [debug] [MainThread]: STDERR: "b''"
[0m14:05:59.398433 [debug] [MainThread]: Executing "git fetch origin --depth 1 --tags HEAD"
[0m14:06:01.571156 [debug] [MainThread]: STDOUT: "b''"
[0m14:06:01.571156 [debug] [MainThread]: STDERR: "b'From https://github.com/xavier-donbosco/dbt_core_finance_\n * branch            HEAD       -> FETCH_HEAD\n'"
[0m14:06:01.571156 [debug] [MainThread]: Executing "git tag --list"
[0m14:06:01.621357 [debug] [MainThread]: STDOUT: "b''"
[0m14:06:01.621357 [debug] [MainThread]: STDERR: "b''"
[0m14:06:01.626389 [debug] [MainThread]: Executing "git reset --hard origin/HEAD"
[0m14:06:01.680670 [debug] [MainThread]: STDOUT: "b'HEAD is now at 8351768 first commit\n'"
[0m14:06:01.680670 [debug] [MainThread]: STDERR: "b''"
[0m14:06:01.680670 [debug] [MainThread]: Executing "git rev-parse HEAD"
[0m14:06:01.723274 [debug] [MainThread]: STDOUT: "b'8351768e5b90e8e8d837ec3488054030d8ed1bd8\n'"
[0m14:06:01.723274 [debug] [MainThread]: STDERR: "b''"
[0m14:06:01.723274 [debug] [MainThread]:   Checked out at 8351768.
[0m14:06:01.723274 [warn ] [MainThread]: [33mWARNING: The git package "https://github.com/xavier-donbosco/dbt_core_finance_.git" 
	is not pinned, using HEAD (default branch).
	This can introduce breaking changes into your project without warning!

See https://docs.getdbt.com/docs/package-management#section-specifying-package-versions[0m
[0m14:06:01.747284 [debug] [MainThread]: Executing "git clone --depth 1 https://github.com/xavier-donbosco/dbt_core_finance.git 36eb6ee112939fa16d73756d0ac618b0"
[0m14:06:03.839871 [debug] [MainThread]: STDOUT: "b''"
[0m14:06:03.839871 [debug] [MainThread]: STDERR: "b"Cloning into '36eb6ee112939fa16d73756d0ac618b0'...\nremote: Repository not found.\nfatal: repository 'https://github.com/xavier-donbosco/dbt_core_finance.git/' not found\n""
[0m14:06:03.839871 [debug] [MainThread]: command return code=128
[0m14:06:03.844363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021DD50E3910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021DD50B9660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021DD50B9F60>]}
[0m14:06:03.844363 [debug] [MainThread]: Flushing usage events
[0m14:06:05.500715 [error] [MainThread]: Encountered an error:
Error checking out spec='None' for repo https://github.com/xavier-donbosco/dbt_core_finance.git
Cloning into '36eb6ee112939fa16d73756d0ac618b0'...
remote: Repository not found.
fatal: repository 'https://github.com/xavier-donbosco/dbt_core_finance.git/' not found


============================== 2022-11-30 14:28:25.514759 | f0a9b07a-8dfb-4969-acda-9dbe5dfbe7e7 ==============================
[0m14:28:25.514759 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:28:25.514759 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\Documents\\dbt_project\\finance', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'deps', 'rpc_method': 'deps', 'indirect_selection': 'eager'}
[0m14:28:25.514759 [debug] [MainThread]: Tracking: tracking
[0m14:28:25.548902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C1B8E3700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C1B8E39A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C1B8E3790>]}
[0m14:28:25.549384 [debug] [MainThread]: Set downloads directory='C:\Users\XAVIER~1\AppData\Local\Temp\dbt-downloads-2z8klxs6'
[0m14:28:25.552310 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m14:28:26.670138 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m14:28:26.670138 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m14:28:27.590901 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m14:28:27.632365 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m14:28:30.330725 [info ] [MainThread]:   Installed from version 0.9.2
[0m14:28:30.331220 [info ] [MainThread]:   Updated version available: 0.9.6
[0m14:28:30.333481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'f0a9b07a-8dfb-4969-acda-9dbe5dfbe7e7', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C1B8E3940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C1B8E04C0>]}
[0m14:28:30.333481 [info ] [MainThread]: 
[0m14:28:30.336124 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
[0m14:28:30.349989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C1B98B7C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C1B8E3CA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C1B8E3070>]}
[0m14:28:30.349989 [debug] [MainThread]: Flushing usage events


============================== 2022-12-01 17:15:53.021064 | 781fd1d9-ae1c-4d62-9a0e-d6fcfe725830 ==============================
[0m17:15:53.021064 [info ] [MainThread]: Running with dbt=1.3.1
[0m17:15:53.021813 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\Documents\\dbt_project\\finance', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/mobiles/dim_product.sql', 'models/mobiles/dim_store.sql', 'models/mobiles/fact_sales.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:15:53.021813 [debug] [MainThread]: Tracking: tracking
[0m17:15:53.031239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028104DD9CF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028104DD98A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028104DD9FC0>]}
[0m17:15:53.081058 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m17:15:53.082055 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m17:15:53.083449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '781fd1d9-ae1c-4d62-9a0e-d6fcfe725830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002817351F850>]}
[0m17:15:54.331530 [debug] [MainThread]: Parsing macros\adapters.sql
[0m17:15:54.350229 [debug] [MainThread]: Parsing macros\catalog.sql
[0m17:15:54.352232 [debug] [MainThread]: Parsing macros\statement.sql
[0m17:15:54.354227 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m17:15:54.357220 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m17:15:54.371487 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m17:15:54.374085 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m17:15:54.375072 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m17:15:54.380055 [debug] [MainThread]: Parsing macros\adapters.sql
[0m17:15:54.412680 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m17:15:54.420244 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m17:15:54.438024 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m17:15:54.441009 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m17:15:54.441009 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m17:15:54.444915 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m17:15:54.451473 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
[0m17:15:54.455837 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m17:15:54.465688 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m17:15:54.472744 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m17:15:54.473742 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m17:15:54.476731 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m17:15:54.481831 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m17:15:54.484583 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m17:15:54.495708 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m17:15:54.497947 [debug] [MainThread]: Parsing macros\adapters\timestamps.sql
[0m17:15:54.499941 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m17:15:54.505920 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m17:15:54.509906 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m17:15:54.511953 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m17:15:54.512994 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m17:15:54.512994 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m17:15:54.514037 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m17:15:54.514987 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m17:15:54.516116 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m17:15:54.518208 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m17:15:54.520197 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m17:15:54.523158 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m17:15:54.529316 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m17:15:54.537289 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m17:15:54.538902 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m17:15:54.552116 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m17:15:54.568391 [debug] [MainThread]: Parsing macros\materializations\models\incremental\strategies.sql
[0m17:15:54.574371 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m17:15:54.578358 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m17:15:54.583119 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m17:15:54.586110 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m17:15:54.588280 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m17:15:54.589308 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m17:15:54.593422 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m17:15:54.645000 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m17:15:54.649971 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m17:15:54.664137 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m17:15:54.672827 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m17:15:54.674821 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m17:15:54.687269 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m17:15:54.688954 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m17:15:54.692015 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m17:15:54.694012 [debug] [MainThread]: Parsing macros\python_model\python.sql
[0m17:15:54.698997 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m17:15:54.699917 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m17:15:54.700989 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m17:15:54.701910 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m17:15:54.703003 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m17:15:54.703903 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m17:15:54.704974 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m17:15:54.705896 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m17:15:54.711187 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m17:15:54.712184 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m17:15:54.713264 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m17:15:54.714837 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m17:15:54.715927 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m17:15:54.716963 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m17:15:54.717989 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m17:15:54.718969 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m17:15:54.720041 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m17:15:54.721043 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m17:15:54.723034 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m17:15:54.723947 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m17:15:54.724945 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m17:15:54.725985 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m17:15:54.726978 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m17:15:54.727972 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m17:15:54.729929 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m17:15:54.732007 [debug] [MainThread]: Parsing macros\cross_db_utils\array_append.sql
[0m17:15:54.733914 [debug] [MainThread]: Parsing macros\cross_db_utils\array_concat.sql
[0m17:15:54.735906 [debug] [MainThread]: Parsing macros\cross_db_utils\array_construct.sql
[0m17:15:54.737901 [debug] [MainThread]: Parsing macros\cross_db_utils\cast_array_to_string.sql
[0m17:15:54.739893 [debug] [MainThread]: Parsing macros\cross_db_utils\current_timestamp.sql
[0m17:15:54.743393 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\any_value.sql
[0m17:15:54.744355 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\bool_or.sql
[0m17:15:54.747217 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\cast_bool_to_text.sql
[0m17:15:54.749880 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\concat.sql
[0m17:15:54.751877 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\datatypes.sql
[0m17:15:54.757814 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\dateadd.sql
[0m17:15:54.758840 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\datediff.sql
[0m17:15:54.759756 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\date_trunc.sql
[0m17:15:54.762403 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\escape_single_quotes.sql
[0m17:15:54.764119 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\except.sql
[0m17:15:54.765115 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\hash.sql
[0m17:15:54.766112 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\identifier.sql
[0m17:15:54.768105 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\intersect.sql
[0m17:15:54.769102 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\last_day.sql
[0m17:15:54.770098 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\length.sql
[0m17:15:54.771095 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\listagg.sql
[0m17:15:54.772290 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\literal.sql
[0m17:15:54.773357 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\position.sql
[0m17:15:54.775082 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\replace.sql
[0m17:15:54.776132 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\right.sql
[0m17:15:54.778444 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\safe_cast.sql
[0m17:15:54.779459 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\split_part.sql
[0m17:15:54.781459 [debug] [MainThread]: Parsing macros\cross_db_utils\deprecated\xdb_deprecation_warning.sql
[0m17:15:54.782431 [debug] [MainThread]: Parsing macros\generic_tests\accepted_range.sql
[0m17:15:54.784422 [debug] [MainThread]: Parsing macros\generic_tests\at_least_one.sql
[0m17:15:54.785419 [debug] [MainThread]: Parsing macros\generic_tests\cardinality_equality.sql
[0m17:15:54.786375 [debug] [MainThread]: Parsing macros\generic_tests\equality.sql
[0m17:15:54.789369 [debug] [MainThread]: Parsing macros\generic_tests\equal_rowcount.sql
[0m17:15:54.790331 [debug] [MainThread]: Parsing macros\generic_tests\expression_is_true.sql
[0m17:15:54.792525 [debug] [MainThread]: Parsing macros\generic_tests\fewer_rows_than.sql
[0m17:15:54.793522 [debug] [MainThread]: Parsing macros\generic_tests\mutually_exclusive_ranges.sql
[0m17:15:54.801684 [debug] [MainThread]: Parsing macros\generic_tests\not_accepted_values.sql
[0m17:15:54.803642 [debug] [MainThread]: Parsing macros\generic_tests\not_constant.sql
[0m17:15:54.804672 [debug] [MainThread]: Parsing macros\generic_tests\not_null_proportion.sql
[0m17:15:54.806669 [debug] [MainThread]: Parsing macros\generic_tests\recency.sql
[0m17:15:54.807706 [debug] [MainThread]: Parsing macros\generic_tests\relationships_where.sql
[0m17:15:54.809802 [debug] [MainThread]: Parsing macros\generic_tests\sequential_values.sql
[0m17:15:54.812708 [debug] [MainThread]: Parsing macros\generic_tests\test_not_null_where.sql
[0m17:15:54.813718 [debug] [MainThread]: Parsing macros\generic_tests\test_unique_where.sql
[0m17:15:54.814771 [debug] [MainThread]: Parsing macros\generic_tests\unique_combination_of_columns.sql
[0m17:15:54.817774 [debug] [MainThread]: Parsing macros\jinja_helpers\log_info.sql
[0m17:15:54.818758 [debug] [MainThread]: Parsing macros\jinja_helpers\pretty_log_format.sql
[0m17:15:54.819722 [debug] [MainThread]: Parsing macros\jinja_helpers\pretty_time.sql
[0m17:15:54.820759 [debug] [MainThread]: Parsing macros\jinja_helpers\slugify.sql
[0m17:15:54.821677 [debug] [MainThread]: Parsing macros\jinja_helpers\_is_ephemeral.sql
[0m17:15:54.823889 [debug] [MainThread]: Parsing macros\jinja_helpers\_is_relation.sql
[0m17:15:54.825876 [debug] [MainThread]: Parsing macros\materializations\insert_by_period_materialization.sql
[0m17:15:54.855185 [debug] [MainThread]: Parsing macros\sql\date_spine.sql
[0m17:15:54.859121 [debug] [MainThread]: Parsing macros\sql\deduplicate.sql
[0m17:15:54.865237 [debug] [MainThread]: Parsing macros\sql\generate_series.sql
[0m17:15:54.869120 [debug] [MainThread]: Parsing macros\sql\get_column_values.sql
[0m17:15:54.875198 [debug] [MainThread]: Parsing macros\sql\get_filtered_columns_in_relation.sql
[0m17:15:54.877647 [debug] [MainThread]: Parsing macros\sql\get_query_results_as_dict.sql
[0m17:15:54.879639 [debug] [MainThread]: Parsing macros\sql\get_relations_by_pattern.sql
[0m17:15:54.882825 [debug] [MainThread]: Parsing macros\sql\get_relations_by_prefix.sql
[0m17:15:54.885834 [debug] [MainThread]: Parsing macros\sql\get_tables_by_pattern_sql.sql
[0m17:15:54.890953 [debug] [MainThread]: Parsing macros\sql\get_tables_by_prefix_sql.sql
[0m17:15:54.892180 [debug] [MainThread]: Parsing macros\sql\get_table_types_sql.sql
[0m17:15:54.893196 [debug] [MainThread]: Parsing macros\sql\groupby.sql
[0m17:15:54.894265 [debug] [MainThread]: Parsing macros\sql\haversine_distance.sql
[0m17:15:54.899329 [debug] [MainThread]: Parsing macros\sql\nullcheck.sql
[0m17:15:54.900815 [debug] [MainThread]: Parsing macros\sql\nullcheck_table.sql
[0m17:15:54.902164 [debug] [MainThread]: Parsing macros\sql\pivot.sql
[0m17:15:54.904382 [debug] [MainThread]: Parsing macros\sql\safe_add.sql
[0m17:15:54.906297 [debug] [MainThread]: Parsing macros\sql\star.sql
[0m17:15:54.909283 [debug] [MainThread]: Parsing macros\sql\surrogate_key.sql
[0m17:15:54.912273 [debug] [MainThread]: Parsing macros\sql\union.sql
[0m17:15:54.921243 [debug] [MainThread]: Parsing macros\sql\unpivot.sql
[0m17:15:54.927223 [debug] [MainThread]: Parsing macros\sql\width_bucket.sql
[0m17:15:54.930214 [debug] [MainThread]: Parsing macros\web\get_url_host.sql
[0m17:15:54.932806 [debug] [MainThread]: Parsing macros\web\get_url_parameter.sql
[0m17:15:54.933804 [debug] [MainThread]: Parsing macros\web\get_url_path.sql
[0m17:15:55.347138 [debug] [MainThread]: 1699: static parser successfully parsed finance\dim_check_month.sql
[0m17:15:55.357460 [debug] [MainThread]: 1699: static parser successfully parsed finance\dim_exp.sql
[0m17:15:55.359471 [debug] [MainThread]: 1699: static parser successfully parsed finance\dim_info.sql
[0m17:15:55.361377 [debug] [MainThread]: 1699: static parser successfully parsed finance\dim_month.sql
[0m17:15:55.364435 [debug] [MainThread]: 1699: static parser successfully parsed finance\dim_profit.sql
[0m17:15:55.366311 [debug] [MainThread]: 1699: static parser successfully parsed finance\dim_unit.sql
[0m17:15:55.369382 [debug] [MainThread]: 1699: static parser successfully parsed finance\fact_finance.sql
[0m17:15:55.371372 [debug] [MainThread]: 1699: static parser successfully parsed mobiles\dim_product.sql
[0m17:15:55.373356 [debug] [MainThread]: 1699: static parser successfully parsed mobiles\dim_store.sql
[0m17:15:55.375351 [debug] [MainThread]: 1699: static parser successfully parsed mobiles\fact_sales.sql
[0m17:15:55.491306 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.finance.example

[0m17:15:55.498321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '781fd1d9-ae1c-4d62-9a0e-d6fcfe725830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028104F7FE80>]}
[0m17:15:55.510441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '781fd1d9-ae1c-4d62-9a0e-d6fcfe725830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028104DEA650>]}
[0m17:15:55.511440 [info ] [MainThread]: Found 10 models, 25 tests, 0 snapshots, 0 analyses, 531 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m17:15:55.511440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '781fd1d9-ae1c-4d62-9a0e-d6fcfe725830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028104DE8CD0>]}
[0m17:15:55.515503 [info ] [MainThread]: 
[0m17:15:55.516513 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:15:55.523001 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m17:15:55.532002 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m17:15:55.532002 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:15:55.532002 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:19:16.345386 [debug] [ThreadPool]: SQL status: OK in 200.81 seconds
[0m17:19:16.361489 [debug] [ThreadPool]: On list_schemas: Close
[0m17:19:18.185114 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m17:19:18.192251 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:19:18.193248 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m17:19:18.193248 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m17:19:18.193248 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:19:24.649881 [debug] [ThreadPool]: SQL status: OK in 6.46 seconds
[0m17:19:24.661736 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m17:19:24.661736 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:19:24.662756 [debug] [ThreadPool]: On list_None_finance: Close
[0m17:19:25.258775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '781fd1d9-ae1c-4d62-9a0e-d6fcfe725830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028104F7EA70>]}
[0m17:19:25.259764 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:19:25.260750 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:19:25.261692 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:25:24.981133 [info ] [MainThread]: 
[0m17:25:24.993128 [debug] [Thread-1 (]: Began running node model.finance.dim_product
[0m17:25:24.993128 [info ] [Thread-1 (]: 1 of 3 START sql table model finance.dim_product ............................... [RUN]
[0m17:25:24.994138 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_product"
[0m17:25:24.995108 [debug] [Thread-1 (]: Began compiling node model.finance.dim_product
[0m17:25:24.995535 [debug] [Thread-1 (]: Compiling model.finance.dim_product
[0m17:25:24.998716 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_product"
[0m17:25:24.999715 [debug] [Thread-1 (]: finished collecting timing info
[0m17:25:24.999715 [debug] [Thread-1 (]: Began executing node model.finance.dim_product
[0m17:25:25.042134 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_product"
[0m17:25:25.042878 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:25:25.042878 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_product"
[0m17:25:25.042878 [debug] [Thread-1 (]: On model.finance.dim_product: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_product"} */

  
    
      create or replace table finance.dim_product
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS dim_product;

CREATE TABLE dim_product (
product_id INT,
product_name STRING,
product_price INT
) USING DELTA;

INSERT INTO dim_product VALUES 
(1, "iphone 12", 60000),
(2, "iphone 12 pro", 65000),
(3, "iphone 13", 80000),
(4, "iphone 13 pro", 85000),
(5, "Samsung S22", 85000),
(6, "Samsung S22 Pro", 90000),
(7, "Realme 9", 40000),
(8, "OnePlus 9", 55000),
(9, "Nokia", 20000),
(10, "Lava", 11000);
  
[0m17:25:25.043895 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:25:29.773084 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_product"} */

  
    
      create or replace table finance.dim_product
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS dim_product;

CREATE TABLE dim_product (
product_id INT,
product_name STRING,
product_price INT
) USING DELTA;

INSERT INTO dim_product VALUES 
(1, "iphone 12", 60000),
(2, "iphone 12 pro", 65000),
(3, "iphone 13", 80000),
(4, "iphone 13 pro", 85000),
(5, "Samsung S22", 85000),
(6, "Samsung S22 Pro", 90000),
(7, "Realme 9", 40000),
(8, "OnePlus 9", 55000),
(9, "Nokia", 20000),
(10, "Lava", 11000);
  
[0m17:25:29.773084 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_product"} */

  
    
      create or replace table finance.dim_product
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS dim_product;
^^^

CREATE TABLE dim_product (
product_id INT,
product_name STRING,
product_price INT
) USING DELTA;

INSERT INTO dim_product VALUES 
(1, "iphone 12", 60000),
(2, "iphone 12 pro", 65000),
(3, "iphone 13", 80000),
(4, "iphone 13 pro", 85000),
(5, "Samsung S22", 85000),
(6, "Samsung S22 Pro", 90000),
(7, "Realme 9", 40000),
(8, "OnePlus 9", 55000),
(9, "Nokia", 20000),
(10, "Lava", 11000)

[0m17:25:29.774087 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_product"} */

  
    
      create or replace table finance.dim_product
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS dim_product;
^^^

CREATE TABLE dim_product (
product_id INT,
product_name STRING,
product_price INT
) USING DELTA;

INSERT INTO dim_product VALUES 
(1, "iphone 12", 60000),
(2, "iphone 12 pro", 65000),
(3, "iphone 13", 80000),
(4, "iphone 13 pro", 85000),
(5, "Samsung S22", 85000),
(6, "Samsung S22 Pro", 90000),
(7, "Realme 9", 40000),
(8, "OnePlus 9", 55000),
(9, "Nokia", 20000),
(10, "Lava", 11000)

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_product"} */

  
    
      create or replace table finance.dim_product
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS dim_product;
^^^

CREATE TABLE dim_product (
product_id INT,
product_name STRING,
product_price INT
) USING DELTA;

INSERT INTO dim_product VALUES 
(1, "iphone 12", 60000),
(2, "iphone 12 pro", 65000),
(3, "iphone 13", 80000),
(4, "iphone 13 pro", 85000),
(5, "Samsung S22", 85000),
(6, "Samsung S22 Pro", 90000),
(7, "Realme 9", 40000),
(8, "OnePlus 9", 55000),
(9, "Nokia", 20000),
(10, "Lava", 11000)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:356)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:166)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:90)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:112)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:346)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:346)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:333)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:392)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:691)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:392)
	... 19 more

[0m17:25:29.775006 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\xde\xe7\x94+_\x95I\xe1\xb2\x9f\xcb\xa9\xc3\xc3\xd7\x12'
[0m17:25:29.776003 [debug] [Thread-1 (]: finished collecting timing info
[0m17:25:29.776003 [debug] [Thread-1 (]: On model.finance.dim_product: ROLLBACK
[0m17:25:29.776003 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m17:25:29.776003 [debug] [Thread-1 (]: On model.finance.dim_product: Close
[0m17:25:30.570498 [debug] [Thread-1 (]: Runtime Error in model dim_product (models\mobiles\dim_product.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_product"} */
  
    
      
        create or replace table finance.dim_product
      
      
      using delta
      
      
      
      
      
      
      as
        
  
  DROP TABLE IF EXISTS dim_product;
  ^^^
  
  CREATE TABLE dim_product (
  product_id INT,
  product_name STRING,
  product_price INT
  ) USING DELTA;
  
  INSERT INTO dim_product VALUES 
  (1, "iphone 12", 60000),
  (2, "iphone 12 pro", 65000),
  (3, "iphone 13", 80000),
  (4, "iphone 13 pro", 85000),
  (5, "Samsung S22", 85000),
  (6, "Samsung S22 Pro", 90000),
  (7, "Realme 9", 40000),
  (8, "OnePlus 9", 55000),
  (9, "Nokia", 20000),
  (10, "Lava", 11000)
  
[0m17:25:30.571495 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '781fd1d9-ae1c-4d62-9a0e-d6fcfe725830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281061B9180>]}
[0m17:25:30.572654 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model finance.dim_product ...................... [[31mERROR[0m in 5.58s]
[0m17:25:30.574486 [debug] [Thread-1 (]: Finished running node model.finance.dim_product
[0m17:25:30.575484 [debug] [Thread-1 (]: Began running node model.finance.dim_store
[0m17:25:30.575484 [info ] [Thread-1 (]: 2 of 3 START sql table model finance.dim_store ................................. [RUN]
[0m17:25:30.576549 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.dim_store"
[0m17:25:30.577517 [debug] [Thread-1 (]: Began compiling node model.finance.dim_store
[0m17:25:30.577517 [debug] [Thread-1 (]: Compiling model.finance.dim_store
[0m17:25:30.581202 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.dim_store"
[0m17:25:30.582621 [debug] [Thread-1 (]: finished collecting timing info
[0m17:25:30.582621 [debug] [Thread-1 (]: Began executing node model.finance.dim_store
[0m17:25:30.586745 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.dim_store"
[0m17:25:30.587905 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:25:30.588139 [debug] [Thread-1 (]: Using databricks connection "model.finance.dim_store"
[0m17:25:30.588310 [debug] [Thread-1 (]: On model.finance.dim_store: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_store"} */

  
    
      create or replace table finance.dim_store
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS dim_store;

CREATE TABLE dim_store (
store_id INT,
store_name STRING,
store_address STRING,
store_city STRING,
store_state STRING
) USING DELTA;

INSERT INTO dim_store VALUES 
(1, "Poorvika", "No. 1/45, Hosa Nagar", "Electronic City", "Karnataka"),
(2, "Richy Rich", "Near Kudlu gate, Silk Board", "Bombanahalli City", "Karnataka"),
(3, "Mohit Mobiles", "Near Novel Tech Park, Hosa Nagar", "Electronic City", "Karnataka"),
(4, "Kareem Mobiles", "No. 4, chittur main Road", "Krishnagiri City", "TamilNadu"),
(5, "Rahul Mobiles", "Near Ganapathi Temple, Kovai", "Thirunelveli", "TamilNadu"),
(6, "James Bond Mobiles", "No. 2/653, Pada Nagar", "Balpura City", "Gujrat"),
(7, "Mobile x", "Near Kudlu gate, Silk Board", "Mahara City", "Andhra Pradesh"),
(8, "Cubot", "Near AIIMS medical college, Hosa Nagar", "Chutta City", "Delhi"),
(9, "Ranga Mobiles", "No. 8, mangatha main Road", "Kasaga City", "Mumbai"),
(10, "Poorvika", "Near holy cross church, Kovai", "Banatha", "Maharastra");
  
[0m17:25:30.588310 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:25:32.374082 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_store"} */

  
    
      create or replace table finance.dim_store
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS dim_store;

CREATE TABLE dim_store (
store_id INT,
store_name STRING,
store_address STRING,
store_city STRING,
store_state STRING
) USING DELTA;

INSERT INTO dim_store VALUES 
(1, "Poorvika", "No. 1/45, Hosa Nagar", "Electronic City", "Karnataka"),
(2, "Richy Rich", "Near Kudlu gate, Silk Board", "Bombanahalli City", "Karnataka"),
(3, "Mohit Mobiles", "Near Novel Tech Park, Hosa Nagar", "Electronic City", "Karnataka"),
(4, "Kareem Mobiles", "No. 4, chittur main Road", "Krishnagiri City", "TamilNadu"),
(5, "Rahul Mobiles", "Near Ganapathi Temple, Kovai", "Thirunelveli", "TamilNadu"),
(6, "James Bond Mobiles", "No. 2/653, Pada Nagar", "Balpura City", "Gujrat"),
(7, "Mobile x", "Near Kudlu gate, Silk Board", "Mahara City", "Andhra Pradesh"),
(8, "Cubot", "Near AIIMS medical college, Hosa Nagar", "Chutta City", "Delhi"),
(9, "Ranga Mobiles", "No. 8, mangatha main Road", "Kasaga City", "Mumbai"),
(10, "Poorvika", "Near holy cross church, Kovai", "Banatha", "Maharastra");
  
[0m17:25:32.375357 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_store"} */

  
    
      create or replace table finance.dim_store
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS dim_store;
^^^

CREATE TABLE dim_store (
store_id INT,
store_name STRING,
store_address STRING,
store_city STRING,
store_state STRING
) USING DELTA;

INSERT INTO dim_store VALUES 
(1, "Poorvika", "No. 1/45, Hosa Nagar", "Electronic City", "Karnataka"),
(2, "Richy Rich", "Near Kudlu gate, Silk Board", "Bombanahalli City", "Karnataka"),
(3, "Mohit Mobiles", "Near Novel Tech Park, Hosa Nagar", "Electronic City", "Karnataka"),
(4, "Kareem Mobiles", "No. 4, chittur main Road", "Krishnagiri City", "TamilNadu"),
(5, "Rahul Mobiles", "Near Ganapathi Temple, Kovai", "Thirunelveli", "TamilNadu"),
(6, "James Bond Mobiles", "No. 2/653, Pada Nagar", "Balpura City", "Gujrat"),
(7, "Mobile x", "Near Kudlu gate, Silk Board", "Mahara City", "Andhra Pradesh"),
(8, "Cubot", "Near AIIMS medical college, Hosa Nagar", "Chutta City", "Delhi"),
(9, "Ranga Mobiles", "No. 8, mangatha main Road", "Kasaga City", "Mumbai"),
(10, "Poorvika", "Near holy cross church, Kovai", "Banatha", "Maharastra")

[0m17:25:32.375674 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_store"} */

  
    
      create or replace table finance.dim_store
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS dim_store;
^^^

CREATE TABLE dim_store (
store_id INT,
store_name STRING,
store_address STRING,
store_city STRING,
store_state STRING
) USING DELTA;

INSERT INTO dim_store VALUES 
(1, "Poorvika", "No. 1/45, Hosa Nagar", "Electronic City", "Karnataka"),
(2, "Richy Rich", "Near Kudlu gate, Silk Board", "Bombanahalli City", "Karnataka"),
(3, "Mohit Mobiles", "Near Novel Tech Park, Hosa Nagar", "Electronic City", "Karnataka"),
(4, "Kareem Mobiles", "No. 4, chittur main Road", "Krishnagiri City", "TamilNadu"),
(5, "Rahul Mobiles", "Near Ganapathi Temple, Kovai", "Thirunelveli", "TamilNadu"),
(6, "James Bond Mobiles", "No. 2/653, Pada Nagar", "Balpura City", "Gujrat"),
(7, "Mobile x", "Near Kudlu gate, Silk Board", "Mahara City", "Andhra Pradesh"),
(8, "Cubot", "Near AIIMS medical college, Hosa Nagar", "Chutta City", "Delhi"),
(9, "Ranga Mobiles", "No. 8, mangatha main Road", "Kasaga City", "Mumbai"),
(10, "Poorvika", "Near holy cross church, Kovai", "Banatha", "Maharastra")

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_store"} */

  
    
      create or replace table finance.dim_store
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS dim_store;
^^^

CREATE TABLE dim_store (
store_id INT,
store_name STRING,
store_address STRING,
store_city STRING,
store_state STRING
) USING DELTA;

INSERT INTO dim_store VALUES 
(1, "Poorvika", "No. 1/45, Hosa Nagar", "Electronic City", "Karnataka"),
(2, "Richy Rich", "Near Kudlu gate, Silk Board", "Bombanahalli City", "Karnataka"),
(3, "Mohit Mobiles", "Near Novel Tech Park, Hosa Nagar", "Electronic City", "Karnataka"),
(4, "Kareem Mobiles", "No. 4, chittur main Road", "Krishnagiri City", "TamilNadu"),
(5, "Rahul Mobiles", "Near Ganapathi Temple, Kovai", "Thirunelveli", "TamilNadu"),
(6, "James Bond Mobiles", "No. 2/653, Pada Nagar", "Balpura City", "Gujrat"),
(7, "Mobile x", "Near Kudlu gate, Silk Board", "Mahara City", "Andhra Pradesh"),
(8, "Cubot", "Near AIIMS medical college, Hosa Nagar", "Chutta City", "Delhi"),
(9, "Ranga Mobiles", "No. 8, mangatha main Road", "Kasaga City", "Mumbai"),
(10, "Poorvika", "Near holy cross church, Kovai", "Banatha", "Maharastra")

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:356)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:166)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:90)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:112)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:346)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:346)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:333)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:392)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:691)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:392)
	... 19 more

[0m17:25:32.375833 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\xebW\xd7\xc7e\xb5C\xa6\x9d\x9a#\xd6;\x84\x049'
[0m17:25:32.376116 [debug] [Thread-1 (]: finished collecting timing info
[0m17:25:32.376116 [debug] [Thread-1 (]: On model.finance.dim_store: ROLLBACK
[0m17:25:32.376116 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m17:25:32.376586 [debug] [Thread-1 (]: On model.finance.dim_store: Close
[0m17:25:32.936303 [debug] [Thread-1 (]: Runtime Error in model dim_store (models\mobiles\dim_store.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_store"} */
  
    
      
        create or replace table finance.dim_store
      
      
      using delta
      
      
      
      
      
      
      as
        
  
  DROP TABLE IF EXISTS dim_store;
  ^^^
  
  CREATE TABLE dim_store (
  store_id INT,
  store_name STRING,
  store_address STRING,
  store_city STRING,
  store_state STRING
  ) USING DELTA;
  
  INSERT INTO dim_store VALUES 
  (1, "Poorvika", "No. 1/45, Hosa Nagar", "Electronic City", "Karnataka"),
  (2, "Richy Rich", "Near Kudlu gate, Silk Board", "Bombanahalli City", "Karnataka"),
  (3, "Mohit Mobiles", "Near Novel Tech Park, Hosa Nagar", "Electronic City", "Karnataka"),
  (4, "Kareem Mobiles", "No. 4, chittur main Road", "Krishnagiri City", "TamilNadu"),
  (5, "Rahul Mobiles", "Near Ganapathi Temple, Kovai", "Thirunelveli", "TamilNadu"),
  (6, "James Bond Mobiles", "No. 2/653, Pada Nagar", "Balpura City", "Gujrat"),
  (7, "Mobile x", "Near Kudlu gate, Silk Board", "Mahara City", "Andhra Pradesh"),
  (8, "Cubot", "Near AIIMS medical college, Hosa Nagar", "Chutta City", "Delhi"),
  (9, "Ranga Mobiles", "No. 8, mangatha main Road", "Kasaga City", "Mumbai"),
  (10, "Poorvika", "Near holy cross church, Kovai", "Banatha", "Maharastra")
  
[0m17:25:32.937299 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '781fd1d9-ae1c-4d62-9a0e-d6fcfe725830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281061B9570>]}
[0m17:25:32.937299 [error] [Thread-1 (]: 2 of 3 ERROR creating sql table model finance.dim_store ........................ [[31mERROR[0m in 2.36s]
[0m17:25:32.938306 [debug] [Thread-1 (]: Finished running node model.finance.dim_store
[0m17:25:32.938306 [debug] [Thread-1 (]: Began running node model.finance.fact_sales
[0m17:25:32.939292 [info ] [Thread-1 (]: 3 of 3 START sql table model finance.fact_sales ................................ [RUN]
[0m17:25:32.940387 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.fact_sales"
[0m17:25:32.940387 [debug] [Thread-1 (]: Began compiling node model.finance.fact_sales
[0m17:25:32.940387 [debug] [Thread-1 (]: Compiling model.finance.fact_sales
[0m17:25:32.943427 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.fact_sales"
[0m17:25:32.944424 [debug] [Thread-1 (]: finished collecting timing info
[0m17:25:32.944424 [debug] [Thread-1 (]: Began executing node model.finance.fact_sales
[0m17:25:32.949756 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.fact_sales"
[0m17:25:32.949756 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:25:32.950747 [debug] [Thread-1 (]: Using databricks connection "model.finance.fact_sales"
[0m17:25:32.950747 [debug] [Thread-1 (]: On model.finance.fact_sales: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.fact_sales"} */

  
    
      create or replace table finance.fact_sales
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS fact_sales;

CREATE TABLE fact_sales (
product_id INT,
store_id INT,
quantity INT,
price INT,
total_price INT
) USING DELTA;

INSERT INTO fact_sales VALUES 
(1, 5, 20, 60000, 1200000),
(5, 8, 11, 85000, 93500000),
(2, 9, 42, 65000, 2730000),
(1, 3, 43, 60000, 258000),
(2, 2, 25, 65000, 1625000),
(3, 6, 17, 80000, 1360000),
(1, 7, 13, 60000, 780000),
(4, 3, 19, 85000, 1615000),
(7, 5, 10, 40000, 400000),
(2, 2, 33, 65000, 2145000);
  
[0m17:25:32.950747 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:25:37.567361 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.fact_sales"} */

  
    
      create or replace table finance.fact_sales
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS fact_sales;

CREATE TABLE fact_sales (
product_id INT,
store_id INT,
quantity INT,
price INT,
total_price INT
) USING DELTA;

INSERT INTO fact_sales VALUES 
(1, 5, 20, 60000, 1200000),
(5, 8, 11, 85000, 93500000),
(2, 9, 42, 65000, 2730000),
(1, 3, 43, 60000, 258000),
(2, 2, 25, 65000, 1625000),
(3, 6, 17, 80000, 1360000),
(1, 7, 13, 60000, 780000),
(4, 3, 19, 85000, 1615000),
(7, 5, 10, 40000, 400000),
(2, 2, 33, 65000, 2145000);
  
[0m17:25:37.569292 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.fact_sales"} */

  
    
      create or replace table finance.fact_sales
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS fact_sales;
^^^

CREATE TABLE fact_sales (
product_id INT,
store_id INT,
quantity INT,
price INT,
total_price INT
) USING DELTA;

INSERT INTO fact_sales VALUES 
(1, 5, 20, 60000, 1200000),
(5, 8, 11, 85000, 93500000),
(2, 9, 42, 65000, 2730000),
(1, 3, 43, 60000, 258000),
(2, 2, 25, 65000, 1625000),
(3, 6, 17, 80000, 1360000),
(1, 7, 13, 60000, 780000),
(4, 3, 19, 85000, 1615000),
(7, 5, 10, 40000, 400000),
(2, 2, 33, 65000, 2145000)

[0m17:25:37.569292 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.fact_sales"} */

  
    
      create or replace table finance.fact_sales
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS fact_sales;
^^^

CREATE TABLE fact_sales (
product_id INT,
store_id INT,
quantity INT,
price INT,
total_price INT
) USING DELTA;

INSERT INTO fact_sales VALUES 
(1, 5, 20, 60000, 1200000),
(5, 8, 11, 85000, 93500000),
(2, 9, 42, 65000, 2730000),
(1, 3, 43, 60000, 258000),
(2, 2, 25, 65000, 1625000),
(3, 6, 17, 80000, 1360000),
(1, 7, 13, 60000, 780000),
(4, 3, 19, 85000, 1615000),
(7, 5, 10, 40000, 400000),
(2, 2, 33, 65000, 2145000)

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:444)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:103)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:356)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:269)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:281)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.fact_sales"} */

  
    
      create or replace table finance.fact_sales
    
    
    using delta
    
    
    
    
    
    
    as
      

DROP TABLE IF EXISTS fact_sales;
^^^

CREATE TABLE fact_sales (
product_id INT,
store_id INT,
quantity INT,
price INT,
total_price INT
) USING DELTA;

INSERT INTO fact_sales VALUES 
(1, 5, 20, 60000, 1200000),
(5, 8, 11, 85000, 93500000),
(2, 9, 42, 65000, 2730000),
(1, 3, 43, 60000, 258000),
(2, 2, 25, 65000, 1625000),
(3, 6, 17, 80000, 1360000),
(1, 7, 13, 60000, 780000),
(4, 3, 19, 85000, 1615000),
(7, 5, 10, 40000, 400000),
(2, 2, 33, 65000, 2145000)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:356)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:166)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:90)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:112)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:346)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:346)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:333)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:392)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:691)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:392)
	... 19 more

[0m17:25:37.570327 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'Z,i\x18n\xf7BO\x98\xb6RB\xd4\xba\x94\x95'
[0m17:25:37.571353 [debug] [Thread-1 (]: finished collecting timing info
[0m17:25:37.572277 [debug] [Thread-1 (]: On model.finance.fact_sales: ROLLBACK
[0m17:25:37.572277 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m17:25:37.573270 [debug] [Thread-1 (]: On model.finance.fact_sales: Close
[0m17:25:38.247092 [debug] [Thread-1 (]: Runtime Error in model fact_sales (models\mobiles\fact_sales.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.fact_sales"} */
  
    
      
        create or replace table finance.fact_sales
      
      
      using delta
      
      
      
      
      
      
      as
        
  
  DROP TABLE IF EXISTS fact_sales;
  ^^^
  
  CREATE TABLE fact_sales (
  product_id INT,
  store_id INT,
  quantity INT,
  price INT,
  total_price INT
  ) USING DELTA;
  
  INSERT INTO fact_sales VALUES 
  (1, 5, 20, 60000, 1200000),
  (5, 8, 11, 85000, 93500000),
  (2, 9, 42, 65000, 2730000),
  (1, 3, 43, 60000, 258000),
  (2, 2, 25, 65000, 1625000),
  (3, 6, 17, 80000, 1360000),
  (1, 7, 13, 60000, 780000),
  (4, 3, 19, 85000, 1615000),
  (7, 5, 10, 40000, 400000),
  (2, 2, 33, 65000, 2145000)
  
[0m17:25:38.248959 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '781fd1d9-ae1c-4d62-9a0e-d6fcfe725830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028106267E50>]}
[0m17:25:38.250000 [error] [Thread-1 (]: 3 of 3 ERROR creating sql table model finance.fact_sales ....................... [[31mERROR[0m in 5.31s]
[0m17:25:38.252612 [debug] [Thread-1 (]: Finished running node model.finance.fact_sales
[0m17:25:38.254615 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:25:38.255613 [debug] [MainThread]: On master: ROLLBACK
[0m17:25:38.255613 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:25:38.959127 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:25:38.960125 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:25:38.960125 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:25:38.961048 [debug] [MainThread]: On master: ROLLBACK
[0m17:25:38.961048 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:25:38.962045 [debug] [MainThread]: On master: Close
[0m17:25:40.642730 [info ] [MainThread]: 
[0m17:25:40.644647 [info ] [MainThread]: Finished running 3 table models in 0 hours 9 minutes and 45.13 seconds (585.13s).
[0m17:25:40.645649 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:25:40.646728 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m17:25:40.646728 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m17:25:40.647638 [debug] [MainThread]: Connection 'model.finance.fact_sales' was properly closed.
[0m17:25:40.660690 [info ] [MainThread]: 
[0m17:25:40.661753 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m17:25:40.662712 [info ] [MainThread]: 
[0m17:25:40.663727 [error] [MainThread]: [33mRuntime Error in model dim_product (models\mobiles\dim_product.sql)[0m
[0m17:25:40.664704 [error] [MainThread]:   
[0m17:25:40.665655 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)
[0m17:25:40.666067 [error] [MainThread]:   
[0m17:25:40.667082 [error] [MainThread]:   == SQL ==
[0m17:25:40.667082 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_product"} */
[0m17:25:40.669946 [error] [MainThread]:   
[0m17:25:40.669946 [error] [MainThread]:     
[0m17:25:40.677257 [error] [MainThread]:       
[0m17:25:40.677257 [error] [MainThread]:         create or replace table finance.dim_product
[0m17:25:40.678257 [error] [MainThread]:       
[0m17:25:40.679253 [error] [MainThread]:       
[0m17:25:40.684736 [error] [MainThread]:       using delta
[0m17:25:40.684736 [error] [MainThread]:       
[0m17:25:40.691954 [error] [MainThread]:       
[0m17:25:40.691954 [error] [MainThread]:       
[0m17:25:40.692955 [error] [MainThread]:       
[0m17:25:40.693949 [error] [MainThread]:       
[0m17:25:40.694520 [error] [MainThread]:       
[0m17:25:40.695523 [error] [MainThread]:       as
[0m17:25:40.696879 [error] [MainThread]:         
[0m17:25:40.696879 [error] [MainThread]:   
[0m17:25:40.700868 [error] [MainThread]:   DROP TABLE IF EXISTS dim_product;
[0m17:25:40.701866 [error] [MainThread]:   ^^^
[0m17:25:40.707844 [error] [MainThread]:   
[0m17:25:40.708841 [error] [MainThread]:   CREATE TABLE dim_product (
[0m17:25:40.709146 [error] [MainThread]:   product_id INT,
[0m17:25:40.709691 [error] [MainThread]:   product_name STRING,
[0m17:25:40.710695 [error] [MainThread]:   product_price INT
[0m17:25:40.710695 [error] [MainThread]:   ) USING DELTA;
[0m17:25:40.711692 [error] [MainThread]:   
[0m17:25:40.712702 [error] [MainThread]:   INSERT INTO dim_product VALUES 
[0m17:25:40.717792 [error] [MainThread]:   (1, "iphone 12", 60000),
[0m17:25:40.718754 [error] [MainThread]:   (2, "iphone 12 pro", 65000),
[0m17:25:40.724805 [error] [MainThread]:   (3, "iphone 13", 80000),
[0m17:25:40.725768 [error] [MainThread]:   (4, "iphone 13 pro", 85000),
[0m17:25:40.726727 [error] [MainThread]:   (5, "Samsung S22", 85000),
[0m17:25:40.727228 [error] [MainThread]:   (6, "Samsung S22 Pro", 90000),
[0m17:25:40.728283 [error] [MainThread]:   (7, "Realme 9", 40000),
[0m17:25:40.728283 [error] [MainThread]:   (8, "OnePlus 9", 55000),
[0m17:25:40.729229 [error] [MainThread]:   (9, "Nokia", 20000),
[0m17:25:40.730222 [error] [MainThread]:   (10, "Lava", 11000)
[0m17:25:40.734315 [error] [MainThread]:   
[0m17:25:40.735313 [info ] [MainThread]: 
[0m17:25:40.739435 [error] [MainThread]: [33mRuntime Error in model dim_store (models\mobiles\dim_store.sql)[0m
[0m17:25:40.740432 [error] [MainThread]:   
[0m17:25:40.741288 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)
[0m17:25:40.742310 [error] [MainThread]:   
[0m17:25:40.742310 [error] [MainThread]:   == SQL ==
[0m17:25:40.743309 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.dim_store"} */
[0m17:25:40.743833 [error] [MainThread]:   
[0m17:25:40.744996 [error] [MainThread]:     
[0m17:25:40.744996 [error] [MainThread]:       
[0m17:25:40.748201 [error] [MainThread]:         create or replace table finance.dim_store
[0m17:25:40.748354 [error] [MainThread]:       
[0m17:25:40.754265 [error] [MainThread]:       
[0m17:25:40.755379 [error] [MainThread]:       using delta
[0m17:25:40.755379 [error] [MainThread]:       
[0m17:25:40.756907 [error] [MainThread]:       
[0m17:25:40.757902 [error] [MainThread]:       
[0m17:25:40.757902 [error] [MainThread]:       
[0m17:25:40.759388 [error] [MainThread]:       
[0m17:25:40.759388 [error] [MainThread]:       
[0m17:25:40.760387 [error] [MainThread]:       as
[0m17:25:40.760387 [error] [MainThread]:         
[0m17:25:40.762903 [error] [MainThread]:   
[0m17:25:40.763902 [error] [MainThread]:   DROP TABLE IF EXISTS dim_store;
[0m17:25:40.770827 [error] [MainThread]:   ^^^
[0m17:25:40.770827 [error] [MainThread]:   
[0m17:25:40.771815 [error] [MainThread]:   CREATE TABLE dim_store (
[0m17:25:40.772811 [error] [MainThread]:   store_id INT,
[0m17:25:40.772962 [error] [MainThread]:   store_name STRING,
[0m17:25:40.773890 [error] [MainThread]:   store_address STRING,
[0m17:25:40.774209 [error] [MainThread]:   store_city STRING,
[0m17:25:40.775267 [error] [MainThread]:   store_state STRING
[0m17:25:40.775454 [error] [MainThread]:   ) USING DELTA;
[0m17:25:40.776467 [error] [MainThread]:   
[0m17:25:40.777846 [error] [MainThread]:   INSERT INTO dim_store VALUES 
[0m17:25:40.786910 [error] [MainThread]:   (1, "Poorvika", "No. 1/45, Hosa Nagar", "Electronic City", "Karnataka"),
[0m17:25:40.786910 [error] [MainThread]:   (2, "Richy Rich", "Near Kudlu gate, Silk Board", "Bombanahalli City", "Karnataka"),
[0m17:25:40.788085 [error] [MainThread]:   (3, "Mohit Mobiles", "Near Novel Tech Park, Hosa Nagar", "Electronic City", "Karnataka"),
[0m17:25:40.788637 [error] [MainThread]:   (4, "Kareem Mobiles", "No. 4, chittur main Road", "Krishnagiri City", "TamilNadu"),
[0m17:25:40.789080 [error] [MainThread]:   (5, "Rahul Mobiles", "Near Ganapathi Temple, Kovai", "Thirunelveli", "TamilNadu"),
[0m17:25:40.789473 [error] [MainThread]:   (6, "James Bond Mobiles", "No. 2/653, Pada Nagar", "Balpura City", "Gujrat"),
[0m17:25:40.789473 [error] [MainThread]:   (7, "Mobile x", "Near Kudlu gate, Silk Board", "Mahara City", "Andhra Pradesh"),
[0m17:25:40.790669 [error] [MainThread]:   (8, "Cubot", "Near AIIMS medical college, Hosa Nagar", "Chutta City", "Delhi"),
[0m17:25:40.795132 [error] [MainThread]:   (9, "Ranga Mobiles", "No. 8, mangatha main Road", "Kasaga City", "Mumbai"),
[0m17:25:40.796132 [error] [MainThread]:   (10, "Poorvika", "Near holy cross church, Kovai", "Banatha", "Maharastra")
[0m17:25:40.804122 [error] [MainThread]:   
[0m17:25:40.805131 [info ] [MainThread]: 
[0m17:25:40.806126 [error] [MainThread]: [33mRuntime Error in model fact_sales (models\mobiles\fact_sales.sql)[0m
[0m17:25:40.808119 [error] [MainThread]:   
[0m17:25:40.808119 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 18, pos 0)
[0m17:25:40.815453 [error] [MainThread]:   
[0m17:25:40.816454 [error] [MainThread]:   == SQL ==
[0m17:25:40.818728 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.fact_sales"} */
[0m17:25:40.819729 [error] [MainThread]:   
[0m17:25:40.819729 [error] [MainThread]:     
[0m17:25:40.820724 [error] [MainThread]:       
[0m17:25:40.821247 [error] [MainThread]:         create or replace table finance.fact_sales
[0m17:25:40.821570 [error] [MainThread]:       
[0m17:25:40.824601 [error] [MainThread]:       
[0m17:25:40.825600 [error] [MainThread]:       using delta
[0m17:25:40.833572 [error] [MainThread]:       
[0m17:25:40.833572 [error] [MainThread]:       
[0m17:25:40.834569 [error] [MainThread]:       
[0m17:25:40.835662 [error] [MainThread]:       
[0m17:25:40.836782 [error] [MainThread]:       
[0m17:25:40.837275 [error] [MainThread]:       
[0m17:25:40.839903 [error] [MainThread]:       as
[0m17:25:40.841071 [error] [MainThread]:         
[0m17:25:40.846098 [error] [MainThread]:   
[0m17:25:40.848094 [error] [MainThread]:   DROP TABLE IF EXISTS fact_sales;
[0m17:25:40.850083 [error] [MainThread]:   ^^^
[0m17:25:40.851087 [error] [MainThread]:   
[0m17:25:40.851565 [error] [MainThread]:   CREATE TABLE fact_sales (
[0m17:25:40.852499 [error] [MainThread]:   product_id INT,
[0m17:25:40.854226 [error] [MainThread]:   store_id INT,
[0m17:25:40.855194 [error] [MainThread]:   quantity INT,
[0m17:25:40.862909 [error] [MainThread]:   price INT,
[0m17:25:40.863904 [error] [MainThread]:   total_price INT
[0m17:25:40.864935 [error] [MainThread]:   ) USING DELTA;
[0m17:25:40.871150 [error] [MainThread]:   
[0m17:25:40.872103 [error] [MainThread]:   INSERT INTO fact_sales VALUES 
[0m17:25:40.878559 [error] [MainThread]:   (1, 5, 20, 60000, 1200000),
[0m17:25:40.879560 [error] [MainThread]:   (5, 8, 11, 85000, 93500000),
[0m17:25:40.880612 [error] [MainThread]:   (2, 9, 42, 65000, 2730000),
[0m17:25:40.885645 [error] [MainThread]:   (1, 3, 43, 60000, 258000),
[0m17:25:40.886657 [error] [MainThread]:   (2, 2, 25, 65000, 1625000),
[0m17:25:40.892636 [error] [MainThread]:   (3, 6, 17, 80000, 1360000),
[0m17:25:40.893633 [error] [MainThread]:   (1, 7, 13, 60000, 780000),
[0m17:25:40.894634 [error] [MainThread]:   (4, 3, 19, 85000, 1615000),
[0m17:25:40.894634 [error] [MainThread]:   (7, 5, 10, 40000, 400000),
[0m17:25:40.895852 [error] [MainThread]:   (2, 2, 33, 65000, 2145000)
[0m17:25:40.897161 [error] [MainThread]:   
[0m17:25:40.897161 [info ] [MainThread]: 
[0m17:25:40.902149 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=0 TOTAL=3
[0m17:25:40.903140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028104DD9FC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028106267610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028106263250>]}
[0m17:25:40.903140 [debug] [MainThread]: Flushing usage events


============================== 2022-12-01 17:45:58.361116 | a8602bb0-b963-470f-b3f6-7fc07392a424 ==============================
[0m17:45:58.361116 [info ] [MainThread]: Running with dbt=1.3.1
[0m17:45:58.361116 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\Documents\\dbt_project\\finance', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/mobiles/g_.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:45:58.362788 [debug] [MainThread]: Tracking: tracking
[0m17:45:58.370763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239E1DA9CF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239E1DA9810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239E1DA9A50>]}
[0m17:45:58.482116 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 1 files added, 0 files changed.
[0m17:45:58.482116 [debug] [MainThread]: Partial parsing: added file: finance://models\mobiles\g_.sql
[0m17:45:58.483617 [debug] [MainThread]: Partial parsing: deleted file: finance://models\mobiles\fact_sales.sql
[0m17:45:58.483617 [debug] [MainThread]: Partial parsing: deleted file: finance://models\mobiles\dim_store.sql
[0m17:45:58.484190 [debug] [MainThread]: Partial parsing: deleted file: finance://models\mobiles\dim_product.sql
[0m17:45:58.499424 [debug] [MainThread]: 1699: static parser successfully parsed mobiles\g_.sql
[0m17:45:58.580092 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.finance.example

[0m17:45:58.585018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a8602bb0-b963-470f-b3f6-7fc07392a424', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239E30B0160>]}
[0m17:45:58.601211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a8602bb0-b963-470f-b3f6-7fc07392a424', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239E2026650>]}
[0m17:45:58.601658 [info ] [MainThread]: Found 8 models, 25 tests, 0 snapshots, 0 analyses, 531 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m17:45:58.601658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a8602bb0-b963-470f-b3f6-7fc07392a424', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239E20264D0>]}
[0m17:45:58.604775 [info ] [MainThread]: 
[0m17:45:58.604775 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:45:58.604775 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m17:45:58.610819 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m17:45:58.610819 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:45:58.610819 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:46:00.671287 [debug] [ThreadPool]: SQL status: OK in 2.06 seconds
[0m17:46:00.681462 [debug] [ThreadPool]: On list_schemas: Close
[0m17:46:02.040979 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_finance"
[0m17:46:02.050786 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:46:02.050786 [debug] [ThreadPool]: Using databricks connection "list_None_finance"
[0m17:46:02.050786 [debug] [ThreadPool]: On list_None_finance: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_None_finance"} */
show table extended in finance like '*'
  
[0m17:46:02.053076 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:46:05.031153 [debug] [ThreadPool]: SQL status: OK in 2.98 seconds
[0m17:46:05.031153 [debug] [ThreadPool]: On list_None_finance: ROLLBACK
[0m17:46:05.031153 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:46:05.031153 [debug] [ThreadPool]: On list_None_finance: Close
[0m17:46:05.631288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a8602bb0-b963-470f-b3f6-7fc07392a424', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239E1FCA0E0>]}
[0m17:46:05.631288 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:46:05.631288 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:46:05.631288 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:46:05.639143 [info ] [MainThread]: 
[0m17:46:05.641389 [debug] [Thread-1 (]: Began running node model.finance.g_
[0m17:46:05.641389 [info ] [Thread-1 (]: 1 of 1 START sql table model finance.g_ ........................................ [RUN]
[0m17:46:05.650895 [debug] [Thread-1 (]: Acquiring new databricks connection "model.finance.g_"
[0m17:46:05.650895 [debug] [Thread-1 (]: Began compiling node model.finance.g_
[0m17:46:05.650895 [debug] [Thread-1 (]: Compiling model.finance.g_
[0m17:46:05.654754 [debug] [Thread-1 (]: Writing injected SQL for node "model.finance.g_"
[0m17:46:05.654754 [debug] [Thread-1 (]: finished collecting timing info
[0m17:46:05.654754 [debug] [Thread-1 (]: Began executing node model.finance.g_
[0m17:46:05.696331 [debug] [Thread-1 (]: Writing runtime sql for node "model.finance.g_"
[0m17:46:05.696331 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:46:05.696331 [debug] [Thread-1 (]: Using databricks connection "model.finance.g_"
[0m17:46:05.700197 [debug] [Thread-1 (]: On model.finance.g_: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "node_id": "model.finance.g_"} */

  
    
      create or replace table finance.g_
    
    
    using delta
    
    
    
    
    
    
    as
      
SELECT fact_sales.product_id, fact_sales.store_id, dim_product.product_name, dim_product.product_price,
fact_sales.quantity,fact_sales.price, fact_sales.total_price, dim_store.store_name, dim_store.store_state
FROM dim_product JOIN fact_sales on dim_product.product_id = fact_sales.product_id
JOIN dim_store ON fact_sales.product_id = dim_store.store_id;
  
[0m17:46:05.700197 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:46:31.131251 [debug] [Thread-1 (]: SQL status: OK in 25.43 seconds
[0m17:46:31.141382 [debug] [Thread-1 (]: finished collecting timing info
[0m17:46:31.141382 [debug] [Thread-1 (]: On model.finance.g_: ROLLBACK
[0m17:46:31.141382 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m17:46:31.141382 [debug] [Thread-1 (]: On model.finance.g_: Close
[0m17:46:33.321198 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a8602bb0-b963-470f-b3f6-7fc07392a424', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239E311B700>]}
[0m17:46:33.321198 [info ] [Thread-1 (]: 1 of 1 OK created sql table model finance.g_ ................................... [[32mOK[0m in 27.67s]
[0m17:46:33.321198 [debug] [Thread-1 (]: Finished running node model.finance.g_
[0m17:46:33.321198 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:46:33.321198 [debug] [MainThread]: On master: ROLLBACK
[0m17:46:33.321198 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:46:33.961176 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:46:33.961176 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:46:33.961176 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:46:33.961176 [debug] [MainThread]: On master: ROLLBACK
[0m17:46:33.965845 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:46:33.965935 [debug] [MainThread]: On master: Close
[0m17:46:44.058816 [info ] [MainThread]: 
[0m17:46:44.060802 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 45.45 seconds (45.45s).
[0m17:46:44.062316 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:46:44.062813 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m17:46:44.062813 [debug] [MainThread]: Connection 'list_None_finance' was properly closed.
[0m17:46:44.063309 [debug] [MainThread]: Connection 'model.finance.g_' was properly closed.
[0m17:46:44.073722 [info ] [MainThread]: 
[0m17:46:44.073722 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:46:44.073722 [info ] [MainThread]: 
[0m17:46:44.081033 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m17:46:44.081033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239E2047F70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239E1FCA3B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239E1DA9EA0>]}
[0m17:46:44.081033 [debug] [MainThread]: Flushing usage events


============================== 2022-12-08 13:22:57.981187 | 4ba70595-a934-4cb2-b24c-66e85b5a10e6 ==============================
[0m13:22:57.981187 [info ] [MainThread]: Running with dbt=1.3.1
[0m13:22:57.982185 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\Documents\\dbt_project\\finance', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:22:57.983182 [debug] [MainThread]: Tracking: tracking
[0m13:22:57.994009 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002410906DCF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002410906D810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002410906DA50>]}
[0m13:22:58.845049 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 2 files added, 0 files changed.
[0m13:22:58.845049 [debug] [MainThread]: Partial parsing: added file: finance://models\mobiles\schema.yml
[0m13:22:58.846048 [debug] [MainThread]: Partial parsing: added file: finance://models\mobiles\store.sql
[0m13:22:58.846048 [debug] [MainThread]: Partial parsing: deleted file: finance://models\mobiles\g_.sql
[0m13:22:58.857070 [debug] [MainThread]: 1699: static parser successfully parsed mobiles\store.sql
[0m13:22:58.918483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002410A31B5B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002410A36E3E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002410A36E740>]}
[0m13:22:58.919475 [debug] [MainThread]: Flushing usage events
[0m13:23:00.556446 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two schema.yml entries for the same resource named dim_info. Resources and their associated columns may only be described a single time. To fix this, remove the resource entry for dim_info in one of these files:
   - models\mobiles\schema.yml
  models\finance\schema.yml


============================== 2022-12-08 13:24:42.439858 | c2ca28ee-131e-41db-aa7a-0a6fc2384004 ==============================
[0m13:24:42.440854 [info ] [MainThread]: Running with dbt=1.3.1
[0m13:24:42.441562 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\Documents\\dbt_project\\finance', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/mobiles/store.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:24:42.441562 [debug] [MainThread]: Tracking: tracking
[0m13:24:42.453753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015742E69CF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015742E698A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015742E69FC0>]}
[0m13:24:42.552671 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 2 files added, 0 files changed.
[0m13:24:42.553678 [debug] [MainThread]: Partial parsing: added file: finance://models\mobiles\schema.yml
[0m13:24:42.553678 [debug] [MainThread]: Partial parsing: added file: finance://models\mobiles\store.sql
[0m13:24:42.553678 [debug] [MainThread]: Partial parsing: deleted file: finance://models\mobiles\g_.sql
[0m13:24:42.564631 [debug] [MainThread]: 1699: static parser successfully parsed mobiles\store.sql
[0m13:24:42.618101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001574410F580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001574415E3B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001574415E710>]}
[0m13:24:42.618101 [debug] [MainThread]: Flushing usage events
[0m13:24:44.285394 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two schema.yml entries for the same resource named dim_info. Resources and their associated columns may only be described a single time. To fix this, remove the resource entry for dim_info in one of these files:
   - models\mobiles\schema.yml
  models\finance\schema.yml


============================== 2022-12-08 13:24:56.994147 | 835f09b5-667c-459e-a44e-22f28b0558a2 ==============================
[0m13:24:56.994147 [info ] [MainThread]: Running with dbt=1.3.1
[0m13:24:56.995161 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\XavierDonBosco\\Documents\\dbt_project\\finance', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/mobiles/store.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:24:56.995161 [debug] [MainThread]: Tracking: tracking
[0m13:24:57.008471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237B8C89CF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237B8C89810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237B8C89A50>]}
[0m13:24:57.107605 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m13:24:57.107605 [debug] [MainThread]: Partial parsing: added file: finance://models\mobiles\store.sql
[0m13:24:57.107605 [debug] [MainThread]: Partial parsing: deleted file: finance://models\mobiles\g_.sql
[0m13:24:57.121328 [debug] [MainThread]: 1699: static parser successfully parsed mobiles\store.sql
[0m13:24:57.181616 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.finance.example

[0m13:24:57.188942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '835f09b5-667c-459e-a44e-22f28b0558a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237B9F80160>]}
[0m13:24:57.202793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '835f09b5-667c-459e-a44e-22f28b0558a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237B9E40BB0>]}
[0m13:24:57.202793 [info ] [MainThread]: Found 8 models, 25 tests, 0 snapshots, 0 analyses, 531 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m13:24:57.203879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '835f09b5-667c-459e-a44e-22f28b0558a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237B9E40B50>]}
[0m13:24:57.204958 [info ] [MainThread]: 
[0m13:24:57.205952 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m13:24:57.208159 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m13:24:57.218207 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m13:24:57.218207 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:24:57.219206 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:24:58.417150 [debug] [ThreadPool]: Databricks adapter: failed to connect: Error during request to server: RESOURCE_DOES_NOT_EXIST: No cluster found matching: 1127-080850-8h8sh280
[0m13:24:58.418070 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.RequestError'>: Error during request to server: RESOURCE_DOES_NOT_EXIST: No cluster found matching: 1127-080850-8h8sh280
[0m13:24:58.418070 [debug] [ThreadPool]: Databricks adapter: attempt: 1/30
[0m13:24:58.419107 [debug] [ThreadPool]: Databricks adapter: bounded-retry-delay: None
[0m13:24:58.420067 [debug] [ThreadPool]: Databricks adapter: elapsed-seconds: 1.1890113353729248/900.0
[0m13:24:58.420067 [debug] [ThreadPool]: Databricks adapter: error-message: RESOURCE_DOES_NOT_EXIST: No cluster found matching: 1127-080850-8h8sh280
[0m13:24:58.420948 [debug] [ThreadPool]: Databricks adapter: http-code: 404
[0m13:24:58.422227 [debug] [ThreadPool]: Databricks adapter: method: OpenSession
[0m13:24:58.422227 [debug] [ThreadPool]: Databricks adapter: no-retry-reason: non-retryable error
[0m13:24:58.423232 [debug] [ThreadPool]: Databricks adapter: original-exception: RESOURCE_DOES_NOT_EXIST: No cluster found matching: 1127-080850-8h8sh280
[0m13:24:58.423488 [debug] [ThreadPool]: Databricks adapter: query-id: None
[0m13:24:58.423488 [debug] [ThreadPool]: Databricks adapter: session-id: None
[0m13:24:58.424193 [debug] [ThreadPool]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "finance", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:24:58.424193 [debug] [ThreadPool]: Databricks adapter: Database Error
  failed to connect
[0m13:24:58.424193 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro list_schemas
[0m13:24:58.424193 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  Database Error
    failed to connect
[0m13:24:58.426388 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:24:58.426388 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m13:24:58.426388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237B8C8BFD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237B9EF3E80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237B9FBB100>]}
[0m13:24:58.426388 [debug] [MainThread]: Flushing usage events
[0m13:24:59.975839 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
